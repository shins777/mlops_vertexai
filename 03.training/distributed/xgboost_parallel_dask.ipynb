{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fPc-KWUi2Xd"
   },
   "outputs": [],
   "source": [
    "# Copyright  2024 Forusone\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoXf8TfQoVth"
   },
   "source": [
    "## XGBoost parallel training with Dask On Vertex AI\n",
    "\n",
    "* https://cloud.google.com/vertex-ai/docs/training/overview\n",
    "* IRIS dataset: https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html\n",
    "* https://distributed.dask.org/en/stable/install.html\n",
    "* https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/xgboost_data_parallel_training_on_cpu_using_dask.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"ai-hangsik\" \n",
    "LOCATION = \"us-central1\"  \n",
    "BUCKET_URI= f\"gs://sllm_checkpoints/xgboost_dask\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4dc6b3ba241c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EStvMiC9tdS"
   },
   "source": [
    "### Training\n",
    "* The `train.py` file checks whether the current node is the chief node or a worker node and runs `dask-scheduler` for the chief node and `dask-worker` for worker nodes. Worker nodes connect to the chief node through the IP address and port number specified in `CLUSTER_SPEC`.\n",
    "\n",
    "* After the Dask scheduler is set up and connected to worker nodes, call `xgb.dask.train` to train a model through Dask. Once model training is complete, the model is uploaded to `AIP_MODEL_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/llmOps_vertexAI/training/custom_training/dask\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/llmOps_vertexAI/training/custom_training/dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "thNtAY2Gsx2h",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "from dask.distributed import Client, wait\n",
    "from xgboost.dask import DaskDMatrix\n",
    "from google.cloud import storage\n",
    "import xgboost as xgb\n",
    "import dask.dataframe as dd\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "\n",
    "IRIS_DATA_FILENAME = 'gs://cloud-samples-data/ai-platform/iris/iris_data.csv'\n",
    "IRIS_TARGET_FILENAME = 'gs://cloud-samples-data/ai-platform/iris/iris_target.csv'\n",
    "MODEL_FILE = 'model.bst'\n",
    "MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n",
    "XGB_PARAMS = {\n",
    "    'verbosity': 2,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 8,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'subsample': 0.6,\n",
    "    'gamma': 1,\n",
    "    'verbose_eval': True,\n",
    "    'tree_method': 'hist',\n",
    "    'nthread': 1\n",
    "}\n",
    "\n",
    "def launch(cmd):\n",
    "    \"\"\" launch dask workers\n",
    "    \"\"\"\n",
    "    return subprocess.check_call(cmd, stdout=sys.stdout, stderr=sys.stderr, shell=True)\n",
    "\n",
    "\n",
    "def get_chief_ip(cluster_config_dict):\n",
    "    if 'workerpool0' in cluster_config_dict['cluster']:\n",
    "      ip_address = cluster_config_dict['cluster']['workerpool0'][0].split(\":\")[0]\n",
    "    else:\n",
    "      # if the job is not distributed, 'chief' will be populated instead of\n",
    "      # workerpool0.\n",
    "      ip_address = cluster_config_dict['cluster']['chief'][0].split(\":\")[0]\n",
    "\n",
    "    print('The ip address of workerpool 0 is : {}'.format(ip_address))\n",
    "    return ip_address\n",
    "\n",
    "def get_chief_port(cluster_config_dict):\n",
    "\n",
    "    if \"open_ports\" in cluster_config_dict:\n",
    "      port = cluster_config_dict['open_ports'][0]\n",
    "    else:\n",
    "      # Use any port for the non-distributed job.\n",
    "      port = 7777\n",
    "    print(\"The open port is: {}\".format(port))\n",
    "\n",
    "    return port\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    cluster_config_str = os.environ.get('CLUSTER_SPEC')\n",
    "    cluster_config_dict  = json.loads(cluster_config_str)\n",
    "    print(json.dumps(cluster_config_dict, indent=2))\n",
    "    print('The workerpool type is:', flush=True)\n",
    "    print(cluster_config_dict['task']['type'], flush=True)\n",
    "    workerpool_type = cluster_config_dict['task']['type']\n",
    "    chief_ip = get_chief_ip(cluster_config_dict)\n",
    "    chief_port = get_chief_port(cluster_config_dict)\n",
    "    chief_address = \"{}:{}\".format(chief_ip, chief_port)\n",
    "\n",
    "    if workerpool_type == \"workerpool0\":\n",
    "        print('Running the dask scheduler.', flush=True)\n",
    "        proc_scheduler = launch('dask-scheduler --dashboard --dashboard-address 8888 --port {} &'.format(chief_port))\n",
    "        print('Done the dask scheduler.', flush=True)\n",
    "\n",
    "        client = Client(chief_address, timeout=1200)\n",
    "        print('Waiting the scheduler to be connected.', flush=True)\n",
    "        client.wait_for_workers(1)\n",
    "\n",
    "        X = dd.read_csv(IRIS_DATA_FILENAME, header=None)\n",
    "        y = dd.read_csv(IRIS_TARGET_FILENAME, header=None)\n",
    "        \n",
    "        X.persist()\n",
    "        y.persist()\n",
    "        wait(X)\n",
    "        wait(y)\n",
    "        dtrain = DaskDMatrix(client, X, y)\n",
    "\n",
    "        output = xgb.dask.train(client, XGB_PARAMS, dtrain,  num_boost_round=100, evals=[(dtrain, 'train')])\n",
    "        print(\"Output: {}\".format(output), flush=True)\n",
    "        print(\"Saving file to: {}\".format(MODEL_FILE), flush=True)\n",
    "        \n",
    "        output['booster'].save_model(MODEL_FILE)\n",
    "        bucket_name = MODEL_DIR.replace(\"gs://\", \"\").split(\"/\", 1)[0]\n",
    "        folder = MODEL_DIR.replace(\"gs://\", \"\").split(\"/\", 1)[1]\n",
    "        bucket = storage.Client().bucket(bucket_name)\n",
    "        print(\"Uploading file to: {}/{}{}\".format(bucket_name, folder, MODEL_FILE), flush=True)\n",
    "        \n",
    "        blob = bucket.blob('{}{}'.format(folder, MODEL_FILE))\n",
    "        blob.upload_from_filename(MODEL_FILE)\n",
    "        print(\"Saved file to: {}/{}\".format(MODEL_DIR, MODEL_FILE), flush=True)\n",
    "\n",
    "        # Waiting 10 mins to connect the Dask dashboard\n",
    "        #time.sleep(60 * 10)        \n",
    "        client.shutdown()\n",
    "\n",
    "    else:\n",
    "        print('Running the dask worker.', flush=True)\n",
    "        client = Client(chief_address, timeout=1200)\n",
    "        print('client: {}.'.format(client), flush=True)\n",
    "        launch('dask-worker {}'.format(chief_address))\n",
    "        print('Done with the dask worker.', flush=True)\n",
    "\n",
    "        # Waiting 10 mins to connect the Dask dashboard\n",
    "        #time.sleep(60 * 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxsT4Vaos2W5"
   },
   "source": [
    "### Write the docker file\n",
    "The docker file is used to build the custom training container and passed to the Vertex Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/llmOps_vertexAI/training/custom_training/dask'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xD60d6Q0i2X0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-9:latest\n",
    "WORKDIR /root\n",
    "\n",
    "# Install sudo\n",
    "RUN apt-get update && apt-get -y install sudo\n",
    "\n",
    "# Update the keyring in order to run apt-get update.\n",
    "RUN rm -rf /usr/share/keyrings/cloud.google.gpg\n",
    "RUN rm -rf /etc/apt/sources.list.d/google-cloud-sdk.list\n",
    "RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "RUN echo \"deb https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
    "\n",
    "# Install packages (without sudo)\n",
    "RUN apt-get update && apt-get install -y telnet netcat iputils-ping net-tools\n",
    "\n",
    "# Determine the default Python version\n",
    "RUN echo python3 --version\n",
    "\n",
    "# Install Python packages using the identified version\n",
    "RUN python3 -m pip install 'xgboost>=1.4.2' 'dask-ml[complete]==2022.5.27' 'dask[complete]==2022.01.0' --upgrade\n",
    "RUN python3 -m pip install dask==2022.01.0 distributed==2022.01.0 bokeh==2.4.3 dask-cuda==22.2.0 click==8.0.1 --upgrade\n",
    "RUN python3 -m pip install gcsfs --upgrade\n",
    "\n",
    "# Make sure gsutil will use the default service account\n",
    "RUN echo '[GoogleCompute]\\nservice_account = default' > /etc/boto.cfg\n",
    "\n",
    "# Copies the trainer code\n",
    "RUN mkdir /root/trainer\n",
    "COPY train.py /root/trainer/train.py\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python3\", \"trainer/train.py\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6Yj8pZWAD7c"
   },
   "source": [
    "### Build a custom training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hd1j9BHeA81h",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0_csN1pAH95F",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n",
      "Listing items under project ai-hangsik, across all locations.\n",
      "\n",
      "                                                                                            ARTIFACT_REGISTRY\n",
      "REPOSITORY                         FORMAT  MODE                 DESCRIPTION                               LOCATION         LABELS  ENCRYPTION          CREATE_TIME          UPDATE_TIME          SIZE (MB)\n",
      "cloud-run-source-deploy            DOCKER  STANDARD_REPOSITORY  Cloud Run Source Deployments              asia-northeast3          Google-managed key  2024-03-01T14:59:17  2024-03-01T23:38:33  1505.522\n",
      "kubeflow-test                      DOCKER  STANDARD_REPOSITORY                                            asia-northeast3          Google-managed key  2024-11-10T07:54:48  2024-11-10T09:23:27  604.943\n",
      "cpr-handler-prediction             DOCKER  STANDARD_REPOSITORY                                            us-central1              Google-managed key  2025-02-03T22:42:11  2025-02-03T22:43:39  495.775\n",
      "custom-container-prediction        DOCKER  STANDARD_REPOSITORY  Docker repository for Customer container  us-central1              Google-managed key  2025-02-03T09:10:17  2025-02-03T09:14:37  525.625\n",
      "custom-container-repo              DOCKER  STANDARD_REPOSITORY  Docker repository for Customer container  us-central1              Google-managed key  2025-02-05T08:36:24  2025-02-05T12:58:33  725.063\n",
      "custom-inference-gpu               DOCKER  STANDARD_REPOSITORY                                            us-central1              Google-managed key  2025-01-31T10:50:54  2025-02-12T13:34:01  11061.685\n",
      "gemma-ray-vertexai                 DOCKER  STANDARD_REPOSITORY  Tutorial repository                       us-central1              Google-managed key  2025-02-10T09:36:40  2025-02-10T15:32:45  17882.858\n",
      "l4-training-repository-unique      DOCKER  STANDARD_REPOSITORY  Vertex L4 training repository             us-central1              Google-managed key  2024-12-07T09:56:58  2024-12-07T09:56:58  0\n",
      "xgboost-distributed-training-repo  DOCKER  STANDARD_REPOSITORY  Docker repository                         us-central1              Google-managed key  2025-02-19T05:55:22  2025-02-19T07:28:27  3258.530\n"
     ]
    }
   ],
   "source": [
    "PRIVATE_REPO = \"xgboost-distributed-training-repo\"\n",
    "\n",
    "! gcloud artifacts repositories create {PRIVATE_REPO} --repository-format=docker --location={LOCATION} --description=\"Docker repository\"\n",
    "\n",
    "! gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KPpGuKi-BOAD",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment: us-central1-docker.pkg.dev/ai-hangsik/xgboost-distributed-training-repo/xgboost-dask-train:latest\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGE = (\n",
    "    f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{PRIVATE_REPO}/xgboost-dask-train:latest\"\n",
    ")\n",
    "print(\"Deployment:\", TRAIN_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RgHDDL8BWgz"
   },
   "source": [
    "### Authenticate Docker to your repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GRLMyQwdKiLr",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "! gcloud auth configure-docker {LOCATION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW4EecX1Bj8j"
   },
   "source": [
    "### Set the custom Docker container image\n",
    "Set the custom Docker container image.\n",
    "\n",
    "1. Pull the corresponding CPU or GPU Docker image from Docker Hub.\n",
    "2. Create a tag for registering the image with Artifact Registry\n",
    "3. Register the image with Artifact Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Z2XTaEDHB9HK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  164.9kB\n",
      "Step 1/16 : FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-9:latest\n",
      " ---> 659118eb6058\n",
      "Step 2/16 : WORKDIR /root\n",
      " ---> Using cache\n",
      " ---> d5e5c617052a\n",
      "Step 3/16 : RUN apt-get update && apt-get -y install sudo\n",
      " ---> Using cache\n",
      " ---> dcbb7c84bec6\n",
      "Step 4/16 : RUN rm -rf /usr/share/keyrings/cloud.google.gpg\n",
      " ---> Using cache\n",
      " ---> 541c29fc4009\n",
      "Step 5/16 : RUN rm -rf /etc/apt/sources.list.d/google-cloud-sdk.list\n",
      " ---> Using cache\n",
      " ---> 5e41fece9bfa\n",
      "Step 6/16 : RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
      " ---> Using cache\n",
      " ---> 0159eb62e71e\n",
      "Step 7/16 : RUN echo \"deb https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
      " ---> Using cache\n",
      " ---> 963de0f3618d\n",
      "Step 8/16 : RUN apt-get update && apt-get install -y telnet netcat iputils-ping net-tools\n",
      " ---> Using cache\n",
      " ---> 9699d0df05b8\n",
      "Step 9/16 : RUN echo python3 --version\n",
      " ---> Using cache\n",
      " ---> 8274ef63cd7e\n",
      "Step 10/16 : RUN python3 -m pip install 'xgboost>=1.4.2' 'dask-ml[complete]==2022.5.27' 'dask[complete]==2022.01.0' \"pickleshare\" --upgrade\n",
      " ---> Using cache\n",
      " ---> 5d522591e5de\n",
      "Step 11/16 : RUN python3 -m pip install dask==2022.01.0 distributed==2022.01.0 bokeh==2.4.3 dask-cuda==22.2.0 click==8.0.1 --upgrade\n",
      " ---> Using cache\n",
      " ---> e37fc17a7806\n",
      "Step 12/16 : RUN python3 -m pip install gcsfs --upgrade\n",
      " ---> Using cache\n",
      " ---> f3e7f8aec03c\n",
      "Step 13/16 : RUN echo '[GoogleCompute]\\nservice_account = default' > /etc/boto.cfg\n",
      " ---> Using cache\n",
      " ---> 6bc37ae96d07\n",
      "Step 14/16 : RUN mkdir /root/trainer\n",
      " ---> Using cache\n",
      " ---> ed71666c8f9d\n",
      "Step 15/16 : COPY train.py /root/trainer/train.py\n",
      " ---> e9741e9baf1b\n",
      "Step 16/16 : ENTRYPOINT [\"python3\", \"trainer/train.py\"]\n",
      " ---> Running in d384076d2d42\n",
      "Removing intermediate container d384076d2d42\n",
      " ---> ffe4314e7bd9\n",
      "Successfully built ffe4314e7bd9\n",
      "Successfully tagged us-central1-docker.pkg.dev/ai-hangsik/xgboost-distributed-training-repo/xgboost-dask-train:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/ai-hangsik/xgboost-distributed-training-repo/xgboost-dask-train]\n",
      "\n",
      "\u001b[1B14560947: Preparing \n",
      "\u001b[1B1950cf06: Preparing \n",
      "\u001b[1B5b69ce10: Preparing \n",
      "\u001b[1Bafb57262: Preparing \n",
      "\u001b[1B7c88c049: Preparing \n",
      "\u001b[1B91c8f9bc: Preparing \n",
      "\u001b[1B9b09114e: Preparing \n",
      "\u001b[1B59744b06: Preparing \n",
      "\u001b[1Bbb1adc3a: Preparing \n",
      "\u001b[1Ba3092e04: Preparing \n",
      "\u001b[1Ba78e3f62: Preparing \n",
      "\u001b[1B49a5bd22: Preparing \n",
      "\u001b[1B95c7b436: Preparing \n",
      "\u001b[1B967c8575: Preparing \n",
      "\u001b[1Bebebc0f4: Preparing \n",
      "\u001b[1Ba16fa757: Preparing \n",
      "\u001b[1B7c997dbf: Preparing \n",
      "\u001b[1Bb5fad3d9: Preparing \n",
      "\u001b[1B3da9e839: Preparing \n",
      "\u001b[1Ba86db338: Preparing \n",
      "\u001b[1B00bd74fd: Preparing \n",
      "\u001b[1Bda764bfb: Preparing \n",
      "\u001b[1Bc7533f08: Preparing \n",
      "\u001b[1B63d13546: Preparing \n",
      "\u001b[1B3573db31: Preparing \n",
      "\u001b[1Bb22458bc: Preparing \n",
      "\u001b[1Bdaf3af9d: Preparing \n",
      "\u001b[1B1a9d2c3a: Preparing \n",
      "\u001b[1Bd9541ffc: Preparing \n",
      "\u001b[1B03ed0fb8: Preparing \n",
      "\u001b[26B1c8f9bc: Waiting g \n",
      "\u001b[1Bd7a08532: Preparing \n",
      "\u001b[20B67c8575: Waiting g \n",
      "\u001b[1B5ce613d3: Layer already exists 2kB\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[22A\u001b[2K\u001b[18A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2Klatest: digest: sha256:fbacb70b81470611d8575bdeb4cd387c3f582306777fe3ade6e4010d6c43262a size: 8050\n"
     ]
    }
   ],
   "source": [
    "! docker build -t $TRAIN_IMAGE -f Dockerfile .\n",
    "! docker push $TRAIN_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GB2j39BCXiy"
   },
   "source": [
    "### Run a Vertex AI SDK CustomContainerTrainingJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aoEkXaaDepfo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gcs_output_uri_prefix = f\"{BUCKET_URI}/output\"\n",
    "replica_count = 3\n",
    "machine_type = \"n1-standard-4\"\n",
    "display_name = \"xgboost-distributed-training\"\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"\n",
    "\n",
    "custom_container_training_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=display_name,\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    container_uri=TRAIN_IMAGE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://sllm_checkpoints/xgboost_dask/output \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Model object at 0x7ffa2f45d390> is waiting for upstream dependencies to complete."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8799683992824578048?project=721521243942\n",
      "CustomContainerTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/8799683992824578048 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6615227067317354496?project=721521243942\n",
      "CustomContainerTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/8799683992824578048 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/8799683992824578048 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/8799683992824578048 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/8799683992824578048 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob run completed. Resource name: projects/721521243942/locations/us-central1/trainingPipelines/8799683992824578048\n",
      "Model available at projects/721521243942/locations/us-central1/models/6769522667914526720\n"
     ]
    }
   ],
   "source": [
    "custom_container_training_job.run(\n",
    "    base_output_dir=gcs_output_uri_prefix,\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    enable_dashboard_access=True,\n",
    "    enable_web_access=True,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://sllm_checkpoints/xgboost_dask/output/model/model.bst\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $gcs_output_uri_prefix/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://sllm_checkpoints/xgboost_dask/output/model/model.bst...\n",
      "/ [1/1 files][ 21.5 KiB/ 21.5 KiB] 100% Done                                    \n",
      "Operation completed over 1 objects/21.5 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r gs://sllm_checkpoints/xgboost_dask/output/model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions ['versicolor', 'setosa']\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "_class_names = load_iris().target_names\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.load_model('model/model.bst')\n",
    "\n",
    "outputs = model.predict([[6.7, 3.1, 4.7, 1.5],[4.6, 3.1, 1.5, 0.2]])\n",
    "\n",
    "print(f\"predictions {[_class_names[class_num] for class_num in outputs]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c26XO4bZDnDH"
   },
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ue3SfrMODunu"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "\n",
    "# Set this to true only if you'd like to delete your bucket\n",
    "delete_bucket = False\n",
    "delete_application_directory = False\n",
    "\n",
    "! gsutil rm -rf $gcs_output_uri_prefix\n",
    "\n",
    "if delete_bucket:\n",
    "    ! gsutil rm -r $BUCKET_URI\n",
    "\n",
    "try:\n",
    "    custom_container_training_job.delete()\n",
    "except Exception as e:\n",
    "    logging.error(traceback.format_exc())\n",
    "    print(e)\n",
    "\n",
    "# Delete application directory\n",
    "if delete_application_directory:\n",
    "    ! rm -rf trainer config.yaml Dockerfile\n",
    "\n",
    "! gcloud artifacts repositories delete {PRIVATE_REPO} --location={LOCATION} --quiet"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "xgboost_data_parallel_training_on_cpu_using_dask.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
