{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright  2024 Forusone\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "## PyTorch distributed training with Vertex AI Reduction Server\n",
    "* https://cloud.google.com/blog/topics/developers-practitioners/optimize-training-performance-reduction-server-vertex-ai?e=13802955\n",
    "* https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/reduction_server/pytorch_distributed_training_reduction_server.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:custom,cifar10,icn"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "In this tutorial, we use [`imdb`](https://huggingface.co/datasets/imdb) dataset from Hugging Face. `imdb` is a large movie review dataset for binary sentiment classification containing a set of 25,000 highly polar movie reviews for training, and 25,000 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2c2cb2109a0"
   },
   "source": [
    "### Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6406a27bfea8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f872cd812d0"
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "294fe4e5a671",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"ai-hangsik\"  \n",
    "LOCATION = \"us-central1\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bucket",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://sllm_checkpoints/reduction_server\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cNEiwLd0lugu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBP_kLymhnYP"
   },
   "source": [
    "#### Recommended training application structure\n",
    "\n",
    "You can structure your training application in any way you like. However, the [following structure](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container#structure) is commonly used in Vertex AI samples, and having your project's organization be similar to the samples can make it easier for you to follow the samples.\n",
    "\n",
    "```\n",
    ".\n",
    "├── python_package\n",
    "│   ├── README.md\n",
    "│   ├── setup.py\n",
    "│   └── trainer\n",
    "│       ├── __init__.py\n",
    "│       └── task.py\n",
    "└── pytorch-distributed-training-reduction-server.ipynb    --> This notebook\n",
    "```\n",
    "\n",
    "1. Main project directory contains your `setup.py` file with the dependencies. \n",
    "2. Use a subdirectory named `trainer` to store your main application module and `scripts` to submit training jobs locally or cloud\n",
    "3. Inside `trainer` directory:\n",
    "    - `task.py` - Main application module 1) initializes PyTorch distributed training environment, and 2) Runs the model training and evaluation experiment, and exports the final model.\n",
    "    - `__init__.py` is required to make Python treat directories containing the file as packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4387867f83f2"
   },
   "source": [
    "### Define variables for the training application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/llmOps_vertexAI/training/custom_training/reduction_server\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/llmOps_vertexAI/training/custom_training/reduction_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mKPS5aGjaKRy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "APP_NAME = \"pytorch-bert\"\n",
    "PYTHON_PACKAGE_DIR = \"python_package\"\n",
    "PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI = (\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-9:latest\")\n",
    "\n",
    "source_package_file_name = f\"{PYTHON_PACKAGE_DIR}/dist/trainer-0.1.tar.gz\"\n",
    "python_package_gcs_uri = (f\"{BUCKET_URI}/pytorch-on-gcp/{APP_NAME}/train/python_package/trainer-0.1.tar.gz\")\n",
    "python_module_name = \"trainer.task\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NqTnsxaAdRp"
   },
   "source": [
    "#### Create file structure of the training application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "O9zLCELGbOjS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘python_package’: File exists\n",
      "mkdir: cannot create directory ‘python_package/trainer’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir {PYTHON_PACKAGE_DIR}\n",
    "! touch {PYTHON_PACKAGE_DIR}/README.md\n",
    "\n",
    "! mkdir {PYTHON_PACKAGE_DIR}/trainer\n",
    "! touch {PYTHON_PACKAGE_DIR}/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rpehVa0iFjY"
   },
   "source": [
    "#### Create the `setup.py` file for the training application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7QNjlhx7cBnj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./python_package/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./{PYTHON_PACKAGE_DIR}/setup.py\n",
    "\n",
    "import os\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "import setuptools\n",
    "\n",
    "from distutils.command.build import build as _build\n",
    "import subprocess\n",
    "\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'transformers==4.28.0',\n",
    "    'datasets',\n",
    "    'evaluate',\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Vertex AI | Training | PyTorch | Text Classification | Python Package'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXlifmTtiTzy"
   },
   "source": [
    "#### Create training application code\n",
    "\n",
    "`task.py` is the main application module. It initializes the PyTorch distributed training environment and runs the model training and evaluation experiment, and exports the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wruDyrEPsLbH",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./python_package/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./{PYTHON_PACKAGE_DIR}/trainer/task.py\n",
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "# you may not use this file except in compliance with the License.\\n\",\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import datasets\n",
    "from datasets import ClassLabel, Sequence, load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer,\n",
    "    EvalPrediction, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    default_data_collator)\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--epochs\", type=int, help=\"Number of training epochs.\", default=2)\n",
    "    parser.add_argument(\"--batch_size\", type=int, help=\"Training batch size for one process.\", default=32)\n",
    "    parser.add_argument(\"--model_dir\", type=str, help=\"Directory for saving models.\", default=os.environ['AIP_MODEL_DIR'] if 'AIP_MODEL_DIR' in os.environ else \"\")\n",
    "    argv = parser.parse_args()\n",
    "\n",
    "    model_name_or_path = \"bert-large-uncased\"\n",
    "    padding = \"max_length\"\n",
    "    max_seq_length = 128\n",
    "\n",
    "    datasets = load_dataset(\"imdb\", verification_mode='no_checks')\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_to_id = {1: 1, 0: 0, -1: 0}\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "      model_name_or_path,\n",
    "      use_fast=True,\n",
    "    )\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        \"\"\"\n",
    "        Tokenize the input example texts\n",
    "        \"\"\"\n",
    "        args = (examples[\"text\"],)\n",
    "        result = tokenizer(\n",
    "          *args, padding=padding, max_length=max_seq_length, truncation=True\n",
    "        )\n",
    "\n",
    "        # Map labels to IDs (not necessary for GLUE tasks)\n",
    "        if label_to_id is not None and \"label\" in examples:\n",
    "          result[\"label\"] = [label_to_id[example] for example in examples[\"label\"]]\n",
    "\n",
    "        return result\n",
    "\n",
    "    # apply preprocessing function to input examples\n",
    "    datasets = datasets.map(preprocess_function, batched=True, load_from_cache_file=True)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "      model_name_or_path, \n",
    "      num_labels=len(label_list)\n",
    "    )\n",
    "\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "    # Since we have ngpus_per_node processes per node, the total world_size\n",
    "    # needs to be adjusted accordingly\n",
    "    world_size =  world_size * ngpus_per_node\n",
    "\n",
    "    start = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    print(f'Starting distributed training: {start}') \n",
    "\n",
    "    # Use torch.multiprocessing.spawn to launch distributed processes\n",
    "    torch.multiprocessing.spawn(main_worker,\n",
    "    args = (ngpus_per_node, world_size, datasets, model, tokenizer, argv),\n",
    "    nprocs = ngpus_per_node,\n",
    "    join = True)\n",
    "\n",
    "    end = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    print(f'Distributed training complete: {end}')\n",
    "\n",
    "def main_worker(local_rank, ngpus_per_node, world_size, datasets, model, tokenizer, argv):\n",
    "\n",
    "    # This is the (global) rank of the current process\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "\n",
    "    # For multiprocessing distributed training, rank needs to be the\n",
    "    # global rank among all the processes\n",
    "    rank = rank * ngpus_per_node + local_rank\n",
    "    print (f\"Distributed and Multi-processing. Setting rank for each worker. rank={rank}\")\n",
    "\n",
    "    dist.init_process_group(\n",
    "      backend=\"nccl\", \n",
    "      init_method=\"env://\",\n",
    "      world_size=world_size, \n",
    "      rank=rank)\n",
    "\n",
    "    per_device_batch_size = int(argv.batch_size / ngpus_per_node)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "      output_dir=\"/tmp/output/\",\n",
    "      num_train_epochs=argv.epochs, \n",
    "      per_device_train_batch_size=per_device_batch_size,\n",
    "      per_device_eval_batch_size=per_device_batch_size,\n",
    "      local_rank=local_rank,\n",
    "    )\n",
    "\n",
    "    def compute_metrics(p: EvalPrediction):\n",
    "        \n",
    "        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        \n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets[\"train\"],\n",
    "        eval_dataset=datasets[\"test\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the trained model locally\n",
    "    model_filename = \"pytorch-bert-model\"\n",
    "    local_path = os.path.join(\"/tmp\", model_filename)\n",
    "    trainer.save_model(local_path)\n",
    "\n",
    "    if (os.path.exists(local_path)):\n",
    "        # Upload the trained model to Cloud storage\n",
    "        model_directory = argv.model_dir\n",
    "        storage_path = os.path.join(model_directory, model_filename)\n",
    "        blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\n",
    "\n",
    "        files = [f for f in os.listdir(local_path) if os.path.isfile(os.path.join(local_path, f))]\n",
    "\n",
    "        for file in files:\n",
    "            local_file = os.path.join(local_path, file)\n",
    "            blob.upload_from_filename(local_file)\n",
    "\n",
    "        print(f\"Saved model files in {model_directory}/{model_filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzmJxNOnix9O"
   },
   "source": [
    "#### Create a source distribution\n",
    "\n",
    "Create a source distribution `dist/trainer-0.1.tar.gz`, and upload the source distribution with training application to Cloud Storage bucket, and then validate the source distribution exists on Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dGgRZ4N-bbW0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying README.md -> trainer-0.1\n",
      "copying setup.py -> trainer-0.1\n",
      "copying trainer/__init__.py -> trainer-0.1/trainer\n",
      "copying trainer/task.py -> trainer-0.1/trainer\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/requires.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n",
      "Copying file://python_package/dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  3.3 KiB/  3.3 KiB]                                                \n",
      "Operation completed over 1 objects/3.3 KiB.                                      \n",
      "      3348  2025-02-19T09:47:11Z  gs://sllm_checkpoints/reduction_server/pytorch-on-gcp/pytorch-bert/train/python_package/trainer-0.1.tar.gz\n",
      "TOTAL: 1 objects, 3348 bytes (3.27 KiB)\n"
     ]
    }
   ],
   "source": [
    "! cd {PYTHON_PACKAGE_DIR} && python3 setup.py sdist --formats=gztar\n",
    "\n",
    "! gsutil cp {source_package_file_name} {python_package_gcs_uri}\n",
    "\n",
    "! gsutil ls -l {python_package_gcs_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gv0PB6WhjKsA"
   },
   "source": [
    "### Run custom training job with Reduction Server on Vertex AI\n",
    "\n",
    "Configure a custom job with the pre-built container image for PyTorch and training code packaged as Python source distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OPWYVIEwcQWX",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APP_NAME=pytorch-bert\n",
      "PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-9:latest\n",
      "python_package_gcs_uri=gs://sllm_checkpoints/reduction_server/pytorch-on-gcp/pytorch-bert/train/python_package/trainer-0.1.tar.gz\n",
      "python_module_name=trainer.task\n"
     ]
    }
   ],
   "source": [
    "print(f\"APP_NAME={APP_NAME}\")\n",
    "print(\n",
    "    f\"PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI={PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI}\"\n",
    ")\n",
    "print(f\"python_package_gcs_uri={python_package_gcs_uri}\")\n",
    "print(f\"python_module_name={python_module_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy_OwOByjW1W"
   },
   "source": [
    "#### Create a training job\n",
    "* https://cloud.google.com/vertex-ai/docs/start/client-libraries#client_libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ups1PMXCcjDB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME=pytorch-bert-reduction-server\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = f\"pytorch-bert-reduction-server\"\n",
    "print(f\"JOB_NAME={JOB_NAME}\")\n",
    "\n",
    "job = aiplatform.CustomPythonPackageTrainingJob(\n",
    "    display_name=f\"{JOB_NAME}\",\n",
    "    python_package_gcs_uri=python_package_gcs_uri,\n",
    "    python_module_name=python_module_name,\n",
    "    container_uri=PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YimBZzZmAUwP"
   },
   "source": [
    "#### Define the training cluster worker pool and experiment configuration parameters\n",
    "* https://cloud.google.com/vertex-ai/docs/training/distributed-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "yIjTAfqvcnJu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training cluster worker pool configuration\n",
    "REPLICA_COUNT = 3\n",
    "MACHINE_TYPE = \"g2-standard-48\"\n",
    "ACCELERATOR_TYPE = \"NVIDIA_L4\"\n",
    "ACCELERATOR_COUNT = 4\n",
    "\n",
    "# Reduction Server configuration\n",
    "REDUCTION_SERVER_COUNT = 4\n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "REDUCTION_SERVER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\"\n",
    ")\n",
    "ENVIRONMENT_VARIABLES = {\"NCCL_DEBUG\": \"INFO\"}\n",
    "\n",
    "# Training experiment parameters\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "MODEL_DIR = f\"{BUCKET_URI}/{JOB_NAME}\"\n",
    "\n",
    "training_args = [\n",
    "    \"--epochs\",\n",
    "    str(EPOCHS),\n",
    "    \"--batch_size\",\n",
    "    str(BATCH_SIZE),\n",
    "    \"--model_dir\",\n",
    "    MODEL_DIR,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xErX0T7LHYjX"
   },
   "source": [
    "#### Submit the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtBA-NTIcr7T",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://sllm_checkpoints/reduction_server/aiplatform-custom-training-2025-02-19-09:47:26.698 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4048210464088260608?project=721521243942\n",
      "CustomPythonPackageTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/4048210464088260608 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2865558165066350592?project=721521243942\n",
      "CustomPythonPackageTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/4048210464088260608 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/4048210464088260608 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/4048210464088260608 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/4048210464088260608 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/4048210464088260608 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/4048210464088260608 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/4048210464088260608 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "model = job.run(\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    "    reduction_server_replica_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    "    reduction_server_container_uri=REDUCTION_SERVER_IMAGE_URI,\n",
    "    environment_variables=ENVIRONMENT_VARIABLES,\n",
    "    args=training_args,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6LpsQI1WpPz"
   },
   "source": [
    "#### Validate the model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_138iEP3HD0L",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifacts are available at gs://sllm_checkpoints/reduction_server/pytorch-bert-reduction-server\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model artifacts are available at {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:custom"
   },
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNmebHf7lug0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "delete_custom_job = True\n",
    "delete_bucket = False\n",
    "\n",
    "if delete_custom_job:\n",
    "    try:\n",
    "        job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch_distributed_training_reduction_server.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
