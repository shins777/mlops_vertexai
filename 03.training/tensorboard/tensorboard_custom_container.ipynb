{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Gpc9ZhH_W4AP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Forusone\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed2pOXQMb8fY"
   },
   "source": [
    "# Vertex AI TensorBoard custom training with custom container\n",
    "\n",
    "* https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tensorboard/tensorboard_custom_training_with_custom_container.ipynb\n",
    "* [flower dataset](https://www.tensorflow.org/datasets/catalog/tf_flowers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2c2cb2109a0"
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n21c5UHogVEY",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user --quiet google-cloud-aiplatform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oM1iC_MfAts1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"ai-hangsik\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type: \"string\"}\n",
    "\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rheJHuCIadAL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MzGDU7TWdts_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://mlops-0221\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "NIq7R4HZCfIc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://mlops-0221/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'mlops-0221' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q73OgpxyDtSk"
   },
   "source": [
    "### Initialize aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yO4wki2xD0Ys",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eaef8c7be0e"
   },
   "source": [
    "### Enable Artifact Registry API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d03035c8fb6f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNmHMIyjBzxx"
   },
   "source": [
    "### Create Docker repository\n",
    "\n",
    "Create a Docker repository named `DOCKER_REPOSITORY` in your `LOCATION`.\n",
    "This docker repository is deleted in the cleaning up section at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iz9CUUbdBTvF",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker repository to create: tensorboard\n"
     ]
    }
   ],
   "source": [
    "DOCKER_REPOSITORY = \"tensorboard\"  # @param {type:\"string\"}\n",
    "print(\"Docker repository to create:\", DOCKER_REPOSITORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "R9eQDigtC8XA",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "! gcloud  artifacts repositories create  $DOCKER_REPOSITORY --project={PROJECT_ID} \\\n",
    "--repository-format=docker \\\n",
    "--location={LOCATION} --description=\"Repository for TensorBoard Custom Training Job\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fgHYvZlwDuAN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing items under project ai-hangsik, across all locations.\n",
      "\n",
      "                                                                                               ARTIFACT_REGISTRY\n",
      "REPOSITORY                         FORMAT  MODE                 DESCRIPTION                                     LOCATION         LABELS  ENCRYPTION          CREATE_TIME          UPDATE_TIME          SIZE (MB)\n",
      "cloud-run-source-deploy            DOCKER  STANDARD_REPOSITORY  Cloud Run Source Deployments                    asia-northeast3          Google-managed key  2024-03-01T14:59:17  2024-03-01T23:38:33  1505.522\n",
      "kubeflow-test                      DOCKER  STANDARD_REPOSITORY                                                  asia-northeast3          Google-managed key  2024-11-10T07:54:48  2024-11-10T09:23:27  604.943\n",
      "cpr-handler-prediction             DOCKER  STANDARD_REPOSITORY                                                  us-central1              Google-managed key  2025-02-03T22:42:11  2025-02-03T22:43:39  495.775\n",
      "custom-container-prediction        DOCKER  STANDARD_REPOSITORY  Docker repository for Customer container        us-central1              Google-managed key  2025-02-03T09:10:17  2025-02-03T09:14:37  525.625\n",
      "custom-container-repo              DOCKER  STANDARD_REPOSITORY  Docker repository for Customer container        us-central1              Google-managed key  2025-02-05T08:36:24  2025-02-05T12:58:33  725.063\n",
      "custom-inference-gpu               DOCKER  STANDARD_REPOSITORY                                                  us-central1              Google-managed key  2025-01-31T10:50:54  2025-02-12T13:34:01  11061.685\n",
      "gemma-ray-vertexai                 DOCKER  STANDARD_REPOSITORY  Tutorial repository                             us-central1              Google-managed key  2025-02-10T09:36:40  2025-02-10T15:32:45  17882.858\n",
      "kubeflow-vertex                    KFP     STANDARD_REPOSITORY  kubeflow-vertex ai test                         us-central1              Google-managed key  2025-02-21T00:06:34  2025-02-21T00:07:30  0.008\n",
      "l4-training-repository-unique      DOCKER  STANDARD_REPOSITORY  Vertex L4 training repository                   us-central1              Google-managed key  2024-12-07T09:56:58  2024-12-07T09:56:58  0\n",
      "my-docker-repo-unique              DOCKER  STANDARD_REPOSITORY  Repository for TensorBoard Custom Training Job  us-central1              Google-managed key  2025-02-21T00:38:07  2025-02-21T00:44:16  1951.033\n",
      "tensorboard                        DOCKER  STANDARD_REPOSITORY  Repository for TensorBoard Custom Training Job  us-central1              Google-managed key  2025-02-21T00:58:47  2025-02-21T00:58:47  0\n",
      "xgboost-distributed-training-repo  DOCKER  STANDARD_REPOSITORY  Docker repository                               us-central1              Google-managed key  2025-02-19T05:55:22  2025-02-19T08:15:09  3258.554\n"
     ]
    }
   ],
   "source": [
    "! gcloud artifacts repositories list --project={PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laaA77LsIt0c"
   },
   "source": [
    "## Create a custom container image and push to Artifact Registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mlops_vertexai/03.training/tensorboard\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AVcNnHfEKBdm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mlops_vertexai/03.training/tensorboard/tb-custom-container\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# Create a folder for the image.\n",
    "\n",
    "!mkdir tb-custom-container\n",
    "%cd tb-custom-container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUcVG77dKmPn"
   },
   "source": [
    "### Create a training code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7k1JDOGNKL3d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "IMG_WIDTH = 128\n",
    "\n",
    "def normalize_img(image):\n",
    "    \"\"\"Normalizes image.\n",
    "\n",
    "    * Resizes image to IMG_WIDTH x IMG_WIDTH pixels\n",
    "    * Casts values from `uint8` to `float32`\n",
    "    * Scales values from [0, 255] to [0, 1]\n",
    "\n",
    "    Returns:\n",
    "      A tensor with shape (IMG_WIDTH, IMG_WIDTH, 3). (3 color channels)\n",
    "    \"\"\"\n",
    "    image = tf.image.resize_with_pad(image, IMG_WIDTH, IMG_WIDTH)\n",
    "    return image / 255.\n",
    "\n",
    "\n",
    "def normalize_img_and_label(image, label):\n",
    "    \"\"\"Normalizes image and label.\n",
    "\n",
    "    * Performs normalize_img on image\n",
    "    * Passes through label unchanged\n",
    "\n",
    "    Returns:\n",
    "      Tuple (image, label) where\n",
    "      * image is a tensor with shape (IMG_WIDTH, IMG_WIDTH, 3). (3 color\n",
    "        channels)\n",
    "      * label is an unchanged integer [0, 4] representing flower type\n",
    "    \"\"\"\n",
    "    return normalize_img(image), label\n",
    "\n",
    "logging.info('Loading and preprocessing data ...')\n",
    "dataset = tfds.load('tf_flowers:3.*.*',\n",
    "                    split='train',\n",
    "                    try_gcs=True,\n",
    "                    shuffle_files=True,\n",
    "                    as_supervised=True)\n",
    "dataset = dataset.map(normalize_img_and_label,\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(1000)\n",
    "dataset = dataset.batch(128)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "logging.info('Creating and training model ...')\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16,\n",
    "                           3,\n",
    "                           padding='same',\n",
    "                           activation='relu',\n",
    "                           input_shape=(IMG_WIDTH, IMG_WIDTH, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(5)  # 5 classes\n",
    "])\n",
    "\n",
    "logging.info('Compiling model ...')\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs\"\n",
    "\n",
    "if 'AIP_TENSORBOARD_LOG_DIR' in os.environ:\n",
    "    log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR']\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
    "                                                      histogram_freq=1)\n",
    "\n",
    "logging.info('Training model ...')\n",
    "model.fit(dataset, epochs=13, callbacks=[tensorboard_callback])\n",
    "\n",
    "logging.info('Model training done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fxi1HZbKQUKY",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "# Specifies base image and tag\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\n",
    "RUN pip install tensorflow-datasets\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages as you need.\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY task.py /root/task.py\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"task.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "BvyoFJW_LJeG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 2 file(s) totalling 2.9 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://ai-hangsik_cloudbuild/source/1740100161.370207-a82e56bab1bb453d9cc6cf781b2dfbbd.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/ai-hangsik/locations/us-central1/builds/f2f48ac2-6636-4345-b89d-c73901d3f6c0].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/f2f48ac2-6636-4345-b89d-c73901d3f6c0?project=721521243942 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"f2f48ac2-6636-4345-b89d-c73901d3f6c0\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://ai-hangsik_cloudbuild/source/1740100161.370207-a82e56bab1bb453d9cc6cf781b2dfbbd.tgz#1740100161561621\n",
      "Copying gs://ai-hangsik_cloudbuild/source/1740100161.370207-a82e56bab1bb453d9cc6cf781b2dfbbd.tgz#1740100161561621...\n",
      "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
      "Operation completed over 1 objects/1.4 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  5.632kB\n",
      "Step 1/5 : FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\n",
      "latest: Pulling from vertex-ai/training/tf-cpu.2-8\n",
      "2ab09b027e7f: Pulling fs layer\n",
      "c7d3a8feeced: Pulling fs layer\n",
      "25cf9530b13c: Pulling fs layer\n",
      "e74e07ced7b5: Pulling fs layer\n",
      "982f1e01177f: Pulling fs layer\n",
      "d704d5405378: Pulling fs layer\n",
      "e90cce37d13d: Pulling fs layer\n",
      "570a72d5008b: Pulling fs layer\n",
      "802c11f474f2: Pulling fs layer\n",
      "78a96bfc2e15: Pulling fs layer\n",
      "cfbfdad48433: Pulling fs layer\n",
      "b3171aef3ea9: Pulling fs layer\n",
      "af599ce703be: Pulling fs layer\n",
      "f834e031e1e5: Pulling fs layer\n",
      "fcd3e0c595bb: Pulling fs layer\n",
      "363aac61da30: Pulling fs layer\n",
      "f376fb1d6697: Pulling fs layer\n",
      "b68d68ffee50: Pulling fs layer\n",
      "f9af1b6b39a5: Pulling fs layer\n",
      "ebd99ebb20bf: Pulling fs layer\n",
      "caeb9ac3c484: Pulling fs layer\n",
      "b3171aef3ea9: Waiting\n",
      "af599ce703be: Waiting\n",
      "f834e031e1e5: Waiting\n",
      "fcd3e0c595bb: Waiting\n",
      "363aac61da30: Waiting\n",
      "f376fb1d6697: Waiting\n",
      "b68d68ffee50: Waiting\n",
      "f9af1b6b39a5: Waiting\n",
      "ebd99ebb20bf: Waiting\n",
      "caeb9ac3c484: Waiting\n",
      "e90cce37d13d: Waiting\n",
      "570a72d5008b: Waiting\n",
      "802c11f474f2: Waiting\n",
      "78a96bfc2e15: Waiting\n",
      "cfbfdad48433: Waiting\n",
      "e74e07ced7b5: Waiting\n",
      "982f1e01177f: Waiting\n",
      "d704d5405378: Waiting\n",
      "25cf9530b13c: Verifying Checksum\n",
      "25cf9530b13c: Download complete\n",
      "c7d3a8feeced: Download complete\n",
      "2ab09b027e7f: Verifying Checksum\n",
      "2ab09b027e7f: Download complete\n",
      "d704d5405378: Verifying Checksum\n",
      "d704d5405378: Download complete\n",
      "e74e07ced7b5: Verifying Checksum\n",
      "e74e07ced7b5: Download complete\n",
      "982f1e01177f: Verifying Checksum\n",
      "982f1e01177f: Download complete\n",
      "802c11f474f2: Verifying Checksum\n",
      "802c11f474f2: Download complete\n",
      "78a96bfc2e15: Verifying Checksum\n",
      "78a96bfc2e15: Download complete\n",
      "cfbfdad48433: Verifying Checksum\n",
      "cfbfdad48433: Download complete\n",
      "e90cce37d13d: Verifying Checksum\n",
      "e90cce37d13d: Download complete\n",
      "af599ce703be: Verifying Checksum\n",
      "af599ce703be: Download complete\n",
      "b3171aef3ea9: Verifying Checksum\n",
      "b3171aef3ea9: Download complete\n",
      "fcd3e0c595bb: Verifying Checksum\n",
      "fcd3e0c595bb: Download complete\n",
      "363aac61da30: Verifying Checksum\n",
      "363aac61da30: Download complete\n",
      "f376fb1d6697: Verifying Checksum\n",
      "f376fb1d6697: Download complete\n",
      "b68d68ffee50: Verifying Checksum\n",
      "b68d68ffee50: Download complete\n",
      "f9af1b6b39a5: Verifying Checksum\n",
      "f9af1b6b39a5: Download complete\n",
      "ebd99ebb20bf: Download complete\n",
      "caeb9ac3c484: Verifying Checksum\n",
      "caeb9ac3c484: Download complete\n",
      "2ab09b027e7f: Pull complete\n",
      "f834e031e1e5: Verifying Checksum\n",
      "f834e031e1e5: Download complete\n",
      "c7d3a8feeced: Pull complete\n",
      "570a72d5008b: Verifying Checksum\n",
      "570a72d5008b: Download complete\n",
      "25cf9530b13c: Pull complete\n",
      "e74e07ced7b5: Pull complete\n",
      "982f1e01177f: Pull complete\n",
      "d704d5405378: Pull complete\n",
      "e90cce37d13d: Pull complete\n",
      "570a72d5008b: Pull complete\n",
      "802c11f474f2: Pull complete\n",
      "78a96bfc2e15: Pull complete\n",
      "cfbfdad48433: Pull complete\n",
      "b3171aef3ea9: Pull complete\n",
      "af599ce703be: Pull complete\n",
      "f834e031e1e5: Pull complete\n",
      "fcd3e0c595bb: Pull complete\n",
      "363aac61da30: Pull complete\n",
      "f376fb1d6697: Pull complete\n",
      "b68d68ffee50: Pull complete\n",
      "f9af1b6b39a5: Pull complete\n",
      "ebd99ebb20bf: Pull complete\n",
      "caeb9ac3c484: Pull complete\n",
      "Digest: sha256:0d1c08feb314816488317273c591534b083aba7f39d35a15e8620711be5dbc1c\n",
      "Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\n",
      " ---> 102a1cdf38c3\n",
      "Step 2/5 : RUN pip install tensorflow-datasets\n",
      " ---> Running in 104480690c70\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.8.2-py3-none-any.whl (5.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 25.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (1.4.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (8.1.3)\n",
      "Collecting dill (from tensorflow-datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 19.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (0.1.8)\n",
      "Collecting etils[enp,epath]>=0.9.0 (from tensorflow-datasets)\n",
      "  Downloading etils-0.9.0-py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.1/140.1 kB 18.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (1.21.6)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (3.20.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (5.9.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (2.29.0)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.3/52.3 kB 8.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (2.3.0)\n",
      "Collecting toml (from tensorflow-datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (4.63.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (4.5.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets) (5.12.0)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.7/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (2022.12.7)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->tensorflow-datasets) (6.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.56.4)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21484 sha256=fa37f18796d6b30c903cf905c129be1061a1f3ca850ca00362bfe8777c10bcf4\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "Successfully built promise\n",
      "Installing collected packages: toml, promise, etils, dill, tensorflow-metadata, tensorflow-datasets\n",
      "Successfully installed dill-0.3.7 etils-0.9.0 promise-2.3 tensorflow-datasets-4.8.2 tensorflow-metadata-1.12.0 toml-0.10.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 104480690c70\n",
      " ---> 678d9ec50b13\n",
      "Step 3/5 : WORKDIR /root\n",
      " ---> Running in 46b3a3e87254\n",
      "Removing intermediate container 46b3a3e87254\n",
      " ---> 106be099c6f0\n",
      "Step 4/5 : COPY task.py /root/task.py\n",
      " ---> dfc5b8c96cef\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"task.py\"]\n",
      " ---> Running in 3b168b763f47\n",
      "Removing intermediate container 3b168b763f47\n",
      " ---> d8e6a6361909\n",
      "Successfully built d8e6a6361909\n",
      "Successfully tagged us-central1-docker.pkg.dev/ai-hangsik/tensorboard/tensorboard-custom-container:v1\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/ai-hangsik/tensorboard/tensorboard-custom-container:v1\n",
      "The push refers to repository [us-central1-docker.pkg.dev/ai-hangsik/tensorboard/tensorboard-custom-container]\n",
      "63d1858fa04b: Preparing\n",
      "b127d94d8d25: Preparing\n",
      "e42695c7b436: Preparing\n",
      "e42695c7b436: Preparing\n",
      "72f0f663075e: Preparing\n",
      "ba4e11d7f54b: Preparing\n",
      "2f17e52b45ce: Preparing\n",
      "180cdfc80d38: Preparing\n",
      "180cdfc80d38: Preparing\n",
      "c43fabf872d0: Preparing\n",
      "e8a91ee696d4: Preparing\n",
      "1282997dbade: Preparing\n",
      "19bc593c4187: Preparing\n",
      "46f2b9d373ec: Preparing\n",
      "6fbd4db77b25: Preparing\n",
      "c64a2c4fdc07: Preparing\n",
      "c64a2c4fdc07: Preparing\n",
      "1c2a71b9e02b: Preparing\n",
      "6a0b55b7bc89: Preparing\n",
      "3cad188888a2: Preparing\n",
      "3872456d0cc6: Preparing\n",
      "27cdcf5ad044: Preparing\n",
      "d446b174c6ca: Preparing\n",
      "aa5fbd63d5ec: Preparing\n",
      "64c29a01f7fa: Preparing\n",
      "b93c1bd012ab: Preparing\n",
      "6fbd4db77b25: Waiting\n",
      "c64a2c4fdc07: Waiting\n",
      "1c2a71b9e02b: Waiting\n",
      "6a0b55b7bc89: Waiting\n",
      "3cad188888a2: Waiting\n",
      "3872456d0cc6: Waiting\n",
      "27cdcf5ad044: Waiting\n",
      "d446b174c6ca: Waiting\n",
      "aa5fbd63d5ec: Waiting\n",
      "64c29a01f7fa: Waiting\n",
      "b93c1bd012ab: Waiting\n",
      "180cdfc80d38: Waiting\n",
      "c43fabf872d0: Waiting\n",
      "2f17e52b45ce: Waiting\n",
      "1282997dbade: Waiting\n",
      "19bc593c4187: Waiting\n",
      "e8a91ee696d4: Waiting\n",
      "46f2b9d373ec: Waiting\n",
      "72f0f663075e: Layer already exists\n",
      "e42695c7b436: Layer already exists\n",
      "ba4e11d7f54b: Layer already exists\n",
      "c43fabf872d0: Layer already exists\n",
      "2f17e52b45ce: Layer already exists\n",
      "180cdfc80d38: Layer already exists\n",
      "1282997dbade: Layer already exists\n",
      "e8a91ee696d4: Layer already exists\n",
      "19bc593c4187: Layer already exists\n",
      "c64a2c4fdc07: Layer already exists\n",
      "6fbd4db77b25: Layer already exists\n",
      "46f2b9d373ec: Layer already exists\n",
      "63d1858fa04b: Pushed\n",
      "1c2a71b9e02b: Layer already exists\n",
      "6a0b55b7bc89: Layer already exists\n",
      "3cad188888a2: Layer already exists\n",
      "3872456d0cc6: Layer already exists\n",
      "27cdcf5ad044: Layer already exists\n",
      "64c29a01f7fa: Layer already exists\n",
      "d446b174c6ca: Layer already exists\n",
      "aa5fbd63d5ec: Layer already exists\n",
      "b93c1bd012ab: Layer already exists\n",
      "b127d94d8d25: Pushed\n",
      "v1: digest: sha256:3d3c8829e842e1aa9acf2165d70cb7bc88411aa5ed48b9f7647b5f74fb9aaadd size: 5757\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                    IMAGES                                                                             STATUS\n",
      "f2f48ac2-6636-4345-b89d-c73901d3f6c0  2025-02-21T01:09:21+00:00  1M46S     gs://ai-hangsik_cloudbuild/source/1740100161.370207-a82e56bab1bb453d9cc6cf781b2dfbbd.tgz  us-central1-docker.pkg.dev/ai-hangsik/tensorboard/tensorboard-custom-container:v1  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME = \"tensorboard-custom-container\"\n",
    "IMAGE_TAG = \"v1\"\n",
    "IMAGE_URI = \"{}-docker.pkg.dev/{}/{}/{}:{}\".format(\n",
    "    LOCATION, PROJECT_ID, DOCKER_REPOSITORY, IMAGE_NAME, IMAGE_TAG\n",
    ")\n",
    "\n",
    "! gcloud builds submit --project {PROJECT_ID} --region={LOCATION} --tag {IMAGE_URI} --timeout=20m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qXFUiHLoFRw"
   },
   "source": [
    "## Setup service account and permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mpKjfsXumuNV",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE_ACCOUNT: 721521243942-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# @title Service account\n",
    "shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(f\"SERVICE_ACCOUNT: {SERVICE_ACCOUNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "c7798d69970b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://mlops-0221/\n",
      "No changes made to gs://mlops-0221/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svUGBOow_Obj"
   },
   "source": [
    "## Create a custom training job with your container\n",
    "Create a TensorBoard instance to be used by the custom training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OAe1xJeS_X3F",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensorboard\n",
      "Create Tensorboard backing LRO: projects/721521243942/locations/us-central1/tensorboards/8757086747701018624/operations/311680657069703168\n",
      "Tensorboard created. Resource name: projects/721521243942/locations/us-central1/tensorboards/8757086747701018624\n",
      "To use this Tensorboard in another session:\n",
      "tb = aiplatform.Tensorboard('projects/721521243942/locations/us-central1/tensorboards/8757086747701018624')\n",
      "TensorBoard resource name: projects/721521243942/locations/us-central1/tensorboards/8757086747701018624\n"
     ]
    }
   ],
   "source": [
    "TENSORBOARD_NAME = \"tensorboard-test\"  # @param {type:\"string\"}\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_NAME, project=PROJECT_ID, location=LOCATION\n",
    ")\n",
    "\n",
    "TENSORBOARD_RESOURCE_NAME = tensorboard.gca_resource.name\n",
    "print(\"TensorBoard resource name:\", TENSORBOARD_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mudxBDal_a_k"
   },
   "source": [
    "Run the following example request to create your own custom training job using the container you just built and uploaded to Artifact Registry, and stream the training results to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wugHEC8czuBe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://mlops-0221/aiplatform-custom-training-2025-02-21-01:13:45.783 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8786164397849444352?project=721521243942\n",
      "CustomContainerTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/8786164397849444352 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1005852993939046400?project=721521243942\n",
      "View tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+721521243942+locations+us-central1+tensorboards+8757086747701018624+experiments+1005852993939046400\n",
      "CustomContainerTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/8786164397849444352 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/8786164397849444352 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/8786164397849444352 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/8786164397849444352 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/8786164397849444352 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/721521243942/locations/us-central1/trainingPipelines/8786164397849444352 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob run completed. Resource name: projects/721521243942/locations/us-central1/trainingPipelines/8786164397849444352\n",
      "Training did not produce a Managed Model returning None. Training Pipeline projects/721521243942/locations/us-central1/trainingPipelines/8786164397849444352 is not configured to upload a Model. Create the Training Pipeline with model_serving_container_image_uri and model_display_name passed in. Ensure that your training script saves to model to os.environ['AIP_MODEL_DIR'].\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"tensorboard-example-job-{}\".format(UUID)\n",
    "BASE_OUTPUT_DIR = \"{}/{}\".format(BUCKET_URI, JOB_NAME)\n",
    "\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    container_uri=IMAGE_URI,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    staging_bucket=BUCKET_URI,\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    replica_count=1,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=TENSORBOARD_RESOURCE_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfMsn_RnEtnj"
   },
   "source": [
    "In Google Cloud console, you can monitor your training job at Vertex AI > Training > Custom Jobs. In each custom training job, near real time updated TensorBoard is available at `OPEN TENSORBOARD` button. \n",
    "Learn more see [View Vertex AI TensorBoard data](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bmbjz-nUW7KE"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, **if you created the individual resources in the notebook** you can delete them as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LSKjrRqW-Bk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Delete docker repository.\n",
    "! gcloud artifacts repositories delete $DOCKER_REPOSITORY --project {PROJECT_ID} --location {LOCATION} --quiet\n",
    "\n",
    "# Delete TensorBoard instance.\n",
    "! gcloud ai tensorboards delete {TENSORBOARD_RESOURCE_NAME}\n",
    "\n",
    "# Delete custom job.\n",
    "job.delete()\n",
    "\n",
    "# Delete GCS bucket.\n",
    "delete_bucket = False\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tensorboard_custom_training_with_custom_container.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
