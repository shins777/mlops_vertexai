{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 설치"
      ],
      "metadata": {
        "id": "kKNYeRJ3-CMs"
      },
      "id": "kKNYeRJ3-CMs"
    },
    {
      "cell_type": "code",
      "id": "IfoVCiiCFX0KTeVLvBkDkVG9",
      "metadata": {
        "tags": [],
        "id": "IfoVCiiCFX0KTeVLvBkDkVG9"
      },
      "source": [
        "!pip install --upgrade pip setuptools wheel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --no-build-isolation langsmith"
      ],
      "metadata": {
        "id": "o69VItNz-HkH"
      },
      "id": "o69VItNz-HkH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --user --quiet  google-cloud-aiplatform \\\n",
        "                             google-cloud-storage \\\n",
        "                             google-cloud-pipeline-components \\\n",
        "                             kfp"
      ],
      "metadata": {
        "id": "gIj7SXRa-Hmk"
      },
      "id": "gIj7SXRa-Hmk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 환경 변수 설정"
      ],
      "metadata": {
        "id": "FbJ0jgRk-MF_"
      },
      "id": "FbJ0jgRk-MF_"
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"ureca-poc-itcen\"\n",
        "LOCATION = \"us-central1\"\n",
        "BUCKET_URI = \"gs://ureca_test\"\n",
        "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline/exp\"\n",
        "MODEL_URI = f\"{PIPELINE_ROOT}/compare-pipeline\"\n",
        "DATASET_ID = \"test_data\"\n",
        "TABLE_ID = \"amazon_review_v1\"\n",
        "\n",
        "\n",
        "SERVICE_ACCOUNT = \"vertex-api@ureca-poc-itcen.iam.gserviceaccount.com\""
      ],
      "metadata": {
        "id": "liIKUxhO-Ho7"
      },
      "id": "liIKUxhO-Ho7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 불러오기"
      ],
      "metadata": {
        "id": "n1Pibfiy-k2r"
      },
      "id": "n1Pibfiy-k2r"
    },
    {
      "cell_type": "code",
      "source": [
        "# 일반 라이브러리\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import uuid\n",
        "from typing import NamedTuple, List\n",
        "\n",
        "# Vertex AI 라이브러리\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from google.cloud.aiplatform_v1.types.pipeline_state import PipelineState\n",
        "\n",
        "# Kubeflow Pipelines(KFP) 관련 라이브러리\n",
        "from kfp import compiler, dsl, client\n",
        "from kfp import components\n",
        "from kfp.dsl import (\n",
        "    Artifact, Metrics, Dataset, Input, Model, Output, component\n",
        ")\n",
        "import kfp.v2.dsl as dsl\n",
        "\n",
        "# 로깅 설정\n",
        "logger = logging.getLogger(\"logger\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Vertex AI 초기화\n",
        "vertex_ai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "nlIEp5fv-HrI"
      },
      "id": "nlIEp5fv-HrI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 빌드된 학습용 컨테이너 이미지 URI\n",
        "TRAIN_IMAGE = vertex_ai.helpers.get_prebuilt_prediction_container_uri(\n",
        "    framework=\"sklearn\",\n",
        "    framework_version=\"1.3\",\n",
        "    accelerator=\"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "RnJ41nOs-HvM"
      },
      "id": "RnJ41nOs-HvM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 컴포넌트 파이프라인"
      ],
      "metadata": {
        "id": "bF7ht4kE-3rE"
      },
      "id": "bF7ht4kE-3rE"
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 생성 컴포넌트\n",
        "@component(\n",
        "    base_image=\"python:3.10\",\n",
        "    packages_to_install=[\"pandas\", \"google-cloud-bigquery\", \"db-dtypes\"]\n",
        ")\n",
        "\n",
        "def create_dataset(\n",
        "    project_id: str,\n",
        "    dataset_id: str,\n",
        "    table_id: str,\n",
        "    in_test_size: float,\n",
        "    train_data_artifact: Output[Artifact],     # (출력) 학습 데이터\n",
        "    test_data_artifact: Output[Artifact],      # (출력) 테스트 데이터\n",
        "    metadata_artifact: Output[Artifact]        # (출력) 사용자/아이템 인덱스 메타데이터\n",
        "):\n",
        "    # 라이브러리\n",
        "    import pandas as pd\n",
        "    from google.cloud import bigquery\n",
        "    import os\n",
        "    import json\n",
        "\n",
        "    # BigQuery 클라이언트 초기화\n",
        "    client = bigquery.Client(project=project_id, location=\"us-central1\")\n",
        "\n",
        "    # 쿼리\n",
        "    query = f\"\"\"\n",
        "        SELECT customer_id, product_id, star_rating\n",
        "        FROM `{project_id}.{dataset_id}.{table_id}`\n",
        "    \"\"\"\n",
        "\n",
        "    df = client.query(query).to_dataframe()\n",
        "\n",
        "    user_ids = {user: i for i, user in enumerate(df['customer_id'].unique())}\n",
        "    item_ids = {item: i for i, item in enumerate(df['product_id'].unique())}\n",
        "\n",
        "    df['user_idx'] = df['customer_id'].map(user_ids)\n",
        "    df['item_idx'] = df['product_id'].map(item_ids)\n",
        "\n",
        "    train_df = df.sample(frac=1 - in_test_size, random_state=0)\n",
        "    test_df = df.drop(train_df.index)\n",
        "\n",
        "    # 학습 데이터 저장\n",
        "    os.makedirs(train_data_artifact.path, exist_ok=True)\n",
        "    train_df.to_csv(os.path.join(train_data_artifact.path, \"train.csv\"), index=False)\n",
        "\n",
        "    # 테스트 데이터 저장\n",
        "    os.makedirs(test_data_artifact.path, exist_ok=True)\n",
        "    test_df.to_csv(os.path.join(test_data_artifact.path, \"test.csv\"), index=False)\n",
        "\n",
        "    # 메타데이터 저장\n",
        "    os.makedirs(metadata_artifact.path, exist_ok=True)\n",
        "    with open(os.path.join(metadata_artifact.path, \"ids.json\"), 'w') as f:\n",
        "        json.dump({\n",
        "            \"user_ids\": user_ids,\n",
        "            \"item_ids\": item_ids\n",
        "        }, f)\n",
        "\n",
        "# 컴포넌트 YAML 파일로 컴파일\n",
        "compiler.Compiler().compile(create_dataset, \"create_dataset.yaml\")\n"
      ],
      "metadata": {
        "id": "NMwI8CXO-3Lw"
      },
      "id": "NMwI8CXO-3Lw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 컴포넌트\n",
        "@component(\n",
        "    base_image=\"python:3.10\",\n",
        "    packages_to_install=[\"numpy\", \"pandas\", \"joblib\"]\n",
        ")\n",
        "\n",
        "def train_model(\n",
        "    train_data: Input[Artifact],           # (입력) 학습 데이터셋\n",
        "    metadata: Input[Artifact],             # (입력) 메타데이터\n",
        "    n_factors: int,\n",
        "    learning_rate: float,\n",
        "    reg: float,\n",
        "    num_epochs: int,\n",
        "    model_artifact: Output[Model]          # (출력) 모델\n",
        "):\n",
        "    # 라이브러리\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import os\n",
        "    import json\n",
        "    import joblib\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # 데이터 불러오기\n",
        "    df = pd.read_csv(os.path.join(train_data.path, \"train.csv\"))\n",
        "    with open(os.path.join(metadata.path, \"ids.json\")) as f:\n",
        "        ids = json.load(f)\n",
        "\n",
        "    num_users = len(ids['user_ids'])\n",
        "    num_items = len(ids['item_ids'])\n",
        "\n",
        "    P = np.random.normal(0, 0.1, (num_users, n_factors))\n",
        "    Q = np.random.normal(0, 0.1, (num_items, n_factors))\n",
        "\n",
        "    # 학습\n",
        "    for epoch in range(num_epochs):\n",
        "        for _, row in df.iterrows():\n",
        "            u, i, r = row['user_idx'], row['item_idx'], row['star_rating']\n",
        "            pred = np.dot(P[u], Q[i].T)\n",
        "            err = r - pred\n",
        "\n",
        "            # 파라미터 업데이트\n",
        "            P[u] += learning_rate * (err * Q[i] - reg * P[u])\n",
        "            Q[i] += learning_rate * (err * P[u] - reg * Q[i])\n",
        "\n",
        "    # 학습된 행렬과 메타데이터 저장\n",
        "    os.makedirs(model_artifact.path, exist_ok=True)\n",
        "    np.save(os.path.join(model_artifact.path, \"P.npy\"), P)\n",
        "    np.save(os.path.join(model_artifact.path, \"Q.npy\"), Q)\n",
        "    joblib.dump(ids, os.path.join(model_artifact.path, \"metadata.joblib\"))\n",
        "\n",
        "# 컴포넌트 YAML 파일로 컴파일\n",
        "compiler.Compiler().compile(train_model, \"train_model.yaml\")\n"
      ],
      "metadata": {
        "id": "CbMiluy06r0v"
      },
      "id": "CbMiluy06r0v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가 컴포넌트\n",
        "@component(\n",
        "    base_image=\"python:3.10\",\n",
        "    packages_to_install=[\"numpy\", \"pandas\", \"joblib\", \"scikit-learn\"]\n",
        ")\n",
        "\n",
        "def evaluate_model(\n",
        "    test_data: Input[Artifact],             # (입력) 테스트 데이터셋\n",
        "    model: Input[Model],                    # (입력) 학습된 모델\n",
        "    evaluate_artifact: Output[Artifact],    # (출력) 평가 결과\n",
        "    metrics: Output[Metrics]\n",
        "):\n",
        "    # 라이브러리\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    import os\n",
        "    import json\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "\n",
        "    # 테스트 데이터 불러오기\n",
        "    df = pd.read_csv(os.path.join(test_data.path, \"test.csv\"))\n",
        "\n",
        "    # 학습된 행렬 불러오기\n",
        "    P = np.load(os.path.join(model.path, \"P.npy\"))\n",
        "    Q = np.load(os.path.join(model.path, \"Q.npy\"))\n",
        "    ids = joblib.load(os.path.join(model.path, \"metadata.joblib\"))\n",
        "\n",
        "    # 예측\n",
        "    predictions = [np.dot(P[row['user_idx']], Q[row['item_idx']]) for _, row in df.iterrows()]\n",
        "\n",
        "    # 평가지표\n",
        "    rmse = np.sqrt(mean_squared_error(df['star_rating'], predictions))\n",
        "\n",
        "    # 평가지표 저장\n",
        "    metrics.log_metric(\"RMSE\", rmse)\n",
        "\n",
        "    # 평가 결과 JSON으로 저장\n",
        "    with open(os.path.join(evaluate_artifact.path, \"metrics.json\"), 'w') as f:\n",
        "        json.dump({\"RMSE\": rmse}, f, indent=4)\n",
        "\n",
        "# 컴포넌트를 YAML로 컴파일\n",
        "compiler.Compiler().compile(evaluate_model, \"evaluate_model.yaml\")\n"
      ],
      "metadata": {
        "id": "kFFwIKUw6sBO"
      },
      "id": "kFFwIKUw6sBO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 분리된 컴포넌트 YAML 파일 로드\n",
        "create_dataset_comp = components.load_component_from_file(\"create_dataset.yaml\")\n",
        "train_model_comp = components.load_component_from_file(\"train_model.yaml\")\n",
        "evaluate_model_comp = components.load_component_from_file(\"evaluate_model.yaml\")\n",
        "\n",
        "# 파이프라인 정의\n",
        "@dsl.pipeline(\n",
        "    name=\"svd_separated_pipeline\",\n",
        "    description=\"SVD 기반 추천 모델을 학습하고 평가하는 파이프라인\"\n",
        ")\n",
        "\n",
        "def pipeline(\n",
        "    project_id: str,\n",
        "    dataset_id: str,\n",
        "    table_id: str,\n",
        "    model_uri: str,\n",
        "    n_factors: int,\n",
        "    learning_rate: float,\n",
        "    reg: float,\n",
        "    num_epochs: int,\n",
        "    in_test_size: float\n",
        "):\n",
        "\n",
        "    # 데이터 생성 및 전처리 컴포넌트 실행\n",
        "    dataset_task = create_dataset_comp(\n",
        "        project_id=project_id,\n",
        "        dataset_id=dataset_id,\n",
        "        table_id=table_id,\n",
        "        in_test_size=in_test_size\n",
        "    )\n",
        "\n",
        "    # 모델 학습 컴포넌트 실행\n",
        "    train_task = train_model_comp(\n",
        "        train_data=dataset_task.outputs[\"train_data_artifact\"],\n",
        "        metadata=dataset_task.outputs[\"metadata_artifact\"],\n",
        "        n_factors=n_factors,\n",
        "        learning_rate=learning_rate,\n",
        "        reg=reg,\n",
        "        num_epochs=num_epochs\n",
        "    )\n",
        "\n",
        "    # 모델 평가 컴포넌트 실행\n",
        "    evaluate_model_comp(\n",
        "        test_data=dataset_task.outputs[\"test_data_artifact\"],\n",
        "        model=train_task.outputs[\"model_artifact\"]\n",
        "    )\n",
        "\n",
        "# 파이프라인 컴파일\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"svd_pipeline.json\")"
      ],
      "metadata": {
        "id": "44MSSZyk-3OG"
      },
      "id": "44MSSZyk-3OG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실험 파라미터 리스트 설정\n",
        "runs = [\n",
        "    {\"n_factors\": 10, \"learning_rate\": 0.01, \"reg\": 0.02, \"num_epochs\": 50, \"in_test_size\": 0.2},\n",
        "    {\"n_factors\": 50, \"learning_rate\": 0.01, \"reg\": 0.1, \"num_epochs\": 50, \"in_test_size\": 0.2},\n",
        "    {\"n_factors\": 100, \"learning_rate\": 0.01, \"reg\": 0.01, \"num_epochs\": 50, \"in_test_size\": 0.2},\n",
        "]"
      ],
      "metadata": {
        "id": "hOYYtKBEF_j0"
      },
      "id": "hOYYtKBEF_j0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실험 설정\n",
        "EXPERIMENT_NAME = \"svd-comparing-exp\"\n",
        "\n",
        "# 파라미터에 대해 파이프라인 반복 실행\n",
        "for i, run in enumerate(runs):\n",
        "\n",
        "    job = vertex_ai.PipelineJob(\n",
        "        display_name=f\"{EXPERIMENT_NAME}-pipeline-run-{i}\",\n",
        "        template_path=\"svd_pipeline.json\",\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        location=\"us-central1\",\n",
        "\n",
        "        parameter_values={\n",
        "            \"project_id\": PROJECT_ID,\n",
        "            \"dataset_id\": DATASET_ID,\n",
        "            \"table_id\": TABLE_ID,\n",
        "            \"model_uri\": MODEL_URI,\n",
        "            **run,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    job.submit(experiment=EXPERIMENT_NAME)\n"
      ],
      "metadata": {
        "id": "b5prH5QgOOWO"
      },
      "id": "b5prH5QgOOWO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3uLA-3SQF_mp"
      },
      "id": "3uLA-3SQF_mp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9KIWcQRU-3Q2"
      },
      "id": "9KIWcQRU-3Q2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "experiment_modularization.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}