{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007dc29-5d01-4ebd-b143-35b150bdfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Forusone\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15422b-ea7f-4697-8d55-ed001a5c0903",
   "metadata": {},
   "source": [
    "## KFP modularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e0040-1008-4855-a14f-e634a6b4109e",
   "metadata": {},
   "source": [
    "This notebook explains how to moudularize the Kubeflow pipeline with compiled files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e22f1a-09d6-4c3e-aa2b-bd84fc681750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "%pip install --user --quiet  google-cloud-aiplatform \\\n",
    "                             google-cloud-storage \\\n",
    "                             google-cloud-pipeline-components \\\n",
    "                             kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70ea789-440f-4529-8590-49d1453db63e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.10.1\n",
      "google_cloud_pipeline_components version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598b2c50-8139-4802-a60c-fcfb2112db08",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fedcb9fa-586e-4e6a-bf44-0f9ccbf6157a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "PROJECT_ID=\"ai-hangsik\"\n",
    "LOCATION=\"us-central1\"\n",
    "\n",
    "# For only colab user, no need this process for Colab Enterprise in Vertex AI.\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user(project_id=PROJECT_ID)\n",
    "\n",
    "# set project.\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4760d65-576c-4219-b793-880e8d5dbf94",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f501f30-9331-494b-83df-8310f950ef2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from kfp import compiler, dsl\n",
    "from kfp import client,compiler, dsl\n",
    "from kfp.dsl import Artifact, Metrics, Dataset, Input, Model, Output, component\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82061b0e-01c4-423f-8a79-05d3fc7ebe9e",
   "metadata": {},
   "source": [
    "### Create a bucket for pipeline root to store artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef0b93d-8568-455a-bad1-25c7722a7001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://mlops-poc-0303/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'mlops-poc-0303' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "# Create a bucket.\n",
    "BUCKET_URI = f\"gs://mlops-poc-0303\"\n",
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9409d1d4-795d-470b-bce6-b9e3e41a187d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline/iris/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5824db-29a8-4dae-9ce1-ecd42bd1ffbf",
   "metadata": {},
   "source": [
    "### Set access to the service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10261014-2d6e-4d30-bb73-1e7d6837cd85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE_ACCOUNT: 721521243942-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(f\"SERVICE_ACCOUNT: {SERVICE_ACCOUNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d84b01f-0a19-43d2-8a26-17db94008ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://mlops-poc-0303/\n",
      "No changes made to gs://mlops-poc-0303/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewerroles/logging.logWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9f1b7b-2da9-4de7-bcc1-1c288f368201",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b996191d-150e-4076-961e-e98892f9a3a7",
   "metadata": {},
   "source": [
    "### Component 1 : Template to create dataset. \n",
    "Generate a yaml file inclusing code to create dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7712ca8d-9076-4ac0-a3bc-a278f5d9f7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image = \"python:3.10\",\n",
    "               packages_to_install=['pandas'])\n",
    "\n",
    "def create_dataset(\n",
    "                data_args : dict,\n",
    "                data_artifact : Output[Artifact],\n",
    "                dataset: Output[Dataset]\n",
    "):\n",
    "        \n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import ast\n",
    "    \n",
    "    csv_url = data_args['csv_url']\n",
    "    \n",
    "    # convert str to list.\n",
    "    col_names = ast.literal_eval(data_args['col_names'])\n",
    "\n",
    "    # Write a data args with type of Artifact\n",
    "    with open(data_artifact.path, 'w') as f:\n",
    "        f.write(json.dumps(data_args))\n",
    "    \n",
    "    df = pd.read_csv(csv_url, names=col_names)\n",
    "    \n",
    "    # Write a dataset with type of Dataset.\n",
    "    with open(dataset.path, 'w') as f:\n",
    "        df.to_csv(f)\n",
    "\n",
    "compiler.Compiler().compile(create_dataset, \"create_dataset.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693577f-76f4-4fee-8f39-16329f79b0ac",
   "metadata": {},
   "source": [
    "### Component 2 : Template to normalize dataset. \n",
    "Generate a yaml file inclusing code to normalize dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56df483e-df27-43dd-92d2-9f4ea9453646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image = \"python:3.10\",\n",
    "               packages_to_install=['pandas', 'scikit-learn'])\n",
    "\n",
    "def normalize_dataset(\n",
    "    input_dataset: Input[Dataset],\n",
    "    normalized_dataset: Output[Dataset],\n",
    "    normalize_args:dict,\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    standard_scaler = normalize_args['standard_scaler']\n",
    "    min_max_scaler = normalize_args['min_max_scaler']\n",
    "\n",
    "    if standard_scaler is min_max_scaler:\n",
    "        raise ValueError(\n",
    "            'Exactly one of standard_scaler or min_max_scaler must be True.')\n",
    "    \n",
    "    with open(input_dataset.path) as f:\n",
    "        df = pd.read_csv(f)\n",
    "    labels = df.pop('Labels')\n",
    "    \n",
    "    if standard_scaler:\n",
    "        scaler = StandardScaler()\n",
    "    if min_max_scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    df = pd.DataFrame(scaler.fit_transform(df))\n",
    "    df['Labels'] = labels\n",
    "    with open(normalized_dataset.path, 'w') as f:\n",
    "        df.to_csv(f)\n",
    "\n",
    "compiler.Compiler().compile(normalize_dataset, \"normalize_dataset.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ee62d-2de3-4e1b-be25-67314df9b2be",
   "metadata": {},
   "source": [
    "### Component 3 : Template to train a model\n",
    "Generate a yaml file inclusing code to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e4e193-ea07-4f38-a3d0-982ecf14e41d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.10\",\n",
    "               packages_to_install=['pandas', 'scikit-learn'])\n",
    "\n",
    "def train_model(\n",
    "    train_args:dict,\n",
    "    normalized_dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "\n",
    "):\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    test_size = float(train_args['hyper_params']['in_test_size'].strip())\n",
    "    n_neighbors = int(train_args['hyper_params']['n_neighbors'].strip())\n",
    "    \n",
    "    with open(normalized_dataset.path) as f:\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "    y = df.pop('Labels')\n",
    "    X = df\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size= test_size)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    metrics.log_metric(\"accuracy\", 99)\n",
    "    metrics.log_metric(\"framework\", \"sklearn\")\n",
    "    metrics.log_metric(\"dataset_size\", 100)\n",
    "    metrics.log_metric(\"AUC\", 0.4)    \n",
    "    \n",
    "    print(f\"Metrics URI : {metrics.uri}\")\n",
    "    \n",
    "    # Export the model to a file\n",
    "    os.makedirs(model.path, exist_ok=True)\n",
    "    joblib.dump(clf, os.path.join(model.path, \"model.joblib\"))        \n",
    "        \n",
    "compiler.Compiler().compile(train_model, \"train_model.yaml\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e83baa-40f1-4689-b990-1939edc12881",
   "metadata": {},
   "source": [
    "### Component 4 : Template to deploy a model\n",
    "Generate a yaml file inclusing code to deploy a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f4bb916-da13-480a-b894-093123a23a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.10\",\n",
    "               packages_to_install=['google-cloud-aiplatform', 'pandas', 'scikit-learn'])\n",
    "\n",
    "def deploy_model(\n",
    "    deploy_args:dict,\n",
    "    model: Input[Model],\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model],\n",
    "):\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    project_id = deploy_args['project_id']\n",
    "    display_name = deploy_args['display_name']\n",
    "    container_image_uri = deploy_args['container_image_uri']\n",
    "    machine_type = deploy_args['machine_type']    \n",
    "    \n",
    "    aiplatform.init(project=project_id)\n",
    "\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=display_name,\n",
    "        artifact_uri=model.uri,\n",
    "        serving_container_image_uri=container_image_uri,\n",
    "    )\n",
    "    \n",
    "    endpoint = deployed_model.deploy(machine_type=machine_type)\n",
    "\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name\n",
    "\n",
    "compiler.Compiler().compile(deploy_model, \"deploy_model.yaml\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0c4848-663b-49f9-8c1e-6623af346082",
   "metadata": {},
   "source": [
    "### Build a pipeline\n",
    "This is a wrapper function for pipeline buider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1da3f114-4baa-42c3-bf56-bcf5df9e6bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_pipeline (pipeline_name:str,\n",
    "                    pipeline_desc:str,\n",
    "                    pipeline_root:str, \n",
    "                    component_yaml_files : dict,\n",
    "                    \n",
    "#                     create_dataset_file:str,\n",
    "#                     normalize_dataset_file:str,\n",
    "#                     train_model_file:str,\n",
    "#                     deploy_model_file:str,\n",
    "                                 \n",
    "                    ):\n",
    "\n",
    "    @dsl.pipeline(\n",
    "        name = pipeline_name,\n",
    "        description = pipeline_desc,\n",
    "        pipeline_root = pipeline_root,)\n",
    "\n",
    "    def module_pipeline(\n",
    "                        data_args: dict,\n",
    "                        normalize_args:dict,\n",
    "                        train_args:dict,\n",
    "                        deploy_args:dict,\n",
    "\n",
    "        ):\n",
    "\n",
    "        from kfp import components\n",
    "\n",
    "        create_dataset_comp = components.load_component_from_file(component_yaml_files['create_dataset_file'])\n",
    "        normalize_dataset_comp = components.load_component_from_file(component_yaml_files['normalize_dataset_file'])\n",
    "        train_model_comp = components.load_component_from_file(component_yaml_files['train_model_file'])\n",
    "        deploy_model_comp = components.load_component_from_file(component_yaml_files['deploy_model_file'])\n",
    "\n",
    "        # 1. create dataset\n",
    "        create_dataset_task = create_dataset_comp(data_args=data_args)\n",
    "        \n",
    "        # 2. normalize dataset\n",
    "        normalize_dataset_task = normalize_dataset_comp(\n",
    "            input_dataset=create_dataset_task.outputs['dataset'],\n",
    "            normalize_args = normalize_args,\n",
    "\n",
    "        )\n",
    "\n",
    "        # 3. model training\n",
    "        train_model_task = train_model_comp(\n",
    "            normalized_dataset=normalize_dataset_task.outputs['normalized_dataset'],\n",
    "            train_args = train_args\n",
    "        )\n",
    "\n",
    "        # 4. deploy model\n",
    "        deploy_model_comp(model= train_model_task.outputs['model'],\n",
    "                          deploy_args = deploy_args,\n",
    "                    )\n",
    "            \n",
    "    compiler.Compiler().compile(pipeline_func=module_pipeline, package_path=\"kfp_module_pipeline.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8016b23b-785b-482e-8a18-e9bf29b1cad9",
   "metadata": {},
   "source": [
    "### Build a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcaa4639-b1af-4d4f-b82f-7e1347b233d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "component_yaml_files = {'create_dataset_file' : 'create_dataset.yaml',\n",
    "                          'normalize_dataset_file' : 'normalize_dataset.yaml',\n",
    "                          'train_model_file' : 'train_model.yaml' ,\n",
    "                          'deploy_model_file' : 'deploy_model.yaml',\n",
    "            }\n",
    "\n",
    "build_pipeline(pipeline_name = \"kfp_module_pipeline\",\n",
    "               pipeline_desc = \"desc for kfp_module_pipeline\",\n",
    "               pipeline_root = PIPELINE_ROOT,\n",
    "               component_yaml_files = component_yaml_files, \n",
    "               # create_dataset_file = 'create_dataset.yaml',\n",
    "               # normalize_dataset_file = 'normalize_dataset.yaml',\n",
    "               # train_model_file = 'train_model.yaml' ,\n",
    "               # deploy_model_file = 'deploy_model.yaml',\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d570ebe5-72c3-4149-bae1-1a3960f31a96",
   "metadata": {},
   "source": [
    "### Run a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "116c5820-51d7-4b47-a2e1-e68bed0b7624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311015708\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311015708')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/kfp-module-pipeline-20250311015708?project=721521243942\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311015708 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311015708 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311015708 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311015708 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311015708 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \" The DAG failed because some tasks failed. The failed tasks are: [train-model].; Job (project_id = ai-hangsik, job_id = 4658415400046821376) is failed due to the above error.; Failed to handle the job: {project_number = 721521243942, job_id = 4658415400046821376}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 37\u001b[0m\n\u001b[1;32m      1\u001b[0m job \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mPipelineJob(\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkfp_module_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     enable_caching \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice_account\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSERVICE_ACCOUNT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:334\u001b[0m, in \u001b[0;36mPipelineJob.run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout, enable_preflight_validations)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m        Optional. Whether to enable preflight validations for the PipelineJob.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_preflight_validations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_preflight_validations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:382\u001b[0m, in \u001b[0;36mPipelineJob._run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout, enable_preflight_validations)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to ensure network synchronization and to run\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03mthe configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m        Optional. Whether to enable preflight validations for the PipelineJob.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    375\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[1;32m    376\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     enable_preflight_validations\u001b[38;5;241m=\u001b[39menable_preflight_validations,\n\u001b[1;32m    380\u001b[0m )\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# AutoSxS view model evaluations\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m details \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_details:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:793\u001b[0m, in \u001b[0;36mPipelineJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \" The DAG failed because some tasks failed. The failed tasks are: [train-model].; Job (project_id = ai-hangsik, job_id = 4658415400046821376) is failed due to the above error.; Failed to handle the job: {project_number = 721521243942, job_id = 4658415400046821376}\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "\n",
    "    display_name=\"kfp_module_pipeline\",\n",
    "    template_path=\"kfp_module_pipeline.yaml\",\n",
    "    \n",
    "    parameter_values = {\n",
    "        'data_args' : {\n",
    "            'csv_url': 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "            'col_names': \"['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels']\",\n",
    "        },\n",
    "        \n",
    "        'normalize_args' : {\n",
    "            'standard_scaler': False,\n",
    "            'min_max_scaler': True,\n",
    "        },\n",
    "        \n",
    "        'train_args' : {\n",
    "            'hyper_params' : {\n",
    "                'n_neighbors': \"3\",\n",
    "                'in_test_size' : \"0.2\"\n",
    "            },\n",
    "        },\n",
    "        \n",
    "        'deploy_args' : {\n",
    "            'project_id' : 'ai-hangsik',\n",
    "            'display_name' : 'kfp_module_model',\n",
    "            'container_image_uri': 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-5:latest',\n",
    "            'machine_type': 'e2-standard-4',\n",
    "        },\n",
    "        \n",
    "    },\n",
    "    \n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching = True\n",
    ")\n",
    "\n",
    "job.run(service_account = SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ab3ae-ed80-4208-8587-14b24b7800eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3be82d1c-9f6a-4fdd-9d51-9853a89cf4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "a = \"['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels']\"\n",
    "\n",
    "b = ast.literal_eval(a)\n",
    "\n",
    "b\n",
    "type(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c5eba6-e98f-467d-984b-26e4bb2ab402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
