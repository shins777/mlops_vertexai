{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007dc29-5d01-4ebd-b143-35b150bdfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Forusone\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15422b-ea7f-4697-8d55-ed001a5c0903",
   "metadata": {},
   "source": [
    "## KFP modularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e0040-1008-4855-a14f-e634a6b4109e",
   "metadata": {},
   "source": [
    "This notebook explains how to moudularize the Kubeflow pipeline with compiled files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e22f1a-09d6-4c3e-aa2b-bd84fc681750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "%pip install --user --quiet  google-cloud-aiplatform \\\n",
    "                             google-cloud-storage \\\n",
    "                             google-cloud-pipeline-components \\\n",
    "                             kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70ea789-440f-4529-8590-49d1453db63e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.10.1\n",
      "google_cloud_pipeline_components version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598b2c50-8139-4802-a60c-fcfb2112db08",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fedcb9fa-586e-4e6a-bf44-0f9ccbf6157a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "PROJECT_ID=\"ai-hangsik\"\n",
    "LOCATION=\"us-central1\"\n",
    "\n",
    "# For only colab user, no need this process for Colab Enterprise in Vertex AI.\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user(project_id=PROJECT_ID)\n",
    "\n",
    "# set project.\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4760d65-576c-4219-b793-880e8d5dbf94",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f501f30-9331-494b-83df-8310f950ef2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from kfp import compiler, dsl\n",
    "from kfp import client,compiler, dsl\n",
    "from kfp.dsl import Artifact, Metrics, Dataset, Input, Model, Output, component\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82061b0e-01c4-423f-8a79-05d3fc7ebe9e",
   "metadata": {},
   "source": [
    "### Create a bucket for pipeline root to store artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef0b93d-8568-455a-bad1-25c7722a7001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://mlops-poc-0303/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'mlops-poc-0303' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "# Create a bucket.\n",
    "BUCKET_URI = f\"gs://mlops-poc-0303\"\n",
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9409d1d4-795d-470b-bce6-b9e3e41a187d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline/iris/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5824db-29a8-4dae-9ce1-ecd42bd1ffbf",
   "metadata": {},
   "source": [
    "### Set access to the service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10261014-2d6e-4d30-bb73-1e7d6837cd85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE_ACCOUNT: 721521243942-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(f\"SERVICE_ACCOUNT: {SERVICE_ACCOUNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d84b01f-0a19-43d2-8a26-17db94008ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://mlops-poc-0303/\n",
      "No changes made to gs://mlops-poc-0303/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewerroles/logging.logWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9f1b7b-2da9-4de7-bcc1-1c288f368201",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b996191d-150e-4076-961e-e98892f9a3a7",
   "metadata": {},
   "source": [
    "### Component 1 : Template to create dataset. \n",
    "Generate a yaml file inclusing code to create dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7712ca8d-9076-4ac0-a3bc-a278f5d9f7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image = \"python:3.10\",\n",
    "               packages_to_install=['pandas'])\n",
    "\n",
    "def create_dataset(\n",
    "                data_args : dict,\n",
    "                data_artifact : Output[Artifact],\n",
    "                dataset: Output[Dataset]\n",
    "):\n",
    "        \n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import ast\n",
    "    \n",
    "    csv_url = data_args['csv_url']\n",
    "    \n",
    "    # convert str to list.\n",
    "    col_names = ast.literal_eval(data_args['col_names'])\n",
    "\n",
    "    # Write a data args with type of Artifact\n",
    "    with open(data_artifact.path, 'w') as f:\n",
    "        f.write(json.dumps(data_args))\n",
    "    \n",
    "    df = pd.read_csv(csv_url, names=col_names)\n",
    "    \n",
    "    # Write a dataset with type of Dataset.\n",
    "    with open(dataset.path, 'w') as f:\n",
    "        df.to_csv(f)\n",
    "\n",
    "compiler.Compiler().compile(create_dataset, \"create_dataset.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693577f-76f4-4fee-8f39-16329f79b0ac",
   "metadata": {},
   "source": [
    "### Component 2 : Template to normalize dataset. \n",
    "Generate a yaml file inclusing code to normalize dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56df483e-df27-43dd-92d2-9f4ea9453646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image = \"python:3.10\",\n",
    "               packages_to_install=['pandas', 'scikit-learn'])\n",
    "\n",
    "def normalize_dataset(\n",
    "    input_dataset: Input[Dataset],\n",
    "    normalized_dataset: Output[Dataset],\n",
    "    normalize_args:dict,\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    standard_scaler = normalize_args['standard_scaler']\n",
    "    min_max_scaler = normalize_args['min_max_scaler']\n",
    "\n",
    "    if standard_scaler is min_max_scaler:\n",
    "        raise ValueError(\n",
    "            'Exactly one of standard_scaler or min_max_scaler must be True.')\n",
    "    \n",
    "    with open(input_dataset.path) as f:\n",
    "        df = pd.read_csv(f)\n",
    "    labels = df.pop('Labels')\n",
    "    \n",
    "    if standard_scaler:\n",
    "        scaler = StandardScaler()\n",
    "    if min_max_scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    df = pd.DataFrame(scaler.fit_transform(df))\n",
    "    df['Labels'] = labels\n",
    "    with open(normalized_dataset.path, 'w') as f:\n",
    "        df.to_csv(f)\n",
    "\n",
    "compiler.Compiler().compile(normalize_dataset, \"normalize_dataset.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ee62d-2de3-4e1b-be25-67314df9b2be",
   "metadata": {},
   "source": [
    "### Component 3 : Template to train a model\n",
    "Generate a yaml file inclusing code to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e4e193-ea07-4f38-a3d0-982ecf14e41d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.10\",\n",
    "               packages_to_install=['pandas', 'scikit-learn'])\n",
    "\n",
    "def train_model(\n",
    "    train_args:dict,\n",
    "    normalized_dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "\n",
    "):\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    test_size = float(train_args['hyper_params']['in_test_size'].strip())\n",
    "    n_neighbors = int(train_args['hyper_params']['n_neighbors'].strip())\n",
    "    \n",
    "    with open(normalized_dataset.path) as f:\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "    y = df.pop('Labels')\n",
    "    X = df\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size= test_size)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    metrics.log_metric(\"accuracy\", 99)\n",
    "    metrics.log_metric(\"framework\", \"sklearn\")\n",
    "    metrics.log_metric(\"dataset_size\", 100)\n",
    "    metrics.log_metric(\"AUC\", 0.4)    \n",
    "    \n",
    "    print(f\"Metrics URI : {metrics.uri}\")\n",
    "    \n",
    "    # Export the model to a file\n",
    "    os.makedirs(model.path, exist_ok=True)\n",
    "    joblib.dump(clf, os.path.join(model.path, \"model.joblib\"))        \n",
    "        \n",
    "compiler.Compiler().compile(train_model, \"train_model.yaml\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e83baa-40f1-4689-b990-1939edc12881",
   "metadata": {},
   "source": [
    "### Component 4 : Template to deploy a model\n",
    "Generate a yaml file inclusing code to deploy a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f4bb916-da13-480a-b894-093123a23a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.10\",\n",
    "               packages_to_install=['google-cloud-aiplatform', 'pandas', 'scikit-learn'])\n",
    "\n",
    "def deploy_model(\n",
    "    deploy_args:dict,\n",
    "    model: Input[Model],\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model],\n",
    "):\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    project_id = deploy_args['project_id']\n",
    "    display_name = deploy_args['display_name']\n",
    "    container_image_uri = deploy_args['container_image_uri']\n",
    "    machine_type = deploy_args['machine_type']    \n",
    "    \n",
    "    aiplatform.init(project=project_id)\n",
    "\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=display_name,\n",
    "        artifact_uri=model.uri,\n",
    "        serving_container_image_uri=container_image_uri,\n",
    "    )\n",
    "    \n",
    "    endpoint = deployed_model.deploy(machine_type=machine_type)\n",
    "\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name\n",
    "\n",
    "compiler.Compiler().compile(deploy_model, \"deploy_model.yaml\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0c4848-663b-49f9-8c1e-6623af346082",
   "metadata": {},
   "source": [
    "### Build a pipeline\n",
    "This is a wrapper function for pipeline buider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1da3f114-4baa-42c3-bf56-bcf5df9e6bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_pipeline (pipeline_name:str,\n",
    "                    pipeline_desc:str,\n",
    "                    pipeline_root:str, \n",
    "                    component_yaml_files : dict,\n",
    "                    \n",
    "#                     create_dataset_file:str,\n",
    "#                     normalize_dataset_file:str,\n",
    "#                     train_model_file:str,\n",
    "#                     deploy_model_file:str,\n",
    "                                 \n",
    "                    ):\n",
    "\n",
    "    @dsl.pipeline(\n",
    "        name = pipeline_name,\n",
    "        description = pipeline_desc,\n",
    "        pipeline_root = pipeline_root,)\n",
    "\n",
    "    def module_pipeline(\n",
    "                        data_args: dict,\n",
    "                        normalize_args:dict,\n",
    "                        train_args:dict,\n",
    "                        deploy_args:dict,\n",
    "\n",
    "        ):\n",
    "\n",
    "        from kfp import components\n",
    "\n",
    "        create_dataset_comp = components.load_component_from_file(component_yaml_files['create_dataset_file'])\n",
    "        normalize_dataset_comp = components.load_component_from_file(component_yaml_files['normalize_dataset_file'])\n",
    "        train_model_comp = components.load_component_from_file(component_yaml_files['train_model_file'])\n",
    "        deploy_model_comp = components.load_component_from_file(component_yaml_files['deploy_model_file'])\n",
    "\n",
    "        # 1. create dataset\n",
    "        create_dataset_task = create_dataset_comp(data_args=data_args)\n",
    "        \n",
    "        # 2. normalize dataset\n",
    "        normalize_dataset_task = normalize_dataset_comp(\n",
    "            input_dataset=create_dataset_task.outputs['dataset'],\n",
    "            normalize_args = normalize_args,\n",
    "\n",
    "        )\n",
    "\n",
    "        # 3. model training\n",
    "        train_model_task = train_model_comp(\n",
    "            normalized_dataset=normalize_dataset_task.outputs['normalized_dataset'],\n",
    "            train_args = train_args\n",
    "        )\n",
    "\n",
    "        # 4. deploy model\n",
    "        deploy_model_comp(model= train_model_task.outputs['model'],\n",
    "                          deploy_args = deploy_args,\n",
    "                    )\n",
    "            \n",
    "    compiler.Compiler().compile(pipeline_func=module_pipeline, package_path=\"kfp_module_pipeline.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8016b23b-785b-482e-8a18-e9bf29b1cad9",
   "metadata": {},
   "source": [
    "### Build a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcaa4639-b1af-4d4f-b82f-7e1347b233d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "component_yaml_files = {'create_dataset_file' : 'create_dataset.yaml',\n",
    "                          'normalize_dataset_file' : 'normalize_dataset.yaml',\n",
    "                          'train_model_file' : 'train_model.yaml' ,\n",
    "                          'deploy_model_file' : 'deploy_model.yaml',\n",
    "            }\n",
    "\n",
    "build_pipeline(pipeline_name = \"kfp_module_pipeline\",\n",
    "               pipeline_desc = \"desc for kfp_module_pipeline\",\n",
    "               pipeline_root = PIPELINE_ROOT,\n",
    "               component_yaml_files = component_yaml_files, \n",
    "               # create_dataset_file = 'create_dataset.yaml',\n",
    "               # normalize_dataset_file = 'normalize_dataset.yaml',\n",
    "               # train_model_file = 'train_model.yaml' ,\n",
    "               # deploy_model_file = 'deploy_model.yaml',\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d570ebe5-72c3-4149-bae1-1a3960f31a96",
   "metadata": {},
   "source": [
    "### Run a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "116c5820-51d7-4b47-a2e1-e68bed0b7624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311020155\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311020155')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/kfp-module-pipeline-20250311020155?project=721521243942\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311020155 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311020155 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311020155 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311020155 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311020155 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311020155 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250311020155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "\n",
    "    display_name=\"kfp_module_pipeline\",\n",
    "    template_path=\"kfp_module_pipeline.yaml\",\n",
    "    \n",
    "    parameter_values = {\n",
    "        'data_args' : {\n",
    "            'csv_url': 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "            'col_names': \"['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels']\",\n",
    "        },\n",
    "        \n",
    "        'normalize_args' : {\n",
    "            'standard_scaler': False,\n",
    "            'min_max_scaler': True,\n",
    "        },\n",
    "        \n",
    "        'train_args' : {\n",
    "            'hyper_params' : {\n",
    "                'n_neighbors': \"3\",\n",
    "                'in_test_size' : \"0.2\"\n",
    "            },\n",
    "        },\n",
    "        \n",
    "        'deploy_args' : {\n",
    "            'project_id' : 'ai-hangsik',\n",
    "            'display_name' : 'kfp_module_model',\n",
    "            'container_image_uri': 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-5:latest',\n",
    "            'machine_type': 'e2-standard-4',\n",
    "        },\n",
    "        \n",
    "    },\n",
    "    \n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching = True\n",
    ")\n",
    "\n",
    "job.run(service_account = SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ab3ae-ed80-4208-8587-14b24b7800eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c5eba6-e98f-467d-984b-26e4bb2ab402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
