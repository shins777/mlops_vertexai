{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007dc29-5d01-4ebd-b143-35b150bdfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Forusone\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15422b-ea7f-4697-8d55-ed001a5c0903",
   "metadata": {},
   "source": [
    "## KFP modularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e22f1a-09d6-4c3e-aa2b-bd84fc681750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "%pip install --user --quiet google-cloud-aiplatform \\\n",
    "                         google-cloud-storage \\\n",
    "                         google-cloud-pipeline-components \\\n",
    "                         kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70ea789-440f-4529-8590-49d1453db63e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.10.1\n",
      "google_cloud_pipeline_components version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fedcb9fa-586e-4e6a-bf44-0f9ccbf6157a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "PROJECT_ID=\"ai-hangsik\"\n",
    "LOCATION=\"us-central1\"\n",
    "\n",
    "# For only colab user, no need this process for Colab Enterprise in Vertex AI.\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user(project_id=PROJECT_ID)\n",
    "\n",
    "# set project.\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1f501f30-9331-494b-83df-8310f950ef2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from kfp import compiler, dsl\n",
    "from kfp import client,compiler, dsl\n",
    "from kfp.dsl import Artifact, Metrics, Dataset, Input, Model, Output, component\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef0b93d-8568-455a-bad1-25c7722a7001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://mlops-poc-0303/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'mlops-poc-0303' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "# Create a bucket.\n",
    "BUCKET_URI = f\"gs://mlops-poc-0303\"\n",
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9409d1d4-795d-470b-bce6-b9e3e41a187d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline/iris/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10261014-2d6e-4d30-bb73-1e7d6837cd85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE_ACCOUNT: 721521243942-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(f\"SERVICE_ACCOUNT: {SERVICE_ACCOUNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d84b01f-0a19-43d2-8a26-17db94008ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://mlops-poc-0303/\n",
      "No changes made to gs://mlops-poc-0303/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewerroles/logging.logWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9f1b7b-2da9-4de7-bcc1-1c288f368201",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7712ca8d-9076-4ac0-a3bc-a278f5d9f7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image = \"python:3.10\",\n",
    "               packages_to_install=['pandas'])\n",
    "\n",
    "def create_dataset(dataset: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "\n",
    "    csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "    col_names = [\n",
    "        'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'\n",
    "    ]\n",
    "    df = pd.read_csv(csv_url, names=col_names)\n",
    "\n",
    "    with open(dataset.path, 'w') as f:\n",
    "        df.to_csv(f)\n",
    "\n",
    "compiler.Compiler().compile(create_dataset, \"create_dataset.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "56df483e-df27-43dd-92d2-9f4ea9453646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image = \"python:3.10\",\n",
    "               packages_to_install=['pandas', 'scikit-learn'])\n",
    "\n",
    "def normalize_dataset(\n",
    "    input_dataset: Input[Dataset],\n",
    "    normalized_dataset: Output[Dataset],\n",
    "    standard_scaler: bool,\n",
    "    min_max_scaler: bool,\n",
    "):\n",
    "    if standard_scaler is min_max_scaler:\n",
    "        raise ValueError(\n",
    "            'Exactly one of standard_scaler or min_max_scaler must be True.')\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    with open(input_dataset.path) as f:\n",
    "        df = pd.read_csv(f)\n",
    "    labels = df.pop('Labels')\n",
    "\n",
    "    if standard_scaler:\n",
    "        scaler = StandardScaler()\n",
    "    if min_max_scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    df = pd.DataFrame(scaler.fit_transform(df))\n",
    "    df['Labels'] = labels\n",
    "    with open(normalized_dataset.path, 'w') as f:\n",
    "        df.to_csv(f)\n",
    "\n",
    "compiler.Compiler().compile(normalize_dataset, \"normalize_dataset.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "55e4e193-ea07-4f38-a3d0-982ecf14e41d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.10\",\n",
    "               packages_to_install=['pandas', 'scikit-learn'])\n",
    "\n",
    "def train_model(\n",
    "    n_neighbors: int,\n",
    "    normalized_dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    # metrics: Output[Metrics],\n",
    "\n",
    "):\n",
    "    # import pickle\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    with open(normalized_dataset.path) as f:\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "    y = df.pop('Labels')\n",
    "    X = df\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # metrics.log_metric(\"accuracy\", \"99\")\n",
    "    # metrics.log_metric(\"framework\", \"sklearn\")\n",
    "    # metrics.log_metric(\"dataset_size\", \"100\")\n",
    "    # metrics.log_metric(\"AUC\", \"0.4\")    \n",
    "    \n",
    "\n",
    "\n",
    "    # Export the model to a file\n",
    "    os.makedirs(model.path, exist_ok=True)\n",
    "    joblib.dump(clf, os.path.join(model.path, \"model.joblib\"))        \n",
    "        \n",
    "compiler.Compiler().compile(train_model, \"train_model.yaml\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2f4bb916-da13-480a-b894-093123a23a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.10\",\n",
    "               packages_to_install=['google-cloud-aiplatform', 'pandas', 'scikit-learn'])\n",
    "\n",
    "def deploy_model(\n",
    "    model: Input[Model],\n",
    "    project_id: str,\n",
    "    \n",
    "    model_display_name:str,\n",
    "    model_serving_container_image_uri:str,\n",
    "    model_serving_machine_type:str,\n",
    "    \n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model],\n",
    "):\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project_id)\n",
    "\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=model.uri,\n",
    "        serving_container_image_uri=model_serving_container_image_uri,\n",
    "    )\n",
    "    \n",
    "    endpoint = deployed_model.deploy(machine_type=model_serving_machine_type)\n",
    "\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name\n",
    "\n",
    "compiler.Compiler().compile(deploy_model, \"deploy_model.yaml\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1da3f114-4baa-42c3-bf56-bcf5df9e6bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_pipeline (pipeline_name:str,\n",
    "                    pipeline_desc:str,\n",
    "                    pipeline_root:str, \n",
    "                    \n",
    "                    create_dataset_file:str,\n",
    "                    normalize_dataset_file:str,\n",
    "                    train_model_file:str,\n",
    "                    deploy_model_file:str,\n",
    "                    \n",
    "                    model_display_name:str,\n",
    "                    model_serving_container_image_uri:str,\n",
    "                    model_serving_machine_type:str,                    \n",
    "                                        \n",
    "                    ):\n",
    "\n",
    "    @dsl.pipeline(\n",
    "        name = pipeline_name,\n",
    "        description = pipeline_desc,\n",
    "        pipeline_root = pipeline_root,)\n",
    "\n",
    "    def module_pipeline(standard_scaler: bool,\n",
    "                        min_max_scaler: bool,\n",
    "                        n_neighbors: int, ):\n",
    "\n",
    "        from kfp import components\n",
    "\n",
    "        create_dataset_comp = components.load_component_from_file(create_dataset_file)\n",
    "        normalize_dataset_comp = components.load_component_from_file(normalize_dataset_file)\n",
    "        train_model_comp = components.load_component_from_file(train_model_file)\n",
    "        deploy_model_comp = components.load_component_from_file(deploy_model_file)\n",
    "\n",
    "        # 1. create dataset\n",
    "        create_dataset_task = create_dataset_comp()\n",
    "        \n",
    "        # 2. normalize dataset\n",
    "        normalize_dataset_task = normalize_dataset_comp(\n",
    "            input_dataset=create_dataset_task.outputs['dataset'],\n",
    "            standard_scaler=standard_scaler,\n",
    "            min_max_scaler=min_max_scaler)\n",
    "\n",
    "        # 3. model training\n",
    "        train_model_task = train_model_comp(\n",
    "            normalized_dataset=normalize_dataset_task.outputs['normalized_dataset'],\n",
    "            n_neighbors=n_neighbors\n",
    "        )\n",
    "\n",
    "        # 4. deploy model\n",
    "        deploy_model_comp(model= train_model_task.outputs['model'],\n",
    "                         project_id=PROJECT_ID,\n",
    "                         model_display_name = model_display_name,\n",
    "                         model_serving_container_image_uri = model_serving_container_image_uri,\n",
    "                         model_serving_machine_type=model_serving_machine_type,\n",
    "                    )\n",
    "            \n",
    "    compiler.Compiler().compile(pipeline_func=module_pipeline, package_path=\"kfp_module_pipeline.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fcaa4639-b1af-4d4f-b82f-7e1347b233d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_pipeline(pipeline_name = \"kfp_module_pipeline\",\n",
    "               pipeline_desc = \"desc for kfp_module_pipeline\",\n",
    "               pipeline_root = PIPELINE_ROOT,\n",
    "\n",
    "               create_dataset_file = 'create_dataset.yaml',\n",
    "               normalize_dataset_file = 'normalize_dataset.yaml',\n",
    "               train_model_file = 'train_model.yaml' ,\n",
    "               deploy_model_file = 'deploy_model.yaml',\n",
    "\n",
    "                model_display_name = 'kfp_module_model',\n",
    "               \n",
    "                # https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#expandable-4\n",
    "                model_serving_container_image_uri ='us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-5:latest', \n",
    "               \n",
    "                # https://cloud.google.com/compute/docs/general-purpose-machines#e2_machine_types_table\n",
    "                model_serving_machine_type='e2-standard-4',          \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "116c5820-51d7-4b47-a2e1-e68bed0b7624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250305183059\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250305183059')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/kfp-module-pipeline-20250305183059?project=721521243942\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250305183059 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250305183059 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250305183059 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250305183059 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250305183059 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250305183059 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250305183059 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/721521243942/locations/us-central1/pipelineJobs/kfp-module-pipeline-20250305183059\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "\n",
    "    display_name=\"kfp_module_pipeline\",\n",
    "    template_path=\"kfp_module_pipeline.yaml\",\n",
    "    parameter_values = {\n",
    "        'min_max_scaler': True,\n",
    "        'standard_scaler': False,\n",
    "        'n_neighbors': 3\n",
    "    },\n",
    "    \n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching = True\n",
    ")\n",
    "\n",
    "job.run(service_account = SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ab3ae-ed80-4208-8587-14b24b7800eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800c748-bef8-4cc1-a9c7-a858ad2a1181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efdbf3-2dfd-4151-ab3d-a5e23e811ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d62cf-4c9a-4603-8312-599cf5f3be3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172150f0-70e7-4e73-b20e-f57922fd44a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
