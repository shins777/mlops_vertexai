{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 설치\n"
      ],
      "metadata": {
        "id": "QWEVWfTnxCF6"
      },
      "id": "QWEVWfTnxCF6"
    },
    {
      "cell_type": "code",
      "id": "2ccq7VwGs1CXadrQf68ArInA",
      "metadata": {
        "tags": [],
        "id": "2ccq7VwGs1CXadrQf68ArInA"
      },
      "source": [
        "!pip install --upgrade pip setuptools wheel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --no-build-isolation langsmith"
      ],
      "metadata": {
        "id": "NU8FjIs1xFhW"
      },
      "id": "NU8FjIs1xFhW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --user --quiet  google-cloud-aiplatform \\\n",
        "                             google-cloud-storage \\\n",
        "                             google-cloud-pipeline-components \\\n",
        "                             kfp"
      ],
      "metadata": {
        "id": "OrR36OHkxFkX"
      },
      "id": "OrR36OHkxFkX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 환경 변수 설정"
      ],
      "metadata": {
        "id": "lsqVEVn-xI_9"
      },
      "id": "lsqVEVn-xI_9"
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"ureca-poc-itcen\"\n",
        "LOCATION = \"us-central1\"\n",
        "BUCKET_URI = \"gs://ureca_test\"\n",
        "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline/\"\n",
        "DATASET_ID = \"test_data\"\n",
        "TABLE_ID = \"amazon_review_v1\"\n",
        "\n",
        "SERVICE_ACCOUNT = \"vertex-api@ureca-poc-itcen.iam.gserviceaccount.com\""
      ],
      "metadata": {
        "id": "o8ot9qF6xFo-"
      },
      "id": "o8ot9qF6xFo-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 불러오기"
      ],
      "metadata": {
        "id": "y-r5ejV9xeEb"
      },
      "id": "y-r5ejV9xeEb"
    },
    {
      "cell_type": "code",
      "source": [
        "# 일반 라이브러리\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import uuid\n",
        "from typing import NamedTuple, List\n",
        "\n",
        "# Vertex AI 라이브러리\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from google.cloud.aiplatform_v1.types.pipeline_state import PipelineState\n",
        "\n",
        "# Kubeflow Pipelines(KFP) 관련 라이브러리\n",
        "from kfp import compiler, dsl, client\n",
        "from kfp import components\n",
        "from kfp.dsl import (\n",
        "    Artifact, Metrics, Dataset, Input, Model, Output, component\n",
        ")\n",
        "import kfp.v2.dsl as dsl\n",
        "\n",
        "# 로깅 설정\n",
        "logger = logging.getLogger(\"logger\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Vertex AI 초기화\n",
        "vertex_ai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "ZlaAd8yfxFtI"
      },
      "id": "ZlaAd8yfxFtI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 빌드된 학습용 컨테이너 이미지 URI\n",
        "TRAIN_IMAGE = vertex_ai.helpers.get_prebuilt_prediction_container_uri(\n",
        "    framework=\"sklearn\",\n",
        "    framework_version=\"1.3\",\n",
        "    accelerator=\"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "OjBph2M9yYMx"
      },
      "id": "OjBph2M9yYMx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 컴포넌트 파이프라인"
      ],
      "metadata": {
        "id": "GcL8L6hOyoRl"
      },
      "id": "GcL8L6hOyoRl"
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 생성 컴포넌트\n",
        "@component(\n",
        "    base_image=\"python:3.10\",\n",
        "    packages_to_install=[\n",
        "        \"numpy==1.23.5\",\n",
        "        \"pandas\",\n",
        "        \"google-cloud-bigquery\",\n",
        "        \"joblib\",\n",
        "        \"db-dtypes\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "def create_dataset(\n",
        "    data_args: dict,                     # 데이터 파라미터\n",
        "    data_artifact: Output[Artifact],     # JSON 형태 데이터 아티팩트\n",
        "    dataset: Output[Dataset],            # CSV 형태 데이터셋\n",
        "):\n",
        "    # 라이브러리\n",
        "    import os\n",
        "    import json\n",
        "    import pandas as pd\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    # GCP 설정 값\n",
        "    PROJECT_ID = \"ureca-poc-itcen\"\n",
        "    LOCATION = \"us-central1\"\n",
        "    DATASET_ID = \"test_data\"\n",
        "    TABLE_ID = \"amazon_review_v1\"\n",
        "\n",
        "    # BigQuery 클라이언트 초기화\n",
        "    client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "    # 쿼리 (데이터 조회)\n",
        "    query = f\"\"\"\n",
        "    SELECT customer_id, product_id, star_rating\n",
        "    FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
        "    \"\"\"\n",
        "\n",
        "    # CSV 저장을 위한 폴더 생성\n",
        "    os.makedirs(dataset.path, exist_ok=True)\n",
        "\n",
        "    # 쿼리 실행 결과 DataFrame으로 변환 및 CSV 저장\n",
        "    df = client.query(query).to_dataframe()\n",
        "    df.to_csv(os.path.join(dataset.path, \"raw_data.csv\"), index=False)\n",
        "\n",
        "    # JSON 형식으로 변환\n",
        "    json_data = df.to_json(orient=\"records\", lines=True)\n",
        "\n",
        "    # JSON 저장을 위한 폴더 생성\n",
        "    os.makedirs(data_artifact.path, exist_ok=True)\n",
        "\n",
        "    # JSON 파일 저장\n",
        "    with open(os.path.join(data_artifact.path, \"data.json\"), \"w\") as f:\n",
        "        json.dump(json_data, f, indent=4)\n",
        "\n",
        "# 컴포넌트를 YAML 파일로 컴파일\n",
        "compiler.Compiler().compile(\n",
        "    create_dataset,\n",
        "    \"create_dataset.yaml\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "oOFSfc8cyYPZ"
      },
      "id": "oOFSfc8cyYPZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 컴포넌트\n",
        "@component(\n",
        "    base_image=\"python:3.10\",\n",
        "    packages_to_install=[\"numpy==1.23.5\", \"pandas\", \"scikit-learn\"],\n",
        ")\n",
        "\n",
        "def preprocess_dataset(\n",
        "    input_dataset: Input[Dataset],            # (입력) 원본 데이터셋\n",
        "    preprocess_args: dict,                    # (입력) 전처리 관련 하이퍼파라미터\n",
        "    processed_train: Output[Dataset],         # (출력) 전처리된 학습 데이터\n",
        "    processed_test: Output[Dataset],          # (출력) 전처리된 테스트 데이터\n",
        "):\n",
        "    # 라이브러리\n",
        "    import os\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # preprocess_args 내 테스트셋 비율 불러오기\n",
        "    test_size = float(preprocess_args['hyper_params']['in_test_size'])\n",
        "\n",
        "    # 데이터셋 불러오기\n",
        "    df = pd.read_csv(os.path.join(input_dataset.path, \"raw_data.csv\"))\n",
        "\n",
        "    # 사용자 ID와 상품 ID를 숫자 인덱스로 매핑\n",
        "    user_ids = {user: i for i, user in enumerate(df['customer_id'].unique())}\n",
        "    item_ids = {item: i for i, item in enumerate(df['product_id'].unique())}\n",
        "    df['user_idx'] = df['customer_id'].map(user_ids)\n",
        "    df['item_idx'] = df['product_id'].map(item_ids)\n",
        "\n",
        "    # 학습 데이터와 테스트 데이터로 분할\n",
        "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=0)\n",
        "\n",
        "    # 출력 디렉토리 생성\n",
        "    os.makedirs(processed_train.path, exist_ok=True)\n",
        "    os.makedirs(processed_test.path, exist_ok=True)\n",
        "\n",
        "    # 분할된 데이터를 각각 CSV로 저장\n",
        "    train_df.to_csv(os.path.join(processed_train.path, \"train_data.csv\"), index=False)\n",
        "    test_df.to_csv(os.path.join(processed_test.path, \"test_data.csv\"), index=False)\n",
        "\n",
        "# 컴포넌트를 YAML 파일로 컴파일\n",
        "compiler.Compiler().compile(\n",
        "    preprocess_dataset,\n",
        "    \"preprocess_dataset.yaml\"\n",
        ")"
      ],
      "metadata": {
        "id": "V2060y4O2Akh"
      },
      "id": "V2060y4O2Akh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 컴포넌트\n",
        "@component(\n",
        "    base_image=\"python:3.10\",\n",
        "    packages_to_install=[\"numpy==1.23.5\", \"pandas\", \"joblib\"],\n",
        ")\n",
        "\n",
        "def train_model(\n",
        "    train_dataset: Input[Dataset],     # (입력) 학습용 데이터셋 (CSV)\n",
        "    train_args: dict,                  # (입력) 학습 관련 하이퍼파라미터\n",
        "    model: Output[Model],              # (출력) 학습된 모델 (joblib 형식)\n",
        "):\n",
        "    # 라이브러리\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    import os\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # 파라미터 설정값 불러오기\n",
        "    n_factors = int(train_args['hyper_params']['n_factors'])\n",
        "    learning_rate = float(train_args['hyper_params']['learning_rate'])\n",
        "    reg = float(train_args['hyper_params']['reg'])\n",
        "    num_epochs = int(train_args['hyper_params']['num_epochs'])\n",
        "\n",
        "    # 학습 데이터 불러오기\n",
        "    df = pd.read_csv(os.path.join(train_dataset.path, \"train_data.csv\"))\n",
        "\n",
        "    # SVD 학습\n",
        "    num_users = df[\"user_idx\"].max() + 1\n",
        "    num_items = df[\"item_idx\"].max() + 1\n",
        "\n",
        "    P = np.random.normal(0, 0.1, (num_users, n_factors))\n",
        "    Q = np.random.normal(0, 0.1, (num_items, n_factors))\n",
        "\n",
        "    item_popularity = defaultdict(int, df[\"item_idx\"].value_counts().to_dict())\n",
        "    item_histories = df.groupby(\"user_idx\")[\"item_idx\"].apply(set).to_dict()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            user_idx = row[\"user_idx\"]\n",
        "            item_idx = row[\"item_idx\"]\n",
        "            rating = row[\"star_rating\"]\n",
        "\n",
        "            pred = np.dot(P[user_idx], Q[item_idx].T)\n",
        "            error = rating - pred\n",
        "\n",
        "            P[user_idx] += learning_rate * (error * Q[item_idx] - reg * P[user_idx])\n",
        "            Q[item_idx] += learning_rate * (error * P[user_idx] - reg * Q[item_idx])\n",
        "\n",
        "            total_loss += error ** 2\n",
        "\n",
        "    # 모델 저장을 위한 디렉토리 생성\n",
        "    os.makedirs(model.path, exist_ok=True)\n",
        "\n",
        "    # 모델 저장 (행렬 및 부가 정보 포함)\n",
        "    model_data = {\n",
        "        \"P\": P,\n",
        "        \"Q\": Q,\n",
        "        \"num_users\": num_users,\n",
        "        \"num_items\": num_items,\n",
        "        \"item_popularity\": dict(item_popularity),\n",
        "        \"item_histories\": {k: list(v) for k, v in item_histories.items()}\n",
        "    }\n",
        "    joblib.dump(model_data, os.path.join(model.path, \"model.joblib\"))\n",
        "\n",
        "# 컴포넌트를 YAML 파일로 컴파일\n",
        "compiler.Compiler().compile(\n",
        "    train_model,\n",
        "    \"train_model_svd.yaml\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "XDmc8RJP3Bdl"
      },
      "id": "XDmc8RJP3Bdl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가 컴포넌트\n",
        "@component(\n",
        "    base_image=\"python:3.10\",\n",
        "    packages_to_install=[\"numpy==1.23.5\", \"pandas\", \"scikit-learn\"],\n",
        ")\n",
        "\n",
        "def evaluate_model(\n",
        "    evaluate_args: dict,               # (입력) 평가 관련 하이퍼파라미터\n",
        "    model: Input[Model],               # (입력) 학습된 모델 (joblib)\n",
        "    test_data: Input[Dataset],         # (입력) 테스트 데이터셋 (CSV)\n",
        "    output_artifact: Output[Artifact], # (출력) 평가 결과 (JSON)\n",
        "    metrics: Output[Metrics],          # (출력) 파이프라인에 기록되는 평가 지표\n",
        "):\n",
        "    # 라이브러리\n",
        "    import os\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    import json\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "    # 테스트 데이터 불러오기\n",
        "    df = pd.read_csv(os.path.join(test_data.path, \"test_data.csv\"))\n",
        "\n",
        "    # 학습된 모델 불러오기\n",
        "    model_data = joblib.load(os.path.join(model.path, \"model.joblib\"))\n",
        "    P, Q = model_data[\"P\"], model_data[\"Q\"]\n",
        "    num_users, num_items = model_data[\"num_users\"], model_data[\"num_items\"]\n",
        "    item_popularity = model_data[\"item_popularity\"]\n",
        "    item_histories = model_data[\"item_histories\"]\n",
        "\n",
        "    # 예측 및 평가\n",
        "    predictions = []\n",
        "    predicted_items = []\n",
        "    actual_items = []\n",
        "    errors = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        user_idx = row[\"user_idx\"]\n",
        "        item_idx = row[\"item_idx\"]\n",
        "\n",
        "        if user_idx < num_users and item_idx < num_items:\n",
        "            pred = np.dot(P[user_idx], Q[item_idx].T)\n",
        "        else:\n",
        "            pred = np.mean(df[\"star_rating\"])\n",
        "\n",
        "        predictions.append(pred)\n",
        "        predicted_items.append(item_idx)\n",
        "        actual_items.append(item_idx)\n",
        "        errors.append(row[\"star_rating\"] - pred)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    errors = np.array(errors)\n",
        "\n",
        "    # 평가 지표 계산\n",
        "    rmse_value = np.sqrt(mean_squared_error(df[\"star_rating\"], predictions))\n",
        "    total_items = len(item_popularity)\n",
        "    unique_recommended = len(set(predicted_items))\n",
        "    coverage = unique_recommended / total_items if total_items > 0 else 0\n",
        "\n",
        "    novelty_scores = [-np.log2(item_popularity.get(item, 1) + 1) for item in predicted_items]\n",
        "    novelty = np.mean(novelty_scores) if novelty_scores else 0\n",
        "\n",
        "    diversity_scores = []\n",
        "    for i in range(len(predicted_items)):\n",
        "        for j in range(i + 1, len(predicted_items)):\n",
        "            if predicted_items[i] in Q and predicted_items[j] in Q:\n",
        "                sim = np.dot(Q[predicted_items[i]], Q[predicted_items[j]]) / (\n",
        "                    np.linalg.norm(Q[predicted_items[i]]) * np.linalg.norm(Q[predicted_items[j]])\n",
        "                )\n",
        "                diversity_scores.append(1 - sim)\n",
        "    diversity = np.mean(diversity_scores) if diversity_scores else 0\n",
        "\n",
        "    serendipity_scores = [\n",
        "        item not in item_histories.get(user_idx, [])\n",
        "        for user_idx, item in zip(df[\"user_idx\"], predicted_items)\n",
        "    ]\n",
        "    serendipity = np.mean(serendipity_scores) if serendipity_scores else 0\n",
        "\n",
        "    # 평가지표 저장\n",
        "    metrics.log_metric(\"Model\", \"SVD (Matrix Factorization)\")\n",
        "    metrics.log_metric(\"RMSE\", rmse_value)\n",
        "    metrics.log_metric(\"Coverage\", coverage)\n",
        "    metrics.log_metric(\"Novelty\", novelty)\n",
        "    metrics.log_metric(\"Diversity\", diversity)\n",
        "    metrics.log_metric(\"Serendipity\", serendipity)\n",
        "\n",
        "    # 결과를 JSON 파일로 저장\n",
        "    output_data = {\n",
        "        \"Model\": \"SVD (Matrix Factorization)\",\n",
        "        \"RMSE\": rmse_value,\n",
        "        \"Coverage\": coverage,\n",
        "        \"Novelty\": novelty,\n",
        "        \"Diversity\": diversity,\n",
        "        \"Serendipity\": serendipity,\n",
        "    }\n",
        "\n",
        "    os.makedirs(output_artifact.path, exist_ok=True)\n",
        "    with open(os.path.join(output_artifact.path, \"output.json\"), 'w') as f:\n",
        "        json.dump(output_data, f, indent=4)\n",
        "\n",
        "# 컴포넌트를 YAML 파일로 컴파일\n",
        "compiler.Compiler().compile(\n",
        "    evaluate_model,\n",
        "    \"evaluate_model.yaml\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "jNhtVZa6350b"
      },
      "id": "jNhtVZa6350b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 배포 컴포넌트 (학습된 모델을 vertex ai에 업로드 및 서빙용 엔드포인트 생성 )\n",
        "@dsl.component(\n",
        "    base_image=\"python:3.10\",\n",
        "    packages_to_install=[\n",
        "        'google-cloud-aiplatform',  # Vertex AI SDK\n",
        "        'pandas',\n",
        "        'scikit-learn'\n",
        "    ]\n",
        ")\n",
        "\n",
        "def deploy_model(\n",
        "    deploy_args: dict,                  # (입력) 배포 관련 하이퍼파라미터\n",
        "    model: Input[Model],                # (입력) 학습된 모델 (joblib)\n",
        "    vertex_endpoint: Output[Artifact],  # (출력) 생성된 Endpoint 정보\n",
        "    vertex_model: Output[Model],        # (출력) Vertex AI 모델 정보\n",
        "):\n",
        "    # 라이브러리\n",
        "    from google.cloud import aiplatform\n",
        "\n",
        "    # 파라미터 설정값 불러오기\n",
        "    project_id = deploy_args['project_id']\n",
        "    display_name = deploy_args['display_name']\n",
        "    container_image_uri = deploy_args['container_image_uri']\n",
        "    machine_type = deploy_args['machine_type']\n",
        "    model_name = deploy_args['model_name']\n",
        "\n",
        "    # Vertex AI 초기화\n",
        "    aiplatform.init(project=project_id)\n",
        "\n",
        "    # 모델 업로드 (학습된 모델을 Vertex AI Model Registry에 등록)\n",
        "    deployed_model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        artifact_uri=model.path,\n",
        "        serving_container_image_uri=container_image_uri\n",
        "    )\n",
        "\n",
        "    # Endpoint 생성 및 모델 배포\n",
        "    endpoint = deployed_model.deploy(machine_type=machine_type)\n",
        "\n",
        "    vertex_endpoint.uri = endpoint.resource_name\n",
        "    vertex_model.uri = deployed_model.resource_name\n",
        "\n",
        "# 컴포넌트를 YAML 파일로 컴파일\n",
        "compiler.Compiler().compile(\n",
        "    deploy_model,\n",
        "    \"deploy_model.yaml\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZLb1tXGh352u"
      },
      "id": "ZLb1tXGh352u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 파이프라인 정의 및 YAML 컴파일 함수\n",
        "def build_pipeline(\n",
        "    pipeline_name: str,               # 파이프라인 이름\n",
        "    pipeline_desc: str,               # 파이프라인 설명\n",
        "    pipeline_root: str,               # 파이프라인 아티팩트가 저장될 GCS 경로\n",
        "    component_yaml_files: dict        # 컴포넌트별 YAML 파일\n",
        "):\n",
        "\n",
        "    # 파이프라인 정의\n",
        "    @dsl.pipeline(\n",
        "        name=pipeline_name,\n",
        "        description=pipeline_desc,\n",
        "        pipeline_root=pipeline_root,\n",
        "    )\n",
        "    def module_pipeline(\n",
        "        data_args: dict,\n",
        "        preprocess_args: dict,\n",
        "        train_args: dict,\n",
        "        evaluate_args: dict,\n",
        "        deploy_args: dict\n",
        "    ):\n",
        "        # 라이브러리\n",
        "        from kfp import components\n",
        "\n",
        "        # 각 컴포넌트 불러오기\n",
        "        create_dataset_comp = components.load_component_from_file(\n",
        "            component_yaml_files['create_dataset_file']\n",
        "        )\n",
        "        preprocess_dataset_comp = components.load_component_from_file(\n",
        "            component_yaml_files['preprocess_dataset_file']\n",
        "        )\n",
        "        train_model_comp = components.load_component_from_file(\n",
        "            component_yaml_files['train_model_file']\n",
        "        )\n",
        "        evaluate_model_comp = components.load_component_from_file(\n",
        "            component_yaml_files['evaluate_model_file']\n",
        "        )\n",
        "        deploy_model_comp = components.load_component_from_file(\n",
        "            component_yaml_files['deploy_model_file']\n",
        "        )\n",
        "\n",
        "        # 데이터 생성 컴포넌트 실행\n",
        "        create_dataset_task = create_dataset_comp(data_args=data_args)\n",
        "\n",
        "        # 데이터 전처리 컴포넌트 실행\n",
        "        preprocess_dataset_task = preprocess_dataset_comp(\n",
        "            preprocess_args=preprocess_args,\n",
        "            input_dataset=create_dataset_task.outputs['dataset']\n",
        "        )\n",
        "\n",
        "        # 모델 학습 컴포넌트 실행\n",
        "        train_model_task = train_model_comp(\n",
        "            train_args=train_args,\n",
        "            train_dataset=preprocess_dataset_task.outputs['processed_train']\n",
        "        )\n",
        "\n",
        "        # 모델 평가 컴포넌트 실행\n",
        "        evaluate_model_task = evaluate_model_comp(\n",
        "            evaluate_args=evaluate_args,\n",
        "            model=train_model_task.outputs['model'],\n",
        "            test_data=preprocess_dataset_task.outputs['processed_test']\n",
        "        )\n",
        "\n",
        "        # 모델 배포 컴포넌트 실행\n",
        "        deploy_model_task = deploy_model_comp(\n",
        "            deploy_args=deploy_args,\n",
        "            model=train_model_task.outputs['model']\n",
        "        )\n",
        "\n",
        "    # 전체 파이프라인을 YAML 파일로 컴파일\n",
        "    compiler.Compiler().compile(\n",
        "        pipeline_func=module_pipeline,\n",
        "        package_path=\"svd_pipeline.yaml\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "7pbgLzp7355C"
      },
      "id": "7pbgLzp7355C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 컴포넌트의 YAML 파일\n",
        "component_yaml_files = {\n",
        "    'create_dataset_file': 'create_dataset.yaml',\n",
        "    'preprocess_dataset_file': 'preprocess_dataset.yaml',\n",
        "    'train_model_file': 'train_model_svd.yaml',\n",
        "    'evaluate_model_file': 'evaluate_model.yaml',\n",
        "    'deploy_model_file': 'deploy_model.yaml',\n",
        "}\n",
        "\n",
        "# 전체 파이프라인 빌드 및 YAML 컴파일 실행\n",
        "build_pipeline(\n",
        "    pipeline_name=\"svd_pipeline\",                      # 파이프라인 이름\n",
        "    pipeline_desc=\"desc for svd_pipeline\",             # 파이프라인 설명\n",
        "    pipeline_root=PIPELINE_ROOT,                       # 파이프라인 결과가 저장될 GCS 경로\n",
        "    component_yaml_files=component_yaml_files          # 각 컴포넌트 정의 YAML 파일 경로\n",
        ")\n"
      ],
      "metadata": {
        "id": "llIhTnwA357s"
      },
      "id": "llIhTnwA357s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vertex AI 파이프라인 실행\n",
        "job = aiplatform.PipelineJob(\n",
        "    display_name=\"svd_pipeline\",\n",
        "    template_path=\"svd_pipeline.yaml\",\n",
        "\n",
        "    # 파라미터 입력\n",
        "    parameter_values={\n",
        "        'data_args': {},\n",
        "\n",
        "        'preprocess_args': {\n",
        "            'hyper_params': {\n",
        "                'in_test_size': 0.2\n",
        "            }\n",
        "        },\n",
        "\n",
        "        'train_args': {\n",
        "            'hyper_params': {\n",
        "                'n_factors': 40,\n",
        "                'learning_rate': 0.01,\n",
        "                'reg': 0.05,\n",
        "                'num_epochs': 50\n",
        "            }\n",
        "        },\n",
        "\n",
        "        'evaluate_args': {},\n",
        "\n",
        "        'deploy_args': {\n",
        "            'project_id': 'ureca-poc-itcen',\n",
        "            'display_name': 'svd',\n",
        "            'container_image_uri': 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-5:latest',\n",
        "            'machine_type': 'n1-standard-16',\n",
        "            'model_name': 'svd'\n",
        "        },\n",
        "    },\n",
        "\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    enable_caching=True\n",
        ")\n",
        "\n",
        "# Vertex AI 파이프라인 실행\n",
        "job.run(service_account=SERVICE_ACCOUNT)\n"
      ],
      "metadata": {
        "id": "1OP9sN01359y"
      },
      "id": "1OP9sN01359y",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "kfp_modularization.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}