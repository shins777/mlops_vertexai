{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"copyright"},"outputs":[],"source":["# Copyright 2024 Forusone(shins777@gmail.com)\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"title:generic"},"source":["# Lightweight KFP Pipelines\n","* In this tutorial, you learn to use the KFP SDK to build lightweight Python function-based components, and then you learn to use Vertex AI Pipelines to execute the pipeline.\n","* This lab simplifies the original notebook [Lightweight kfp](https://colab.sandbox.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/lightweight_functions_component_io_kfp.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"install_aip:mbsdk"},"source":["## Install Vertex AI SDK for Python and other required packages"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"NOFzTGzzL6no","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735857919334,"user_tz":-540,"elapsed":5235,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"19221a6e-4bcc-4079-f896-9588cf9852fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n","                                 google-cloud-storage \\\n","                                 kfp \\\n","                                 \"numpy<2\" \\\n","                                 google-cloud-pipeline-components"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"c97be6a73155","executionInfo":{"status":"ok","timestamp":1735858420656,"user_tz":-540,"elapsed":146,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Authentication to GCP\n","import sys\n","\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"oM1iC_MfAts1","cellView":"form","executionInfo":{"status":"ok","timestamp":1735857945016,"user_tz":-540,"elapsed":3,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Set GCP information\n","PROJECT_ID = \"ai-hangsik\"  # @param {type:\"string\"}\n","LOCATION = \"us-central1\"  # @param {type:\"string\"}"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"MzGDU7TWdts_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735857999759,"user_tz":-540,"elapsed":2725,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"8b0a8afb-10ac-4167-e9fd-9ec60d93c35e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating gs://mlops-ai-hangsik-1209/...\n","ServiceException: 409 A Cloud Storage bucket named 'mlops-ai-hangsik-1209' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"]}],"source":["# @title Create a bucket.\n","BUCKET_URI = f\"gs://mlops-{PROJECT_ID}-1209\"\n","! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"KovkBbqHL6nq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735858013488,"user_tz":-540,"elapsed":1731,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"d38850db-837e-4291-a6a4-674ca9b61b80"},"outputs":[{"output_type":"stream","name":"stdout","text":["SERVICE_ACCOUNT: 721521243942-compute@developer.gserviceaccount.com\n"]}],"source":["# @title Service account\n","shell_output = ! gcloud projects describe  $PROJECT_ID\n","project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n","\n","SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n","\n","print(f\"SERVICE_ACCOUNT: {SERVICE_ACCOUNT}\")"]},{"cell_type":"code","source":["! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n","! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1kHmikQM2ie","executionInfo":{"status":"ok","timestamp":1735858029813,"user_tz":-540,"elapsed":5436,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"e3a4fe25-8d31-4b91-b9ee-d40ca0ab43af"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["No changes made to gs://mlops-ai-hangsik-1209/\n","No changes made to gs://mlops-ai-hangsik-1209/\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"import_aip:mbsdk","executionInfo":{"status":"ok","timestamp":1735858036920,"user_tz":-540,"elapsed":3952,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Import libraries\n","from typing import NamedTuple\n","\n","import kfp\n","from google.cloud import aiplatform\n","from kfp import compiler, dsl\n","from kfp.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n","                     OutputPath, component)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"FhqajZGfL6nq","executionInfo":{"status":"ok","timestamp":1735858042293,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Pipelines constants\n","PIPELINE_ROOT = \"{}/pipeline_root/shakespeare\".format(BUCKET_URI)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"3y_baPUbL6nq","executionInfo":{"status":"ok","timestamp":1735858045157,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Initialize Vertex AI SDK\n","aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"IigaMQotL6nq","executionInfo":{"status":"ok","timestamp":1735858048619,"user_tz":-540,"elapsed":17,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Define Python function-based pipeline components\n","\n","@component(base_image=\"python:3.9\")\n","def preprocess(\n","    # An input parameter of type string.\n","    message: str,\n","    # Use Output to get a metadata-rich handle to the output artifact\n","    # of type `Dataset`.\n","    output_dataset_one: Output[Dataset],\n","    # A locally accessible filepath for another output artifact of type\n","    # `Dataset`.\n","    output_dataset_two_path: OutputPath(\"Dataset\"),\n","    # A locally accessible filepath for an output parameter of type string.\n","    output_parameter_path: OutputPath(str),\n","):\n","    \"\"\"'Mock' preprocessing step.\n","    Writes out the passed in message to the output \"Dataset\"s and the output message.\n","    \"\"\"\n","    output_dataset_one.metadata[\"hello\"] = \"there\"\n","    # Use OutputArtifact.path to access a local file path for writing.\n","    # One can also use OutputArtifact.uri to access the actual URI file path.\n","    with open(output_dataset_one.path, \"w\") as f:\n","        f.write(message)\n","\n","    # OutputPath is used to just pass the local file path of the output artifact\n","    # to the function.\n","    with open(output_dataset_two_path, \"w\") as f:\n","        f.write(message)\n","\n","    with open(output_parameter_path, \"w\") as f:\n","        f.write(message)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"adTHYS1IL6nq","executionInfo":{"status":"ok","timestamp":1735858051549,"user_tz":-540,"elapsed":36,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Define train component\n","\n","@component(\n","    base_image=\"python:3.9\",  # Use a different base image.\n",")\n","def train(\n","    # An input parameter of type string.\n","    message: str,\n","    # Use InputPath to get a locally accessible path for the input artifact\n","    # of type `Dataset`.\n","    dataset_one_path: InputPath(\"Dataset\"),\n","    # Use InputArtifact to get a metadata-rich handle to the input artifact\n","    # of type `Dataset`.\n","    dataset_two: Input[Dataset],\n","    # Output artifact of type Model.\n","    imported_dataset: Input[Dataset],\n","    model: Output[Model],\n","    # An input parameter of type int with a default value.\n","    num_steps: int = 3,\n","    # Use NamedTuple to return either artifacts or parameters.\n","    # When returning artifacts like this, return the contents of\n","    # the artifact. The assumption here is that this return value\n","    # fits in memory.\n",") -> NamedTuple(\n","    \"Outputs\",\n","    [\n","        (\"output_message\", str),  # Return parameter.\n","        (\"generic_artifact\", Artifact),  # Return generic Artifact.\n","    ],\n","):\n","    \"\"\"'Mock' Training step.\n","    Combines the contents of dataset_one and dataset_two into the\n","    output Model.\n","    Constructs a new output_message consisting of message repeated num_steps times.\n","    \"\"\"\n","\n","    # Directly access the passed in GCS URI as a local file (uses GCSFuse).\n","    with open(dataset_one_path) as input_file:\n","        dataset_one_contents = input_file.read()\n","\n","    # dataset_two is an Artifact handle. Use dataset_two.path to get a\n","    # local file path (uses GCSFuse).\n","    # Alternately, use dataset_two.uri to access the GCS URI directly.\n","    with open(dataset_two.path) as input_file:\n","        dataset_two_contents = input_file.read()\n","\n","    with open(model.path, \"w\") as f:\n","        f.write(\"My Model\")\n","\n","    with open(imported_dataset.path) as f:\n","        data = f.read()\n","    print(\"Imported Dataset:\", data)\n","\n","    # Use model.get() to get a Model artifact, which has a .metadata dictionary\n","    # to store arbitrary metadata for the output artifact. This metadata is\n","    # recorded in Managed Metadata and can be queried later. It also shows up\n","    # in the Google Cloud console.\n","    model.metadata[\"accuracy\"] = 0.9\n","    model.metadata[\"framework\"] = \"Tensorflow\"\n","    model.metadata[\"time_to_train_in_seconds\"] = 257\n","\n","    artifact_contents = \"{}\\n{}\".format(dataset_one_contents, dataset_two_contents)\n","    output_message = \" \".join([message for _ in range(num_steps)])\n","    return (output_message, artifact_contents)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"ytZaHwGZL6nr","executionInfo":{"status":"ok","timestamp":1735858054734,"user_tz":-540,"elapsed":6,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Define read_artifact_input component\n","@component(base_image=\"python:3.9\")\n","def read_artifact_input(\n","    generic: Input[Artifact],\n","):\n","    with open(generic.path) as input_file:\n","        generic_contents = input_file.read()\n","        print(f\"generic contents: {generic_contents}\")"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"p7vNP-EzL6nr","executionInfo":{"status":"ok","timestamp":1735858057119,"user_tz":-540,"elapsed":15,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Define a pipeline that uses your components and the Importer\n","@dsl.pipeline(\n","    # Default pipeline root. You can override it when submitting the pipeline.\n","    pipeline_root=PIPELINE_ROOT,\n","    # A name for the pipeline. Use to determine the pipeline Context.\n","    name=\"metadata-pipeline-v2\",\n",")\n","def pipeline(message: str):\n","    importer = kfp.dsl.importer(\n","        artifact_uri=\"gs://ml-pipeline-playground/shakespeare1.txt\",\n","        artifact_class=Dataset,\n","        reimport=False,\n","    )\n","    preprocess_task = preprocess(message=message)\n","    train_task = train(\n","        dataset_one_path=preprocess_task.outputs[\"output_dataset_one\"],\n","        dataset_two=preprocess_task.outputs[\"output_dataset_two_path\"],\n","        imported_dataset=importer.output,\n","        message=preprocess_task.outputs[\"output_parameter_path\"],\n","        num_steps=5,\n","    )\n","    read_task = read_artifact_input(  # noqa: F841\n","        generic=train_task.outputs[\"generic_artifact\"]\n","    )"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"LC0JlqZnL6nr","executionInfo":{"status":"ok","timestamp":1735858059419,"user_tz":-540,"elapsed":29,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Compile the pipeline\n","compiler.Compiler().compile(\n","    pipeline_func=pipeline, package_path=\"lightweight_pipeline.yaml\"\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"bHNE84kDL6nu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735858420310,"user_tz":-540,"elapsed":358965,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"fd2a0129-eeaa-4068-e955-ea92bd310045"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n","INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741\n","INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n","INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741')\n","INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n","https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/metadata-pipeline-v2-20250102224741?project=721521243942\n","INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741 current state:\n","PipelineState.PIPELINE_STATE_RUNNING\n","INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741 current state:\n","PipelineState.PIPELINE_STATE_RUNNING\n","INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741 current state:\n","PipelineState.PIPELINE_STATE_RUNNING\n","INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741 current state:\n","PipelineState.PIPELINE_STATE_RUNNING\n","INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741 current state:\n","PipelineState.PIPELINE_STATE_RUNNING\n","INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741 current state:\n","PipelineState.PIPELINE_STATE_RUNNING\n","INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741\n"]}],"source":["# @title Run the pipeline\n","DISPLAY_NAME = \"shakespeare\"\n","\n","job = aiplatform.PipelineJob(\n","    display_name=DISPLAY_NAME,\n","    template_path=\"lightweight_pipeline.yaml\",\n","    pipeline_root=PIPELINE_ROOT,\n","    parameter_values={\"message\": \"Hello, World\"},\n","    enable_caching=False,\n",")\n","\n","job.run()"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"425adbf24044","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735860483766,"user_tz":-540,"elapsed":666,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"d3b38e75-f2d7-4cfb-deea-269620ce2f61"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.base:Deleting PipelineJob : projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741\n","INFO:google.cloud.aiplatform.base:PipelineJob deleted. . Resource name: projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741\n","INFO:google.cloud.aiplatform.base:Deleting PipelineJob resource: projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741\n","INFO:google.cloud.aiplatform.base:Delete PipelineJob backing LRO: projects/721521243942/locations/us-central1/operations/1685094924975865856\n","INFO:google.cloud.aiplatform.base:PipelineJob resource projects/721521243942/locations/us-central1/pipelineJobs/metadata-pipeline-v2-20250102224741 deleted.\n"]}],"source":["# @title Delete the pipeline job\n","job.delete()"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"BvedLNmgL6nu","executionInfo":{"status":"ok","timestamp":1735860485842,"user_tz":-540,"elapsed":109,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Cleaning up\n","delete_bucket = False\n","\n","if delete_bucket:\n","    ! gsutil rm -r $BUCKET_URI\n","\n","! rm lightweight_pipeline.yaml"]}],"metadata":{"colab":{"toc_visible":true,"provenance":[{"file_id":"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/lightweight_functions_component_io_kfp.ipynb","timestamp":1735857781144}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}