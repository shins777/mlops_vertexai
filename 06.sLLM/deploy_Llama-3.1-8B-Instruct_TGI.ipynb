{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ur8xi4C7S06n"},"outputs":[],"source":["# Copyright 2024 Forusone\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"JAPoU8Sm5E6e"},"source":["# Serving Llama with Text Generation Inference (TGI) on Vertex AI\n","> [**Deploy Meta Llama 3.1 405B on Google Cloud Vertex AI** ](https://github.com/huggingface/blog/blob/main/llama31-on-vertex-ai.md)\n","\n","> [**Text Generation Inference (TGI)**](https://github.com/huggingface/text-generation-inference) is a toolkit developed by Hugging Face for deploying and serving LLMs, with high performance text generation.\n","\n","> [**Hugging Face DLCs**](https://github.com/huggingface/Google-Cloud-Containers) are pre-built and optimized Deep Learning Containers (DLCs) maintained by Hugging Face and Google Cloud teams to simplify environment configuration for your ML workloads."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"tFy3H3aPgx12","executionInfo":{"status":"ok","timestamp":1736036344752,"user_tz":-540,"elapsed":6382,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Install Vertex AI SDK and other required packages\n","%pip install --upgrade --user --quiet google-cloud-aiplatform huggingface_hub"]},{"cell_type":"code","source":["# @title Define project information\n","PROJECT_ID = \"ai-hangsik\"  # @param {type:\"string\"}\n","LOCATION = \"us-central1\"  # @param {type:\"string\"}"],"metadata":{"cellView":"form","id":"MBY9UdUWH1d0","executionInfo":{"status":"ok","timestamp":1736041269024,"user_tz":-540,"elapsed":7,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{"id":"984a0526fb68","executionInfo":{"status":"ok","timestamp":1736041289064,"user_tz":-540,"elapsed":2624,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title GCP Authentication\n","\n","# Use OAuth to access the GCP environment.\n","import sys\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user(project_id=PROJECT_ID)\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"8d836e0210fe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736041375646,"user_tz":-540,"elapsed":10721,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"cdb57a43-7c3a-4786-85c5-8ce5a30f5612"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","Enter your token (input will not be visible): ··········\n","Add token as git credential? (Y/n) y\n"]}],"source":["# @title Authenticate your Hugging Face account\n","from huggingface_hub import interpreter_login\n","\n","interpreter_login()"]},{"cell_type":"code","source":["# @title Check Project ID and Location\n","PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n","LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n","\n","PROJECT_ID, LOCATION"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOk0hGTrIXxA","executionInfo":{"status":"ok","timestamp":1736041476710,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"e86b836d-6d66-40ad-910e-edccdcafb7ec"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('ai-hangsik', 'us-central1')"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"ee37e1544281"},"source":["## Requirements"]},{"cell_type":"markdown","metadata":{"id":"877cd3fb2dce"},"source":["You will need to have the following IAM roles set:\n","\n","- Artifact Registry Reader (roles/artifactregistry.reader)\n","- Vertex AI User (roles/aiplatform.user)\n","\n","For more information about granting roles, see [Manage access](https://cloud.google.com/iam/docs/granting-changing-revoking-access).\n","\n","---\n","\n","You will also need to enable the following APIs (if not enabled already):\n","\n","- Vertex AI API (aiplatform.googleapis.com)\n","- Artifact Registry API (artifactregistry.googleapis.com)\n","\n","For more information about API enablement, see [Enabling APIs](https://cloud.google.com/apis/docs/getting-started#enabling_apis).\n","\n","---\n","\n","To access Llama on Hugging Face, you’re required to review and agree to the model usage license on the Hugging Face Hub for any of the models from the [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct), and the access request will be processed inmediately."]},{"cell_type":"code","source":["shell_output = ! gcloud projects describe  $PROJECT_ID\n","project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n","\n","SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n","\n","print(f\"SERVICE_ACCOUNT: {SERVICE_ACCOUNT}\")"],"metadata":{"id":"A5mAMH24k0f2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gcloud projects add-iam-policy-binding $PROJECT_ID \\\n","    --member= $SERVICE_ACCOUNT \\\n","    --role=\"roles/artifactregistry.reader\""],"metadata":{"id":"vQnwo6Fck5te"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gcloud projects add-iam-policy-binding $PROJECT_ID \\\n","    --member= $SERVICE_ACCOUNT \\\n","    --role=\"roles/aiplatform.user\""],"metadata":{"id":"3j5UYJwKljJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gcloud services enable aiplatform.googleapis.com\n","!gcloud services enable artifactregistry.googleapis.com"],"metadata":{"id":"OM2bA1djluP2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EdvJRUWRNGHE"},"source":["## Register Google Gemma on Vertex AI"]},{"cell_type":"markdown","metadata":{"id":"28d24c673821"},"source":["To serve Gemma with Text Generation Inference (TGI) on Vertex AI, you start importing the model on Vertex AI Model Registry.\n","\n","The Vertex AI Model Registry is a central repository where you can manage the lifecycle of your ML models. From the Model Registry, you have an overview of your models so you can better organize, track, and train new versions. When you have a model version you would like to deploy, you can assign it to an endpoint directly from the registry, or using aliases, deploy models to an endpoint. The models on the Vertex AI Model Registry just contain the model configuration and not the weights per se, as it's not a storage.\n","\n","Before going into the code to upload or import a model on Vertex AI, let's quickly review the arguments provided to the `aiplatform.Model.upload` method:\n","\n","* **`display_name`** is the name that will be shown in the Vertex AI Model Registry.\n","\n","* **`serving_container_image_uri`** is the location of the Hugging Face DLC for TGI that will be used for serving the model.\n","\n","* **`serving_container_environment_variables`** are the environment variables that will be used during the container runtime, so these are aligned with the environment variables defined by `text-generation-inference`, which are analog to the [`text-generation-launcher` arguments](https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/launcher). Additionally, the Hugging Face DLCs for TGI also capture the `AIP_` environment variables from Vertex AI as in [Vertex AI Documentation - Custom container requirements for prediction](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements).\n","\n","    * `MODEL_ID` is the identifier of the model in the Hugging Face Hub. To explore all the supported models you can check [the Hugging Face Hub](https://huggingface.co/models?sort=trending&other=text-generation-inference).\n","    * `NUM_SHARD` is the number of shards to use if you don't want to use all GPUs on a given machine e.g. if you have two GPUs but you just want to use one for TGI then `NUM_SHARD=1`, otherwise it matches the `CUDA_VISIBLE_DEVICES`.\n","    * `MAX_INPUT_TOKENS` is the maximum allowed input length (expressed in number of tokens), the larger it is, the larger the prompt can be, but also more memory will be consumed.\n","    * `MAX_TOTAL_TOKENS` is the most important value to set as it defines the \"memory budget\" of running clients requests, the larger this value, the larger amount each request will be in your RAM and the less effective batching can be.\n","    * `MAX_BATCH_PREFILL_TOKENS` limits the number of tokens for the prefill operation, as it takes the most memory and is compute bound, it is interesting to limit the number of requests that can be sent.\n","    * `HUGGING_FACE_HUB_TOKEN` is the Hugging Face Hub token, required as [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) is a gated model.\n","\n","* (optional) **`serving_container_ports`** is the port where the Vertex AI endpoint |will be exposed, by default 8080.\n","\n","For more information on the supported `aiplatform.Model.upload` arguments, check [its Python reference](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_upload)."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"ef487ce082f3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736041726953,"user_tz":-540,"elapsed":2151,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"36a617cc-5121-4bb9-f1b4-d8805e9d7343"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.models:Creating Model\n","INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/721521243942/locations/us-central1/models/704449403334688768/operations/3795330417561698304\n","INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/721521243942/locations/us-central1/models/704449403334688768@1\n","INFO:google.cloud.aiplatform.models:To use this Model in another session:\n","INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/721521243942/locations/us-central1/models/704449403334688768@1')\n"]}],"source":["from huggingface_hub import get_token\n","\n","model = aiplatform.Model.upload(\n","    display_name=\"Llama-3.1-8B-Instruct\",\n","    serving_container_image_uri=\"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310\",\n","    serving_container_environment_variables={\n","        \"MODEL_ID\": \"meta-llama/Llama-3.1-8B-Instruct\",\n","        \"NUM_SHARD\": \"1\",\n","        \"MAX_INPUT_TOKENS\": \"512\",\n","        \"MAX_TOTAL_TOKENS\": \"1024\",\n","        \"MAX_BATCH_PREFILL_TOKENS\": \"1512\",\n","        \"HUGGING_FACE_HUB_TOKEN\": get_token(),\n","    },\n","    serving_container_ports=[8080],\n",")\n","model.wait()"]},{"cell_type":"markdown","metadata":{"id":"c427c0a87016"},"source":["## Deploy Google Gemma on Vertex AI"]},{"cell_type":"markdown","metadata":{"id":"ef4cd160dc09"},"source":["After the model is registered on Vertex AI, you can deploy the model to an endpoint.\n","\n","You need to first deploy a model to an endpoint before that model can be used to serve online predictions. Deploying a model associates physical resources with the model so it can serve online predictions with low latency.\n","\n","Before going into the code to deploy a model to an endpoint, let's quickly review the arguments provided to the `aiplatform.Model.deploy` method:\n","\n","- **`endpoint`** is the endpoint to deploy the model to, which is optional, and by default will be set to the model display name with the `_endpoint` suffix.\n","- **`machine_type`**, **`accelerator_type`** and **`accelerator_count`** are arguments that define which instance to use, and additionally, the accelerator to use and the number of accelerators, respectively. The `machine_type` and the `accelerator_type` are tied together, so you will need to select an instance that supports the accelerator that you are using and vice-versa. More information about the different instances at [Compute Engine Documentation - GPU machine types](https://cloud.google.com/compute/docs/gpus), and about the `accelerator_type` naming at [Vertex AI Documentation - MachineSpec](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec).\n","\n","For more information on the supported `aiplatform.Model.deploy` arguments, you can check [its Python reference](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_deploy)."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"e777427015db","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736042233087,"user_tz":-540,"elapsed":472789,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"43fed0bb-97f7-4534-ba58-cde0d80f39f1"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.models:Creating Endpoint\n","INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/721521243942/locations/us-central1/endpoints/9019277389972111360/operations/3311193457619369984\n","INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/721521243942/locations/us-central1/endpoints/9019277389972111360\n","INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n","INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/721521243942/locations/us-central1/endpoints/9019277389972111360')\n","INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/721521243942/locations/us-central1/endpoints/9019277389972111360\n","INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/721521243942/locations/us-central1/endpoints/9019277389972111360/operations/7182037337344311296\n","INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/721521243942/locations/us-central1/endpoints/9019277389972111360\n"]}],"source":["deployed_model = model.deploy(\n","    endpoint=aiplatform.Endpoint.create(display_name=\"Llama-3.1-8B-Instruct\"),\n","    machine_type=\"g2-standard-4\",\n","    accelerator_type=\"NVIDIA_L4\",\n","    accelerator_count=1,\n",")"]},{"cell_type":"markdown","metadata":{"id":"5f2380d3ea60"},"source":["> Note that the model deployment on Vertex AI can take around 15 to 25 minutes; most of the time being the allocation / reservation of the resources, setting up the network and security, and such."]},{"cell_type":"markdown","metadata":{"id":"9b3fd1898241"},"source":["## Online predictions on Vertex AI"]},{"cell_type":"markdown","metadata":{"id":"2aa9cee03bd0"},"source":["Once the model is deployed on Vertex AI, you can run the online predictions using the `aiplatform.Endpoint.predict` method, which will send the requests to the running endpoint in the `/predict` route specified within the container following Vertex AI I/O payload formatting.\n","\n","As you are serving a `text-generation` model fine-tuned for instruction-following, you will need to make sure that the chat template, if any, is applied correctly to the input conversation; meaning that `transformers` need to be installed so as to instantiate the `tokenizer` for [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) and run the `apply_chat_template` method over the input conversation before sending the input within the payload to the Vertex AI endpoint.\n","\n","> Note that the Messages API will be supported on Vertex AI on upcoming TGI releases, starting on 2.3, meaning that at the time of writing this post, the prompts need to be formatted before sending the request if you want to achieve nice results (assuming you are using an intruction-following model and not a base model)."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"d46c0a866ffc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736037709679,"user_tz":-540,"elapsed":4431,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"883bba37-58c1-43fa-ad20-4d44d5722f62"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m133.1/134.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.6/134.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip install --upgrade --user --quiet transformers jinja2"]},{"cell_type":"markdown","metadata":{"id":"7e1ab8de0b5c"},"source":["After the installation is complete, the following snippet will apply the chat template to the conversation:"]},{"cell_type":"code","source":["import os\n","from huggingface_hub import get_token\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    \"meta-llama/Llama-3.1-8B-Instruct\",\n","    token=get_token(),\n",")\n","\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are an assistant that responds as an AI expert.\"},\n","    {\"role\": \"user\", \"content\": \"What's Deep learning?\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True,\n",")\n"],"metadata":{"id":"uft7tDd-QXkg","executionInfo":{"status":"ok","timestamp":1736043593623,"user_tz":-540,"elapsed":1128,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["output = deployed_model.predict(\n","    instances=[\n","        {\n","            \"inputs\": inputs,\n","            \"parameters\": {\n","                \"max_new_tokens\": 128,\n","                \"do_sample\": True,\n","                \"top_p\": 0.95,\n","                \"temperature\": 0.7,\n","            },\n","        },\n","    ]\n",")\n","\n","output\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQtghdzEQXh3","executionInfo":{"status":"ok","timestamp":1736043604906,"user_tz":-540,"elapsed":8640,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"120f22e2-b79c-41eb-9f48-0ac12abbefab"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Prediction(predictions=['Deep learning is a subfield of machine learning (ML) that specializes in artificial neural networks with multiple layers. These networks are modeled after the human brain\\'s neural structures and are capable of learning complex patterns in data.\\n\\nIn a traditional neural network, the data flows through a series of interconnected nodes or \"neurons,\" which perform computations to transform the input data into a predicted output. The architecture of deep neural networks involves numerous hidden layers of these nodes, with each layer learning representations of the data from the previous layer.\\n\\nDeep learning models are particularly useful for image and speech recognition tasks. Some of the key characteristics of deep learning include:\\n\\n1.'], deployed_model_id='6301157102761017344', metadata=None, model_version_id='1', model_resource_name='projects/721521243942/locations/us-central1/models/704449403334688768', explanations=None)"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"d0522c9b9bc2"},"source":["Which is what you will be sending within the payload to the deployed Vertex AI Endpoint, as well as [the generation parameters](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation)."]},{"cell_type":"markdown","metadata":{"id":"b28efb473c7e"},"source":["#### From a different session\n","\n","To run the online prediction from a different session, you can run the following snippet."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"e003ab03f30f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736044152315,"user_tz":-540,"elapsed":9977,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"75854433-775a-4076-a4ba-fce850ff03a0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Prediction(predictions=[\"Deep learning is a subset of machine learning that utilizes neural networks to analyze and learn from data. In essence, it's a type of artificial intelligence (AI) that is modeled after the human brain's functions of vision, speech, pattern recognition, and decision-making.\\n\\nHere's a more detailed explanation:\\n\\n1. **Neural Networks**: Neural networks are the base of deep learning. They consist of layers of interconnected nodes (neurons) that process and transmit information. Each node receives input from the previous layer, performs a computation, and then sends the output to the next layer.\\n2. **Hierarchical Representation**: Deep learning models consist of\"], deployed_model_id='6301157102761017344', metadata=None, model_version_id='1', model_resource_name='projects/721521243942/locations/us-central1/models/704449403334688768', explanations=None)"]},"metadata":{},"execution_count":35}],"source":["import os\n","\n","from google.cloud import aiplatform\n","\n","\n","aiplatform.init(project=PROJECT_ID, location=LOCATION)\n","\n","endpoint_display_name = (\n","    \"Llama-3.1-8B-Instruct\"  # TODO: change to your endpoint display name\n",")\n","\n","\n","# Iterates over all the Vertex AI Endpoints within the current project and keeps the first match (if any), otherwise set to None\n","ENDPOINT_ID = next(\n","    (\n","        endpoint.name\n","        for endpoint in aiplatform.Endpoint.list()\n","        if endpoint.display_name == endpoint_display_name\n","    ),\n","    None,\n",")\n","assert ENDPOINT_ID, (\n","    \"`ENDPOINT_ID` is not set, please make sure that the `endpoint_display_name` is correct at \"\n","    f\"https://console.cloud.google.com/vertex-ai/online-prediction/endpoints?project={os.getenv('PROJECT_ID')}\"\n",")\n","\n","endpoint = aiplatform.Endpoint(\n","    f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}\"\n",")\n","output = endpoint.predict(\n","    instances=[\n","        {\n","            \"inputs\": \"<bos><start_of_turn>user\\nWhat's Deep Learning? in details<end_of_turn>\\n<start_of_turn>model\\n\",\n","            \"parameters\": {\n","                \"max_new_tokens\": 128,\n","                \"do_sample\": True,\n","                \"top_p\": 0.95,\n","                \"temperature\": 0.7,\n","            },\n","        },\n","    ],\n",")\n","output"]},{"cell_type":"markdown","metadata":{"id":"7cc0b19ab641"},"source":["### Via gcloud"]},{"cell_type":"markdown","metadata":{"id":"8fed7007c31c"},"source":["You can also send the requests using the `gcloud` CLI via the `gcloud ai endpoints` command.\n","\n","> Note that, before proceeding, you should either replace the values or set the following environment variables in advance from the Python variables set in the example, as follows:\n",">\n","> ```python\n","> import os\n","> os.environ[\"PROJECT_ID\"] = PROJECT_ID\n","> os.environ[\"LOCATION\"] = LOCATION\n","> os.environ[\"ENDPOINT_NAME\"] = \"google--gemma-7b-it-endpoint\"\n","> ```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7fb1547d409f"},"outputs":[],"source":["%%bash\n","ENDPOINT_ID=$(gcloud ai endpoints list \\\n","  --project=$PROJECT_ID \\\n","  --region=$LOCATION \\\n","  --filter=\"display_name=$ENDPOINT_NAME\" \\\n","  --format=\"value(name)\" \\\n","  | cut -d'/' -f6)\n","\n","echo '{\n","  \"instances\": [\n","    {\n","      \"inputs\": \"<bos><start_of_turn>user\\nWhat'\\''s Deep Learning?<end_of_turn>\\n<start_of_turn>model\\n\",\n","      \"parameters\": {\n","        \"max_new_tokens\": 20,\n","        \"do_sample\": true,\n","        \"top_p\": 0.95,\n","        \"temperature\": 1.0\n","      }\n","    }\n","  ]\n","}' | gcloud ai endpoints predict $ENDPOINT_ID \\\n","  --project=$PROJECT_ID \\\n","  --region=$LOCATION \\\n","  --json-request=\"-\""]},{"cell_type":"markdown","metadata":{"id":"7e588a65759e"},"source":["### Via cURL"]},{"cell_type":"markdown","metadata":{"id":"cf3de731bcf4"},"source":["Alternatively, you can also send the requests via `cURL`.\n","\n","> Note that, before proceeding, you should either replace the values or set the following environment variables in advance from the Python variables set in the example, as follows:\n",">\n","> ```python\n","> import os\n","> os.environ[\"PROJECT_ID\"] = PROJECT_ID\n","> os.environ[\"LOCATION\"] = LOCATION\n","> os.environ[\"ENDPOINT_NAME\"] = \"google--gemma-7b-it-endpoint\"\n","> ```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a71da4e1152b"},"outputs":[],"source":["%%bash\n","ENDPOINT_ID=$(gcloud ai endpoints list \\\n","  --project=$PROJECT_ID \\\n","  --region=$LOCATION \\\n","  --filter=\"display_name=$ENDPOINT_NAME\" \\\n","  --format=\"value(name)\" \\\n","  | cut -d'/' -f6)\n","\n","curl -X POST \\\n","    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n","    -H \"Content-Type: application/json\" \\\n","    https://$LOCATION-aiplatform.googleapis.com/v1/projects/$PROJECT_ID/locations/$LOCATION/endpoints/$ENDPOINT_ID:predict \\\n","    -d '{\n","        \"instances\": [\n","            {\n","                \"inputs\": \"<bos><start_of_turn>user\\nWhat'\\''s Deep Learning?<end_of_turn>\\n<start_of_turn>model\\n\",\n","                \"parameters\": {\n","                    \"max_new_tokens\": 20,\n","                    \"do_sample\": true,\n","                    \"top_p\": 0.95,\n","                    \"temperature\": 1.0\n","                }\n","            }\n","        ]\n","    }'"]},{"cell_type":"markdown","metadata":{"id":"f6f17f9aff65"},"source":["## Cleaning up"]},{"cell_type":"markdown","metadata":{"id":"2a4e033321ad"},"source":["Finally, you can already release the resources that you've created as follows, to avoid unnecessary costs:\n","\n","- `deployed_model.undeploy_all` to undeploy the model from all the endpoints.\n","- `deployed_model.delete` to delete the endpoint/s where the model was deployed gracefully, after the `undeploy_all` method.\n","- `model.delete` to delete the model from the registry."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b074ffdbd0c4"},"outputs":[],"source":["deployed_model.undeploy_all()\n","deployed_model.delete()\n","model.delete()"]},{"cell_type":"markdown","metadata":{"id":"6bc4da2fcc35"},"source":["Alternatively, you can also remove those from the Google Cloud Console following the steps:\n","\n","- Go to Vertex AI in Google Cloud\n","- Go to Deploy and use -> Online prediction\n","- Click on the endpoint and then on the deployed model/s to \"Undeploy model from endpoint\"\n","- Then go back to the endpoint list and remove the endpoint\n","- Finally, go to Deploy and use -> Model Registry, and remove the model"]},{"cell_type":"markdown","metadata":{"id":"137d82da56f2"},"source":["## References\n","\n","- [GitHub Repository - Hugging Face DLCs for Google Cloud](https://github.com/huggingface/Google-Cloud-Containers): contains all the containers developed by the collaboration of both Hugging Face and Google Cloud teams; as well as a lot of examples on both training and inference, covering both CPU and GPU, as well support for most of the models within the Hugging Face Hub.\n","- [Google Cloud Documentation - Hugging Face DLCs](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face): contains a table with the latest released Hugging Face DLCs on Google Cloud.\n","- [Google Artifact Registry - Hugging Face DLCs](https://console.cloud.google.com/artifacts/docker/deeplearning-platform-release/us/gcr.io): contains all the DLCs released by Google Cloud that can be used.\n","- [Hugging Face Documentation - Google Cloud](https://huggingface.co/docs/google-cloud): contains the official Hugging Face documentation for the Google Cloud DLCs."]}],"metadata":{"colab":{"provenance":[{"file_id":"1YA_Q5gR_rsWKnQeO3b9LovVK3TsidHmG","timestamp":1736035804576},{"file_id":"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/vertex_ai_text_generation_inference_gemma.ipynb","timestamp":1736019848158}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}