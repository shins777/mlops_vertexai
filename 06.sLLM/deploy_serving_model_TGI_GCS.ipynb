{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ur8xi4C7S06n"},"outputs":[],"source":["# Copyright 2024 Forusone\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"JAPoU8Sm5E6e"},"source":["# Deploy model in GCS and Serving it with Text Generation Inference (TGI) on Vertex AI\n","\n","> [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n","\n","> [Import a model programmatically](https://cloud.google.com/vertex-ai/docs/model-registry/import-model#import_a_model_programmatically)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"tFy3H3aPgx12","executionInfo":{"status":"ok","timestamp":1736064898254,"user_tz":-540,"elapsed":7954,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Install Vertex AI SDK and other required packages\n","%pip install --upgrade --user --quiet google-cloud-aiplatform \\\n","                                      huggingface_hub \\\n","                                      transformers"]},{"cell_type":"code","source":["# @title Define project information\n","PROJECT_ID = \"ai-hangsik\"  # @param {type:\"string\"}\n","LOCATION = \"us-central1\"  # @param {type:\"string\"}"],"metadata":{"cellView":"form","id":"MBY9UdUWH1d0","executionInfo":{"status":"ok","timestamp":1736064901160,"user_tz":-540,"elapsed":9,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"984a0526fb68","executionInfo":{"status":"ok","timestamp":1736064917572,"user_tz":-540,"elapsed":14614,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title GCP Authentication\n","\n","# Use OAuth to access the GCP environment.\n","import sys\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user(project_id=PROJECT_ID)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"8d836e0210fe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736053485142,"user_tz":-540,"elapsed":15274,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"d521a366-8325-477c-b9ea-47eb864c0274"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Enter your token (input will not be visible): ··········\n","Add token as git credential? (Y/n) y\n"]}],"source":["# @title Authenticate your Hugging Face account\n","from huggingface_hub import interpreter_login\n","\n","interpreter_login()"]},{"cell_type":"code","source":["# @title Check Project ID and Location\n","import os\n","\n","PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n","LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n","\n","PROJECT_ID, LOCATION"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOk0hGTrIXxA","executionInfo":{"status":"ok","timestamp":1736064946049,"user_tz":-540,"elapsed":9,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"b8adb997-a72c-476d-b71c-93bbff93194f"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('ai-hangsik', 'us-central1')"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"ee37e1544281"},"source":["## Requirements"]},{"cell_type":"markdown","metadata":{"id":"877cd3fb2dce"},"source":["You will need to have the following IAM roles set:\n","\n","- Artifact Registry Reader (roles/artifactregistry.reader)\n","- Vertex AI User (roles/aiplatform.user)\n","\n","For more information about granting roles, see [Manage access](https://cloud.google.com/iam/docs/granting-changing-revoking-access).\n","\n","---\n","\n","You will also need to enable the following APIs (if not enabled already):\n","\n","- Vertex AI API (aiplatform.googleapis.com)\n","- Artifact Registry API (artifactregistry.googleapis.com)\n","\n","For more information about API enablement, see [Enabling APIs](https://cloud.google.com/apis/docs/getting-started#enabling_apis).\n","\n","---\n","\n","To access Llama on Hugging Face, you’re required to review and agree to the model usage license on the Hugging Face Hub for any of the models from the [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct), and the access request will be processed inmediately."]},{"cell_type":"code","source":["!gcloud services enable aiplatform.googleapis.com\n","!gcloud services enable artifactregistry.googleapis.com"],"metadata":{"id":"OM2bA1djluP2","executionInfo":{"status":"ok","timestamp":1736063045495,"user_tz":-540,"elapsed":3239,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# from google.cloud import aiplatform\n","\n","# def upload_model_from_gcs(project_id, model_display_name, artifact_uri, serving_container_image_uri):\n","#   \"\"\"Uploads a model to Vertex AI from a GCS location.\n","\n","#   Args:\n","#     project_id: Your Google Cloud project ID.\n","#     model_display_name: The display name for the uploaded model.\n","#     artifact_uri: The GCS URI where the model artifact is stored.\n","#     serving_container_image_uri: The URI of the serving container image.\n","#   \"\"\"\n","\n","#   aiplatform.init(project=project_id, location='us-central1')  # Replace with your region\n","\n","#   model = aiplatform.Model.upload(\n","#       display_name=model_display_name,\n","#       artifact_uri=artifact_uri,\n","#       serving_container_image_uri=serving_container_image_uri,\n","#       sync=True  # Wait for the upload to complete\n","#   )\n","\n","#   print(f\"Model uploaded: {model.resource_name}\")\n","#   return model"],"metadata":{"id":"GgmtrIpD4RYr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Published TGI Containers\n","!gcloud container images list --repository=\"us-docker.pkg.dev/deeplearning-platform-release/gcr.io\" | grep \"huggingface-text-generation-inference\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w99XpDipkb0B","executionInfo":{"status":"ok","timestamp":1736065573579,"user_tz":-540,"elapsed":2530,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"d0257898-4fa2-4da1-c186-adbdd01632e1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.1-4.ubuntu2204.py310\n","us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-0.ubuntu2204.py310\n","us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-1.ubuntu2204.py310\n","us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310\n","us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu124.2-3.ubuntu2204.py311\n","us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu124.2-4.ubuntu2204.py311\n"]}]},{"cell_type":"code","source":["# @title Upload model from GCS\n","from google.cloud import aiplatform\n","\n","project_id = \"ai-hangsik\"  # Replace with your project ID\n","model_display_name = \"Llama-3.1-8B-Instruct_gcs\"\n","artifact_uri = \"gs://sllm_0106/model\"  # Replace with your GCS URI\n","serving_container_image_uri=\"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310\"\n","#serving_container_image_uri=\"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu124.2-4.ubuntu2204.py311\"\n","\n","aiplatform.init(project=project_id, location='us-central1')  # Replace with your region\n","\n","model = aiplatform.Model.upload(\n","    display_name=model_display_name,\n","    artifact_uri=artifact_uri,\n","    serving_container_image_uri=serving_container_image_uri,\n","    serving_container_environment_variables={\n","            \"MODEL_ID\": \"meta-llama/Llama-3.1-8B-Instruct\",\n","            \"NUM_SHARD\": \"1\",\n","            \"MAX_INPUT_TOKENS\": \"512\",\n","            \"MAX_TOTAL_TOKENS\": \"1024\",\n","            \"MAX_BATCH_PREFILL_TOKENS\": \"1512\",\n","        }\n",")\n","\n","print(f\"Model uploaded: {model.resource_name}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_Sw2oU74RV6","executionInfo":{"status":"ok","timestamp":1736065125971,"user_tz":-540,"elapsed":128633,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"2031e60e-0022-45ef-ab60-0e162465ca18"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.models:Creating Model\n","INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/721521243942/locations/us-central1/models/6947564386777038848/operations/4051683752600928256\n","INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/721521243942/locations/us-central1/models/6947564386777038848@1\n","INFO:google.cloud.aiplatform.models:To use this Model in another session:\n","INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/721521243942/locations/us-central1/models/6947564386777038848@1')\n"]},{"output_type":"stream","name":"stdout","text":["Model uploaded: projects/721521243942/locations/us-central1/models/6947564386777038848\n"]}]},{"cell_type":"code","source":["deployed_model = model.deploy(\n","    endpoint=aiplatform.Endpoint.create(display_name=\"Llama-3.1-8B-Instruct_gcs\"),\n","    machine_type=\"g2-standard-4\",\n","    accelerator_type=\"NVIDIA_L4\",\n","    accelerator_count=1,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":567},"id":"XPTr01Fl4RQ3","executionInfo":{"status":"error","timestamp":1736065534665,"user_tz":-540,"elapsed":394680,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"419db2c9-e9c8-4d11-d70f-443cc5f09935"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.models:Creating Endpoint\n","INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/721521243942/locations/us-central1/endpoints/9197169575253245952/operations/8732049665345716224\n","INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/721521243942/locations/us-central1/endpoints/9197169575253245952\n","INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n","INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/721521243942/locations/us-central1/endpoints/9197169575253245952')\n","INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/721521243942/locations/us-central1/endpoints/9197169575253245952\n","INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/721521243942/locations/us-central1/endpoints/9197169575253245952/operations/6079429484824494080\n"]},{"output_type":"error","ename":"FailedPrecondition","evalue":"400 Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=721521243942&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%229197169575253245952%22%0Aresource.labels.location%3D%22us-central1%22. 9: Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=721521243942&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%229197169575253245952%22%0Aresource.labels.location%3D%22us-central1%22.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-3e9dc60f067a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m deployed_model = model.deploy(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maiplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEndpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Llama-3.1-8B-Instruct_gcs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmachine_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"g2-standard-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NVIDIA_L4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maccelerator_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, tpu_topology, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, enable_access_logging, disable_container_logging, private_service_connect_config, deployment_resource_pool, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, spot, fast_tryout_enabled, system_labels)\u001b[0m\n\u001b[1;32m   5367\u001b[0m         )\n\u001b[1;32m   5368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5369\u001b[0;31m         return self._deploy(\n\u001b[0m\u001b[1;32m   5370\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5371\u001b[0m             \u001b[0mdeployed_model_display_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployed_model_display_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, tpu_topology, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, service_account, explanation_spec, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, spot, enable_access_logging, disable_container_logging, private_service_connect_config, deployment_resource_pool, fast_tryout_enabled, system_labels)\u001b[0m\n\u001b[1;32m   5604\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_start_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Deploying model to\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5606\u001b[0;31m         endpoint._deploy_call(\n\u001b[0m\u001b[1;32m   5607\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5608\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy_call\u001b[0;34m(cls, api_client, endpoint_resource_name, model, endpoint_resource_traffic_split, network, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, tpu_topology, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, service_account, explanation_spec, metadata, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, spot, enable_access_logging, disable_container_logging, deployment_resource_pool, fast_tryout_enabled, system_labels)\u001b[0m\n\u001b[1;32m   2022\u001b[0m         )\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2024\u001b[0;31m         \u001b[0moperation_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m     def undeploy(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPrecondition\u001b[0m: 400 Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=721521243942&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%229197169575253245952%22%0Aresource.labels.location%3D%22us-central1%22. 9: Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=721521243942&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%229197169575253245952%22%0Aresource.labels.location%3D%22us-central1%22."]}]},{"cell_type":"markdown","metadata":{"id":"9b3fd1898241"},"source":["## Online predictions on Vertex AI"]},{"cell_type":"markdown","metadata":{"id":"2aa9cee03bd0"},"source":["Once the model is deployed on Vertex AI, you can run the online predictions using the `aiplatform.Endpoint.predict` method, which will send the requests to the running endpoint in the `/predict` route specified within the container following Vertex AI I/O payload formatting.\n","\n","As you are serving a `text-generation` model fine-tuned for instruction-following, you will need to make sure that the chat template, if any, is applied correctly to the input conversation; meaning that `transformers` need to be installed so as to instantiate the `tokenizer` for [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) and run the `apply_chat_template` method over the input conversation before sending the input within the payload to the Vertex AI endpoint.\n","\n","> Note that the Messages API will be supported on Vertex AI on upcoming TGI releases, starting on 2.3, meaning that at the time of writing this post, the prompts need to be formatted before sending the request if you want to achieve nice results (assuming you are using an intruction-following model and not a base model)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d46c0a866ffc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736037709679,"user_tz":-540,"elapsed":4431,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"883bba37-58c1-43fa-ad20-4d44d5722f62"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m133.1/134.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.6/134.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip install --upgrade --user --quiet transformers jinja2"]},{"cell_type":"markdown","metadata":{"id":"7e1ab8de0b5c"},"source":["After the installation is complete, the following snippet will apply the chat template to the conversation:"]},{"cell_type":"code","source":["import os\n","from huggingface_hub import get_token\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    \"meta-llama/Llama-3.1-8B-Instruct\",\n","    token=get_token(),\n",")\n","\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are an assistant that responds as an AI expert.\"},\n","    {\"role\": \"user\", \"content\": \"What's Deep learning?\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True,\n",")"],"metadata":{"id":"uft7tDd-QXkg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = deployed_model.predict(\n","    instances=[\n","        {\n","            \"inputs\": \"What's Deep Learning? in details\",\n","            \"parameters\": {\n","                \"max_new_tokens\": 128,\n","                \"do_sample\": True,\n","                \"top_p\": 0.95,\n","                \"temperature\": 0.7,\n","            },\n","        },\n","    ]\n",")\n","\n","output\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQtghdzEQXh3","executionInfo":{"status":"ok","timestamp":1736043604906,"user_tz":-540,"elapsed":8640,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"120f22e2-b79c-41eb-9f48-0ac12abbefab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Prediction(predictions=['Deep learning is a subfield of machine learning (ML) that specializes in artificial neural networks with multiple layers. These networks are modeled after the human brain\\'s neural structures and are capable of learning complex patterns in data.\\n\\nIn a traditional neural network, the data flows through a series of interconnected nodes or \"neurons,\" which perform computations to transform the input data into a predicted output. The architecture of deep neural networks involves numerous hidden layers of these nodes, with each layer learning representations of the data from the previous layer.\\n\\nDeep learning models are particularly useful for image and speech recognition tasks. Some of the key characteristics of deep learning include:\\n\\n1.'], deployed_model_id='6301157102761017344', metadata=None, model_version_id='1', model_resource_name='projects/721521243942/locations/us-central1/models/704449403334688768', explanations=None)"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"d0522c9b9bc2"},"source":["Which is what you will be sending within the payload to the deployed Vertex AI Endpoint, as well as [the generation parameters](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation)."]},{"cell_type":"markdown","metadata":{"id":"b28efb473c7e"},"source":["#### From a different session\n","\n","To run the online prediction from a different session, you can run the following snippet."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e003ab03f30f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736044152315,"user_tz":-540,"elapsed":9977,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"75854433-775a-4076-a4ba-fce850ff03a0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Prediction(predictions=[\"Deep learning is a subset of machine learning that utilizes neural networks to analyze and learn from data. In essence, it's a type of artificial intelligence (AI) that is modeled after the human brain's functions of vision, speech, pattern recognition, and decision-making.\\n\\nHere's a more detailed explanation:\\n\\n1. **Neural Networks**: Neural networks are the base of deep learning. They consist of layers of interconnected nodes (neurons) that process and transmit information. Each node receives input from the previous layer, performs a computation, and then sends the output to the next layer.\\n2. **Hierarchical Representation**: Deep learning models consist of\"], deployed_model_id='6301157102761017344', metadata=None, model_version_id='1', model_resource_name='projects/721521243942/locations/us-central1/models/704449403334688768', explanations=None)"]},"metadata":{},"execution_count":35}],"source":["import os\n","\n","from google.cloud import aiplatform\n","\n","\n","aiplatform.init(project=PROJECT_ID, location=LOCATION)\n","\n","endpoint_display_name = (\n","    \"Llama-3.1-8B-Instruct\"  # TODO: change to your endpoint display name\n",")\n","\n","\n","# Iterates over all the Vertex AI Endpoints within the current project and keeps the first match (if any), otherwise set to None\n","ENDPOINT_ID = next(\n","    (\n","        endpoint.name\n","        for endpoint in aiplatform.Endpoint.list()\n","        if endpoint.display_name == endpoint_display_name\n","    ),\n","    None,\n",")\n","assert ENDPOINT_ID, (\n","    \"`ENDPOINT_ID` is not set, please make sure that the `endpoint_display_name` is correct at \"\n","    f\"https://console.cloud.google.com/vertex-ai/online-prediction/endpoints?project={os.getenv('PROJECT_ID')}\"\n",")\n","\n","endpoint = aiplatform.Endpoint(\n","    f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}\"\n",")\n","output = endpoint.predict(\n","    instances=[\n","        {\n","            \"inputs\": \"<bos><start_of_turn>user\\nWhat's Deep Learning? in details<end_of_turn>\\n<start_of_turn>model\\n\",\n","            \"parameters\": {\n","                \"max_new_tokens\": 128,\n","                \"do_sample\": True,\n","                \"top_p\": 0.95,\n","                \"temperature\": 0.7,\n","            },\n","        },\n","    ],\n",")\n","output"]},{"cell_type":"markdown","metadata":{"id":"7cc0b19ab641"},"source":["### Via gcloud"]},{"cell_type":"markdown","metadata":{"id":"8fed7007c31c"},"source":["You can also send the requests using the `gcloud` CLI via the `gcloud ai endpoints` command.\n","\n","> Note that, before proceeding, you should either replace the values or set the following environment variables in advance from the Python variables set in the example, as follows:\n",">\n","> ```python\n","> import os\n","> os.environ[\"PROJECT_ID\"] = PROJECT_ID\n","> os.environ[\"LOCATION\"] = LOCATION\n","> os.environ[\"ENDPOINT_NAME\"] = \"google--gemma-7b-it-endpoint\"\n","> ```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7fb1547d409f"},"outputs":[],"source":["%%bash\n","ENDPOINT_ID=$(gcloud ai endpoints list \\\n","  --project=$PROJECT_ID \\\n","  --region=$LOCATION \\\n","  --filter=\"display_name=$ENDPOINT_NAME\" \\\n","  --format=\"value(name)\" \\\n","  | cut -d'/' -f6)\n","\n","echo '{\n","  \"instances\": [\n","    {\n","      \"inputs\": \"<bos><start_of_turn>user\\nWhat'\\''s Deep Learning?<end_of_turn>\\n<start_of_turn>model\\n\",\n","      \"parameters\": {\n","        \"max_new_tokens\": 20,\n","        \"do_sample\": true,\n","        \"top_p\": 0.95,\n","        \"temperature\": 1.0\n","      }\n","    }\n","  ]\n","}' | gcloud ai endpoints predict $ENDPOINT_ID \\\n","  --project=$PROJECT_ID \\\n","  --region=$LOCATION \\\n","  --json-request=\"-\""]},{"cell_type":"markdown","metadata":{"id":"7e588a65759e"},"source":["### Via cURL"]},{"cell_type":"markdown","metadata":{"id":"cf3de731bcf4"},"source":["Alternatively, you can also send the requests via `cURL`.\n","\n","> Note that, before proceeding, you should either replace the values or set the following environment variables in advance from the Python variables set in the example, as follows:\n",">\n","> ```python\n","> import os\n","> os.environ[\"PROJECT_ID\"] = PROJECT_ID\n","> os.environ[\"LOCATION\"] = LOCATION\n","> os.environ[\"ENDPOINT_NAME\"] = \"google--gemma-7b-it-endpoint\"\n","> ```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a71da4e1152b"},"outputs":[],"source":["%%bash\n","ENDPOINT_ID=$(gcloud ai endpoints list \\\n","  --project=$PROJECT_ID \\\n","  --region=$LOCATION \\\n","  --filter=\"display_name=$ENDPOINT_NAME\" \\\n","  --format=\"value(name)\" \\\n","  | cut -d'/' -f6)\n","\n","curl -X POST \\\n","    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n","    -H \"Content-Type: application/json\" \\\n","    https://$LOCATION-aiplatform.googleapis.com/v1/projects/$PROJECT_ID/locations/$LOCATION/endpoints/$ENDPOINT_ID:predict \\\n","    -d '{\n","        \"instances\": [\n","            {\n","                \"inputs\": \"<bos><start_of_turn>user\\nWhat'\\''s Deep Learning?<end_of_turn>\\n<start_of_turn>model\\n\",\n","                \"parameters\": {\n","                    \"max_new_tokens\": 20,\n","                    \"do_sample\": true,\n","                    \"top_p\": 0.95,\n","                    \"temperature\": 1.0\n","                }\n","            }\n","        ]\n","    }'"]},{"cell_type":"markdown","metadata":{"id":"f6f17f9aff65"},"source":["## Cleaning up"]},{"cell_type":"markdown","metadata":{"id":"2a4e033321ad"},"source":["Finally, you can already release the resources that you've created as follows, to avoid unnecessary costs:\n","\n","- `deployed_model.undeploy_all` to undeploy the model from all the endpoints.\n","- `deployed_model.delete` to delete the endpoint/s where the model was deployed gracefully, after the `undeploy_all` method.\n","- `model.delete` to delete the model from the registry."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b074ffdbd0c4"},"outputs":[],"source":["deployed_model.undeploy_all()\n","deployed_model.delete()\n","model.delete()"]},{"cell_type":"markdown","metadata":{"id":"6bc4da2fcc35"},"source":["Alternatively, you can also remove those from the Google Cloud Console following the steps:\n","\n","- Go to Vertex AI in Google Cloud\n","- Go to Deploy and use -> Online prediction\n","- Click on the endpoint and then on the deployed model/s to \"Undeploy model from endpoint\"\n","- Then go back to the endpoint list and remove the endpoint\n","- Finally, go to Deploy and use -> Model Registry, and remove the model"]},{"cell_type":"markdown","metadata":{"id":"137d82da56f2"},"source":["## References\n","\n","- [GitHub Repository - Hugging Face DLCs for Google Cloud](https://github.com/huggingface/Google-Cloud-Containers): contains all the containers developed by the collaboration of both Hugging Face and Google Cloud teams; as well as a lot of examples on both training and inference, covering both CPU and GPU, as well support for most of the models within the Hugging Face Hub.\n","- [Google Cloud Documentation - Hugging Face DLCs](https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face): contains a table with the latest released Hugging Face DLCs on Google Cloud.\n","- [Google Artifact Registry - Hugging Face DLCs](https://console.cloud.google.com/artifacts/docker/deeplearning-platform-release/us/gcr.io): contains all the DLCs released by Google Cloud that can be used.\n","- [Hugging Face Documentation - Google Cloud](https://huggingface.co/docs/google-cloud): contains the official Hugging Face documentation for the Google Cloud DLCs."]}],"metadata":{"colab":{"provenance":[{"file_id":"1qwoO1IVNLMllwdCBViQKQomKaJtXWNEX","timestamp":1736052449904},{"file_id":"1YA_Q5gR_rsWKnQeO3b9LovVK3TsidHmG","timestamp":1736035804576},{"file_id":"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/serving/vertex_ai_text_generation_inference_gemma.ipynb","timestamp":1736019848158}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}