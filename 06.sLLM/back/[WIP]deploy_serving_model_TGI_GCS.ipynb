{"cells":[{"cell_type":"code","source":["# Copyright 2024 Forusone\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"metadata":{"id":"yjEbJ2J5Fkpb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q4wt4y-h92QJ"},"source":["# Deploy_serving_model_TGI_GCS.ipynb\n","* [Deploy Gemma 7B with TGI DLC from GCS on Vertex AI](https://github.com/huggingface/Google-Cloud-Containers/blob/main/examples/vertex-ai/notebooks/deploy-gemma-from-gcs-on-vertex-ai/vertex-notebook.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KyCokWIE92QK"},"outputs":[],"source":["# @title Install Vertex AI SDK and other required packages\n","%pip install --upgrade --user --quiet google-cloud-aiplatform \\\n","                                      huggingface_hub[hf_transfer] \\\n","                                      transformers"]},{"cell_type":"code","source":["# @title Define project information\n","PROJECT_ID = \"ai-hangsik\"  # @param {type:\"string\"}\n","LOCATION = \"us-central1\"  # @param {type:\"string\"}\n","BUCKET_URI = \"gs://sllm_0103\"  # @param {type:\"string\"}\n","MODEL_NAME=\"meta-llama/Llama-3.1-8B-Instruct\"  # @param {type:\"string\"}\n","ARTIFACT_NAME = \"llama3.1_8b_inst\"  # @param {type:\"string\"}"],"metadata":{"cellView":"form","id":"GuBmB2ZqGDBn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title GCP Authentication\n","\n","# Use OAuth to access the GCP environment.\n","import sys\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user(project_id=PROJECT_ID)"],"metadata":{"id":"n0CW0qxiGxhc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Published TGI Containers\n","!gcloud container images list --repository=\"us-docker.pkg.dev/deeplearning-platform-release/gcr.io\" | grep \"huggingface-text-generation-inference\""],"metadata":{"id":"ORJ6GImqGTxH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title TGI container uri\n","CONTAINER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu124.2-4.ubuntu2204.py311\" # @param {type:\"string\"}"],"metadata":{"cellView":"form","id":"SdV5wIQFGgNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKdKN0AY92QK"},"outputs":[],"source":["# @title Enable apis\n","!gcloud services enable aiplatform.googleapis.com\n","!gcloud services enable compute.googleapis.com\n","!gcloud services enable container.googleapis.com\n","!gcloud services enable containerregistry.googleapis.com\n","!gcloud services enable containerfilesystem.googleapis.com"]},{"cell_type":"code","source":["# @title Create a bucket.\n","! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"],"metadata":{"id":"QuLhxKPJHXuw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMmR47ZL92QL"},"outputs":[],"source":["# @title Authenticate your Hugging Face account\n","from huggingface_hub import interpreter_login\n","interpreter_login()"]},{"cell_type":"markdown","metadata":{"id":"FP__FwBp92QL"},"source":["After `huggingface_hub` installation and login are completed, you can run the following bash script to download the model locally within a temporary directory, and then upload those to the GCS Bucket."]},{"cell_type":"markdown","source":["### Download a model in Hugging face into GCS"],"metadata":{"id":"FU39ylEQHvPh"}},{"cell_type":"code","source":["# @title Download configuration\n","LOCAL_DIR=\"model/llama3.1-8b-it\"\n","! mkdir -p {LOCAL_DIR}"],"metadata":{"id":"97GlDyJuXFO9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Download model from Hugging face\n","\n","! huggingface-cli download meta-llama/Llama-3.1-8B-Instruct --exclude \"*.bin\" \"*.pth\" \"*.gguf\" \".gitattributes\" --local-dir {LOCAL_DIR}"],"metadata":{"id":"t0j75rjKXIzq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Upload model files into GCS\n","! gsutil -o GSUtil:parallel_composite_upload_threshold=150M -m cp -e -r {LOCAL_DIR}/* {BUCKET_URI}/{ARTIFACT_NAME}"],"metadata":{"id":"VQLJsyYkXaeL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bPo8gPGm92QL"},"source":["## Register model on Vertex AI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NawKaOOH92QL"},"outputs":[],"source":["# @title Initalize vertex ai\n","import os\n","from google.cloud import aiplatform\n","\n","aiplatform.init(\n","    project=PROJECT_ID,\n","    location=LOCATION,\n","    staging_bucket=BUCKET_URI,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bx10LOD692QM"},"outputs":[],"source":["# @title Upload a model to registry on Vertex AI\n","model = aiplatform.Model.upload(\n","    display_name=\"llama3.1_8b_inst\",\n","    artifact_uri=f\"{BUCKET_URI}/{ARTIFACT_NAME}\",\n","    serving_container_image_uri=CONTAINER_URI,\n","    serving_container_environment_variables={\n","        \"NUM_SHARD\": \"1\",\n","        \"MAX_INPUT_TOKENS\": \"512\",\n","        \"MAX_TOTAL_TOKENS\": \"1024\",\n","        \"MAX_BATCH_PREFILL_TOKENS\": \"1512\",\n","    },\n","    serving_container_ports=[8080],\n",")\n","model.wait()"]},{"cell_type":"markdown","metadata":{"id":"Oevu7EI-92QM"},"source":["## Deploy model on Vertex AI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-69cdwfu92QM"},"outputs":[],"source":["# @title Create endpoint\n","endpoint_name = \"llama3.1_8b_inst-endpoint\"\n","endpoint = aiplatform.Endpoint.create(display_name=endpoint_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvjhR74v92QM"},"outputs":[],"source":["# @title Deploy a model\n","deployed_model = model.deploy(\n","    endpoint=endpoint,\n","    machine_type=\"g2-standard-4\",\n","    accelerator_type=\"NVIDIA_L4\",\n","    accelerator_count=1,\n",")"]},{"cell_type":"markdown","metadata":{"id":"lIWfZfJn92QM"},"source":["## Online predictions on Vertex AI"]},{"cell_type":"code","source":["# @title Get tokenizer\n","import os\n","from huggingface_hub import get_token\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    MODEL_NAME,\n","    token=get_token(),\n",")"],"metadata":{"id":"aN1Bjp_6EHei"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8beAdkI92QN"},"outputs":[],"source":["# @title Predict within the same session\n","messages = [\n","    {\"role\": \"user\", \"content\": \"What's Deep Learning?\"},\n","]\n","\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True,\n",")\n","# <bos><start_of_turn>user\\nWhat's Deep Learning?<end_of_turn>\\n<start_of_turn>model\\n\n","\n","output = deployed_model.predict(\n","    instances=[\n","        {\n","            \"inputs\": inputs,\n","            \"parameters\": {\n","                \"max_new_tokens\": 256, \"do_sample\": True,\n","                \"top_p\": 0.95, \"temperature\": 1.0,\n","            },\n","        },\n","    ]\n",")\n","print(output.predictions[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jdEgTBow92QN"},"outputs":[],"source":["# @title Predict within a different session\n","import os\n","from google.cloud import aiplatform\n","\n","aiplatform.init(project=PROJECT_ID, location=LOCATION)\n","\n","endpoint_display_name = \"llama3.1_8b_inst-endpoint\"  # TODO: change to your endpoint display name\n","\n","# Iterates over all the Vertex AI Endpoints within the current project and keeps the first match (if any), otherwise set to None\n","ENDPOINT_ID = next(\n","    (endpoint.name for endpoint in aiplatform.Endpoint.list()\n","     if endpoint.display_name == endpoint_display_name),\n","    None\n",")\n","\n","print(ENDPOINT_ID)\n","\n","assert ENDPOINT_ID, (\n","    \"`ENDPOINT_ID` is not set, please make sure that the `endpoint_display_name` is correct at \"\\\n","    f\"https://console.cloud.google.com/vertex-ai/online-prediction/endpoints?project={PROJECT_ID}\"\n",")\n","\n","endpoint = aiplatform.Endpoint(f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}\")\n","print(endpoint)\n","\n","output = endpoint.predict(\n","    instances=[\n","        {\n","            \"inputs\": \"<bos><start_of_turn>user\\nWhat's Deep Learning?<end_of_turn>\\n<start_of_turn>model\\n\",\n","            \"parameters\": {\n","                \"max_new_tokens\": 128,\n","                \"do_sample\": True,\n","                \"top_p\": 0.95,\n","                \"temperature\": 0.7,\n","            },\n","        },\n","    ],\n",")\n","print(output.predictions[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVOh7qOu92QO"},"outputs":[],"source":["# @title Resource clean up\n","deployed_model.undeploy_all()\n","deployed_model.delete()\n","model.delete()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7ujbNG292QO"},"outputs":[],"source":["!gcloud storage rm -r $BUCKET_URI"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}