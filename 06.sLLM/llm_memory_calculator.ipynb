{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNnE9EWfLYQUzmcn9J+NiDu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LLM Memory calculator\n","\n","* Requried memory(GB) = # of parameter X precision(bytes) / 1024^3\n","\n","LLM 필요 메모리는 모델 파라미터 개수와 precision 외에도 여러 요인에 따라 달라지기 때문에 정확히 계산하기는 어렵습니다. 하지만, 간단한 추정을 위해 다음과 같은 공식을 사용할 수 있습니다.\n","\n","*** 필요 메모리 (GB) ≈ (파라미터 개수 × Precision (bytes)) / (1024^3) ***\n","\n","파라미터 개수: 모델의 파라미터 개수입니다. 예를 들어, 100억 개의 파라미터를 가진 모델의 경우 10,000,000,000 을 입력합니다.\n","Precision (bytes): 모델의 precision에 따라 달라지는 값입니다. 일반적인 precision과 그에 해당하는 bytes는 다음과 같습니다.\n","\n","* FP32 (float32): 4 bytes\n","* FP16 (float16): 2 bytes\n","* BF16 (bfloat16): 2 bytes\n","* INT8 (int8): 1 byte\n","\n","예시:\n","\n","100억 개의 파라미터를 가진 모델을 FP16 precision으로 실행하는 경우 필요한 메모리는 다음과 같이 계산할 수 있습니다.\n","\n","*** 필요 메모리 (GB) ≈ (10,000,000,000 × 2) / (1024^3) ≈ 18.63 GB ***\n","\n","주의 사항:\n","\n","* 추가적인 메모리: 위 공식은 모델 가중치를 저장하는 데 필요한 메모리만 고려합니다. 그래디언트, optimizer 상태, 활성화 값 등을 저장하기 위한 추가 메모리가 필요합니다. 일반적으로 이러한 추가 메모리는 모델 가중치 메모리의 20% 정도로 추정할 수 있습니다.\n","* 활성화 값: 모델의 활성화 값은 입력 시퀀스 길이에 따라 달라지므로 필요한 메모리에 영향을 미칩니다. 긴 시퀀스를 처리할수록 더 많은 메모리가 필요합니다.\n","* 배치 크기: 배치 크기가 클수록 더 많은 메모리가 필요합니다.\n","* 기타 요인: 모델 아키텍처, 최적화 기술, 하드웨어 및 소프트웨어 환경 등 여러 요인이 필요한 메모리에 영향을 미칠 수 있습니다.\n","정확한 메모리 요구량을 확인하려면:\n","\n","* 실험: 모델을 직접 실행하여 메모리 사용량을 모니터링하는 것이 가장 정확합니다.\n","* 문서: 모델 제공자의 문서 또는 모델 카드를 참조하여 메모리 요구량에 대한 정보를 확인할 수 있습니다.\n","\n","참고:\n","\n","위 공식은 대략적인 추정치를 제공하며 실제 메모리 사용량은 다를 수 있습니다.\n","LLM을 효율적으로 실행하려면 충분한 메모리를 가진 GPU를 사용하는 것이 중요합니다.\n","메모리 사용량을 줄이기 위해 quantization, mixed precision training, gradient checkpointing 등의 기술을 사용할 수 있습니다."],"metadata":{"id":"ZzMJddT1a1vR"}},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2byqup4oava4","executionInfo":{"status":"ok","timestamp":1737036362664,"user_tz":-540,"elapsed":44,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"009434b5-7221-4e58-de29-b05f60d23396"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["74.50580596923828"]},"metadata":{},"execution_count":7}],"source":["# 40B\n","memory_gb = (2 * 40000000000)/(1024*1024*1024)\n","memory_gb\n"]},{"cell_type":"code","source":["# 405B\n","memory_gb = (2 * 405000000000)/(1024*1024*1024)\n","memory_gb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VdJcIRh1bb6E","executionInfo":{"status":"ok","timestamp":1737036469094,"user_tz":-540,"elapsed":41,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"f2fbd804-2182-427d-b00d-565adeb73dda"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["754.3712854385376"]},"metadata":{},"execution_count":9}]}]}